{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Keeping Trump on Topic: LIN353C Final Project\n",
    "\n",
    "By Hannah Brinsko and Aditya Kharosekar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import csv\n",
    "import datetime\n",
    "import gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocessing - Scraping tweets, and cleaning them up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Importing tweets from the CSV file - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3218, 28)\n",
      "5     2016-09-27T22:13:24\n",
      "8     2016-09-27T21:08:22\n",
      "11    2016-09-27T20:31:14\n",
      "12    2016-09-27T20:14:33\n",
      "13    2016-09-27T20:06:25\n",
      "Name: time, dtype: object\n"
     ]
    }
   ],
   "source": [
    "tweets_csv = pd.read_csv(\"tweets.csv\")\n",
    "trump_tweets = tweets_csv[tweets_csv['handle']==\"realDonaldTrump\"]\n",
    "print(trump_tweets.shape)\n",
    "print(trump_tweets['time'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Looking at the CSV file, we see that it contains tweets only up to 09/27/2016. We need his more recent tweets as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Getting his most recent ~3200 tweets.\n",
    "\n",
    "3200 is approximately the limit to how many tweets Tweepy allows us to scrape. As it turns out, this is more than enough for our use when combined with our CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import json\n",
    "from tweepy import OAuthHandler\n",
    "import codecs\n",
    "\n",
    "consumer_key = \"i387QW7Eqgh12UHmK3VoQO9K5\"\n",
    "consumer_secret = \"BQI8c5eKale4etdA21mawnFqOmAziDQpnThm679V7UtLjbWlMG\"\n",
    "access_token = \"816857419338764288-S8Ay111O2Mo32QAs88tSnv5uKvmGCkF\"\n",
    "access_secret = \"HVU19yLuV0klltJl1fsDibAi7Hiq1U4GwsEV9kozTAc1m\"\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "all_tweets = []\n",
    "\n",
    "new_tweets = api.user_timeline(screen_name=\"realDonaldTrump\", count=200)\n",
    "\n",
    "\n",
    "all_tweets.extend(new_tweets)\n",
    "oldest = all_tweets[-1].id-1\n",
    "\n",
    "t = new_tweets[0];\n",
    "\n",
    "while len(new_tweets) > 0:\n",
    "   new_tweets = api.user_timeline(screen_name = \"realDonaldTrump\", count=200, max_id = oldest)\n",
    "   all_tweets.extend(new_tweets)\n",
    "   oldest = all_tweets[-1].id-1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3227\n"
     ]
    }
   ],
   "source": [
    "print(len(all_tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We now have his most recent tweets.\n",
    "\n",
    "However, there is significant overlap between the tweets that we have scraped from his account and the tweets that are in the CSV file.\n",
    "\n",
    "The latest tweet in the CSV file was posted on September 27, 2016 at 22:13:24. So, we need to keep any scraped tweets which were posted after this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tweets = []\n",
    "i = 0\n",
    "while (all_tweets[i].created_at!=datetime.datetime(2016, 9, 27, 22, 13, 24)):\n",
    "    tweets.append(all_tweets[i].text)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "We have  4728 tweets to work with\n"
     ]
    }
   ],
   "source": [
    "tweets1 = trump_tweets['text']\n",
    "tweets1 = tweets1.tolist()\n",
    "for t in tweets1:\n",
    "    tweets.append(t)\n",
    "print(type(tweets))\n",
    "print(\"We have \", len(tweets), \"tweets to work with\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Making distributional vectors from each tweet\n",
    "\n",
    "But to do that, we need to - \n",
    "1. Remove any twitter links and image links\n",
    "2. Remove any stopwords\n",
    "3. Make sure that we have a list of tweets where each tweet is a string\n",
    "4. Then use CountVectorizer http://scikit-learn.org/stable/modules/feature_extraction.html#common-vectorizer-usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Removing links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'u.s.', 'recorded', 'its', 'slowest', 'economic', 'growth', 'in', 'five', 'years', '(2016).', 'gdp', 'up', 'only', '1.6%.', 'trade', 'deficits', 'hurt', 'the', 'economy', 'very', 'badly.']\n",
      "['the', 'u.s.', 'recorded', 'its', 'slowest', 'economic', 'growth', 'in', 'five', 'years', '(2016).', 'gdp', 'up', 'only', '1.6%.', 'trade', 'deficits', 'hurt', 'the', 'economy', 'very', 'badly.']\n"
     ]
    }
   ],
   "source": [
    "temp_tweets = []\n",
    "for t in tweets:\n",
    "    temp_tweets.append(t.lower().split())\n",
    "\n",
    "print(temp_tweets[1])\n",
    "for t in temp_tweets:\n",
    "    for w in t:\n",
    "        if \"http\" in w or \"@\" in w: #I've removed any instances where he tags anyone in his tweets. \n",
    "                                    #I thought the word vectors might be too sparse if I left those in.\n",
    "            t.remove(w)\n",
    "print(temp_tweets[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## NOTE: This link removal is not working properly. Have to fix later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "for t in temp_tweets:\n",
    "    for w in t:\n",
    "        if w in stop:\n",
    "            t.remove(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Succesfully removed stopwords. At this point, each tweet is a list of words and temp_tweets is a list. What we need to use CountVectorizer is a list where each element is a string.\n",
    "\n",
    "Therefore, we need to convert each tweet from a lists of words to a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = []\n",
    "for t in temp_tweets:\n",
    "    tweets.append(' '.join(t))\n",
    "type(tweets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Our methodology for classifying tweets\n",
    "\n",
    "Step 0 - Download a pre-trained Word2Vec model. We tried training our own model, but we did not have enough data.\n",
    "\n",
    "Step 1 - Hand tag some number of tweets (we ended up tagging about 280 tweets) and classify them into the following categories - \n",
    "1. Foreign Policy / International News\n",
    "2. Domestic Policy / domestic news\n",
    "3. Tweets about the media\n",
    "4. Attack tweets\n",
    "5. Other tweets\n",
    "6. Tweets about the election\n",
    "\n",
    "Step 2 - From our hand-tagged corpus, and for each category, create a list of words used.\n",
    "\n",
    "Step 3 - Create a word vector for each category by summing up the individual word vectors\n",
    "\n",
    "Step 4 - For each subsequent tweet, find cosine similarity between it and each category vector. Assign that tweet to the category it is most similar to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Step 0 - Downloading a pre-trained Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "google_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.24023438, -0.046875  , -0.05786133, -0.17285156,  0.13476562,\n",
       "       -0.03466797,  0.05957031, -0.02209473,  0.00334167, -0.03564453,\n",
       "       -0.04589844,  0.04248047, -0.09570312,  0.21582031, -0.12597656,\n",
       "       -0.06835938,  0.15332031,  0.17773438, -0.03662109,  0.03515625,\n",
       "        0.04418945,  0.28320312,  0.05297852, -0.01953125, -0.27929688,\n",
       "       -0.23828125,  0.00238037, -0.04345703,  0.26367188,  0.06591797,\n",
       "       -0.02624512,  0.03369141,  0.02880859, -0.15332031,  0.11083984,\n",
       "       -0.046875  , -0.02355957,  0.01000977,  0.23632812, -0.07421875,\n",
       "        0.27734375, -0.14746094,  0.02478027,  0.10351562, -0.33007812,\n",
       "        0.0050354 , -0.04736328,  0.16699219,  0.015625  ,  0.30859375,\n",
       "        0.15039062, -0.09472656,  0.08349609,  0.05883789, -0.17578125,\n",
       "       -0.00273132, -0.04101562, -0.30859375, -0.15332031, -0.05200195,\n",
       "       -0.19140625,  0.13476562, -0.28515625, -0.06445312, -0.00058365,\n",
       "        0.01348877, -0.00527954,  0.10498047,  0.20605469,  0.01538086,\n",
       "        0.06445312,  0.13574219,  0.05737305, -0.05541992,  0.02648926,\n",
       "        0.02868652,  0.33007812,  0.0246582 , -0.08642578, -0.0625    ,\n",
       "       -0.05834961, -0.25390625,  0.01055908, -0.20019531,  0.02770996,\n",
       "        0.15820312, -0.38867188, -0.06005859,  0.24414062,  0.09472656,\n",
       "        0.12695312, -0.14746094, -0.08300781, -0.10253906, -0.03540039,\n",
       "       -0.2421875 ,  0.03637695,  0.0057373 ,  0.265625  ,  0.31835938,\n",
       "       -0.25976562, -0.15429688, -0.06079102,  0.14941406,  0.00765991,\n",
       "       -0.09716797, -0.07226562, -0.01306152, -0.03955078, -0.01245117,\n",
       "       -0.140625  , -0.13964844,  0.01080322,  0.10253906,  0.11816406,\n",
       "       -0.31640625, -0.05371094,  0.06225586, -0.01977539,  0.15039062,\n",
       "       -0.05664062,  0.03564453,  0.11669922,  0.04956055, -0.171875  ,\n",
       "        0.11621094,  0.16601562, -0.0378418 , -0.03149414, -0.10986328,\n",
       "        0.03515625, -0.18359375,  0.03759766, -0.36914062, -0.18847656,\n",
       "        0.03833008,  0.03588867,  0.07763672, -0.01123047, -0.01904297,\n",
       "        0.09912109,  0.11328125,  0.02050781, -0.12792969,  0.16015625,\n",
       "       -0.23242188,  0.15722656, -0.13867188, -0.12792969,  0.0234375 ,\n",
       "        0.2734375 , -0.078125  ,  0.30078125, -0.11914062,  0.35742188,\n",
       "       -0.0703125 , -0.04394531, -0.20507812,  0.16308594, -0.10888672,\n",
       "       -0.05419922, -0.00585938,  0.27734375,  0.22363281, -0.02185059,\n",
       "       -0.33984375,  0.21386719, -0.28125   ,  0.19824219, -0.06982422,\n",
       "        0.03930664, -0.12695312,  0.04272461, -0.23925781,  0.08398438,\n",
       "        0.09912109,  0.04467773,  0.02246094, -0.06835938,  0.02282715,\n",
       "       -0.07373047,  0.04589844, -0.15234375,  0.08154297,  0.06884766,\n",
       "       -0.13183594, -0.29296875,  0.13085938, -0.03051758,  0.07324219,\n",
       "       -0.09375   ,  0.00778198,  0.22558594,  0.296875  , -0.21679688,\n",
       "        0.06884766, -0.01019287,  0.22167969, -0.10302734,  0.14941406,\n",
       "       -0.03417969,  0.22460938,  0.09472656, -0.06494141,  0.24414062,\n",
       "        0.04321289,  0.08154297, -0.06347656, -0.05981445, -0.02392578,\n",
       "       -0.08642578,  0.03686523, -0.24414062,  0.12402344,  0.11816406,\n",
       "        0.00180817, -0.01470947,  0.12109375, -0.15039062, -0.02612305,\n",
       "       -0.10058594,  0.07128906,  0.29492188,  0.02038574,  0.19238281,\n",
       "        0.25195312,  0.05224609,  0.16015625,  0.140625  , -0.13964844,\n",
       "        0.04321289,  0.12060547,  0.01098633, -0.09423828, -0.06542969,\n",
       "        0.01977539, -0.03222656,  0.1484375 ,  0.05737305, -0.30859375,\n",
       "       -0.1171875 , -0.03491211,  0.06835938, -0.03857422, -0.06347656,\n",
       "        0.03491211,  0.00891113,  0.17382812,  0.05151367,  0.03466797,\n",
       "       -0.04736328, -0.13964844, -0.12207031, -0.13085938,  0.02844238,\n",
       "       -0.04736328,  0.16503906,  0.30664062,  0.07958984, -0.2890625 ,\n",
       "        0.02941895,  0.04614258,  0.07519531,  0.13378906, -0.30664062,\n",
       "       -0.04174805, -0.23535156, -0.13964844, -0.23535156,  0.15722656,\n",
       "        0.01623535, -0.03051758,  0.0703125 , -0.13085938, -0.05664062,\n",
       "       -0.12988281,  0.23046875,  0.03881836, -0.0559082 , -0.0201416 ,\n",
       "       -0.08398438, -0.1953125 ,  0.09277344,  0.05444336, -0.03442383,\n",
       "       -0.11474609, -0.23046875,  0.10058594,  0.26953125, -0.18066406,\n",
       "       -0.18554688, -0.22363281, -0.06054688, -0.25195312,  0.02832031,\n",
       "       -0.28710938,  0.25      , -0.14355469, -0.140625  ,  0.03222656], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_model['campaign']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "google_model is our pre-trained model which we will be using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Step 1 - Hand tagging tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Using a .csv file of a number of hand tagged tweets, we place the tweets into the preselected categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "foreign = []\n",
    "domestic =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "f = open('tagged_tweets.csv')\n",
    "csv_f = csv.reader(f)\n",
    "\n",
    "for row in csv_f:\n",
    "    tweet = row[0]\n",
    "    cats = row[1]\n",
    "    if \"1\" in cats:\n",
    "        foreign.append(tweet)\n",
    "    elif \"2\" in cats:\n",
    "        domestic.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domestic:  104\n",
      "Foreign:  56\n"
     ]
    }
   ],
   "source": [
    "print(\"Domestic: \",len(domestic))\n",
    "print(\"Foreign: \", len(foreign))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Step 2 - Making a list of words used in each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Each tweet is a string right now.\n",
    "#This function will split up the string into individual words, remove any words which start with @\n",
    "#(i.e our generated tweets won't have any tags) and remove any punctuation\n",
    "\n",
    "def clean_up(tweets):\n",
    "    tweets1 = []\n",
    "    for t in tweets:\n",
    "        tweets1.append(t.split())\n",
    "        \n",
    "    tweets_words = []\n",
    "    for t in tweets1:\n",
    "        for w in t:\n",
    "            tweets_words.append(w)\n",
    "    tweets_words = tweets_words\n",
    "    \n",
    "    #removing '@' from any word which has it. The google_model does not have any words which start with @\n",
    "    temp_words = []\n",
    "    for word in tweets_words:\n",
    "        if word[0]=='@':\n",
    "            temp_words.append(word[1:])\n",
    "        else:\n",
    "            temp_words.append(word)\n",
    "    return temp_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "domestic_words = clean_up(domestic)\n",
    "foreign_words = clean_up(foreign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(words):\n",
    "    x = [''.join(c for c in s if c not in string.punctuation) for s in words]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "domestic_words = remove_punctuation(domestic_words)\n",
    "foreign_words = remove_punctuation(foreign_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "When we generate tweets, we will use these words as the basis for our bigram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def remove_short_words(words, length):\n",
    "    temp_words = []\n",
    "    for word in words:\n",
    "        if len(word)>=length:\n",
    "            temp_words.append(word)\n",
    "    return temp_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "d_short_words = remove_short_words(domestic_words, 4)\n",
    "f_short_words = remove_short_words(foreign_words, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Step 3 - Create a category vector by adding up individual word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_vector(words):\n",
    "    vector = np.ones(300)\n",
    "    for word in words:\n",
    "        try:\n",
    "            vector = vector + google_model[word]\n",
    "        except KeyError: #some words are not in model. I don't want to pre-process everything so I'm just handling each exception\n",
    "            pass\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "domestic_vector = create_vector(domestic_words)\n",
    "foreign_vector = create_vector(foreign_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'getting ready visit walter reed medical center melania. looking forward seeing bravest greatest americans!'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_vector(tweet):\n",
    "    vector = np.ones(300)\n",
    "    for word in tweet:\n",
    "        try:\n",
    "            vector = vector + google_model[word]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "specific_foreign = []\n",
    "specific_domestic = []\n",
    "for word in foreign_words:\n",
    "    if word not in domestic_words:\n",
    "        specific_foreign.append(word)\n",
    "for word in domestic_words:\n",
    "    if word not in foreign_words:\n",
    "        specific_domestic.append(word)\n",
    "\n",
    "specific_dshort = []\n",
    "specific_fshort = []\n",
    "for word in d_short_words:\n",
    "    #if word not in f_short_words and not in stop:\n",
    "    if word not in stop:\n",
    "        specific_fshort.append(word)\n",
    "for word in f_short_words:\n",
    "    #if word not in d_short_words:\n",
    "    if word not in stop:\n",
    "        specific_dshort.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dshort_vector = create_vector(specific_dshort)\n",
    "fshort_vector = create_vector(specific_fshort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "domestic_tags = ['press', 'media', 'election', 'healthcare', 'Obamacare', 'obamacare','american', 'immigrant', 'immigrants',\n",
    "                'Committee', 'wall','Wall', 'jobs', 'taxes', 'senate', 'congress', 'dems','drugs']\n",
    "foreign_tags = ['Russia', 'russia', 'China', 'trade', 'mexico', 'terrorist', 'terrorists', 'terrorism',\n",
    "               'migrants', 'immigration','Immigration', 'President', 'Egypt', 'Syria', 'Minister', 'Ambassador','Korea','war']\n",
    "\n",
    "def count_tag_occurrences(tweet):\n",
    "    dcount= 0\n",
    "    fcount = 0\n",
    "    tweet = tweet.split()\n",
    "    tweet = remove_punctuation(tweet)\n",
    "    for word in tweet:\n",
    "        if word in domestic_tags:\n",
    "            dcount+=1\n",
    "        if word in foreign_tags:\n",
    "            fcount+=1\n",
    "    return dcount, fcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def calc_cosine_similarity(tweet_vector, category_vector):\n",
    "    return cosine_similarity(tweet_vector, category_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def calc_scores(tweet):\n",
    "    dcount, fcount = count_tag_occurrences(tweet)\n",
    "    dscore = calc_cosine_similarity(create_vector(tweet), dshort_vector)\n",
    "    fscore = calc_cosine_similarity(create_vector(tweet), fshort_vector)\n",
    "    return dscore, fscore, dcount, fcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def model(dom, fore):\n",
    "    dcount = 0\n",
    "    fcount = 0\n",
    "    total = 0\n",
    "    for tweet in dom:\n",
    "        domestic_score, foreign_score, domestic_tag_count, foreign_tag_count = calc_scores(tweet)\n",
    "        total+=1\n",
    "        if abs(domestic_score - foreign_score) <= 0.005:\n",
    "            if domestic_tag_count >=foreign_tag_count:\n",
    "                dcount+=1\n",
    "            else:\n",
    "                fcount+=1\n",
    "        else:\n",
    "            if domestic_score > foreign_score:\n",
    "                dcount+=1\n",
    "            else:\n",
    "                fcount+=1\n",
    "\n",
    "    print(\"Number of domestic tweets = \", total)\n",
    "    print(\"Number of domestic tweets tagged as domestic = \", dcount)\n",
    "    print(\"Accuracy of domestic tweets = \", dcount / total)\n",
    "\n",
    "    dcount = 0\n",
    "    fcount = 0\n",
    "    total = 0\n",
    "    for tweet in fore:\n",
    "        domestic_score, foreign_score,domestic_tag_count, foreign_tag_count = calc_scores(tweet)\n",
    "        total+=1\n",
    "        if domestic_score > foreign_score:\n",
    "            dcount+=1\n",
    "        else:\n",
    "            fcount+=1\n",
    "        \n",
    "    print(\"\\nNumber of foreign tweets = \", total)\n",
    "    print(\"Number of foreign tweets tagged as foreign = \", fcount)\n",
    "    print(\"Accuracy of foreign tweets = \", fcount / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of domestic tweets =  104\n",
      "Number of domestic tweets tagged as domestic =  76\n",
      "Accuracy of domestic tweets =  0.7307692307692307\n",
      "\n",
      "Number of foreign tweets =  56\n",
      "Number of foreign tweets tagged as foreign =  39\n",
      "Accuracy of foreign tweets =  0.6964285714285714\n"
     ]
    }
   ],
   "source": [
    "model(domestic, foreign)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The above accuracy scores are based on testing our model on our training set.\n",
    "\n",
    "We will now test our model on a small test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of domestic tweets =  23\n",
      "Number of domestic tweets tagged as domestic =  18\n",
      "Accuracy of domestic tweets =  0.782608695652174\n",
      "\n",
      "Number of foreign tweets =  20\n",
      "Number of foreign tweets tagged as foreign =  14\n",
      "Accuracy of foreign tweets =  0.7\n"
     ]
    }
   ],
   "source": [
    "domestic_index = [1, 5, 8, 9, 11, 13, 38, 41, 42, 43, 50, 51, 54, 57, 66, 67, 68, 74, 79, 100, 105, 112, 120]\n",
    "foreign_index = [16, 28, 30, 32, 59, 64, 65, 75, 76, 82, 87, 96, 98, 123, 157, 175, 178, 224, 332, 333]\n",
    "\n",
    "dom_tweets = [tweets[i] for i in domestic_index]\n",
    "fore_tweets = [tweets[i] for i in foreign_index]\n",
    "\n",
    "model(dom_tweets, fore_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : happy birthday our melania! https://t.co/np7kyhglsv\n",
      "1 : u.s. recorded slowest economic growth five years (2016). gdp only 1.6%. trade deficits hurt economy badly.\n",
      "2 : ...the ninth circuit, has terrible record being overturned (close 80%). used call \"judge shopping!\" messy system.\n",
      "3 : of very big country, many choices, everyone notice both \"ban\" case now \"sanctuary\" case brought ...\n",
      "4 : first ninth circuit rules ban &amp; it hits on sanctuary cities-both ridiculous rulings. see in the supreme court!\n",
      "5 : 'presidential executive order promoting agriculture rural prosperity america' executive order:‚Ä¶\n",
      "6 : remarks the united states holocaust memorial museum's national days remembrance. full remarks:‚Ä¶\n",
      "7 : don't let fake media tell that have changed position the wall. will get built help stop drugs, human trafficking etc.\n",
      "8 : canada made business our dairy farmers wisconsin other border states difficult. will stand this. watch!\n",
      "9 : proud for leadership these important issues. looking forward hearing speak the w20!\n",
      "10 : today, signed holocaust remembrance proclamation: #icymi- statement last night at‚Ä¶\n",
      "11 : our healthcare plan approved, see real healthcare premiums will start tumbling down. obamacare in death spiral!\n",
      "12 : join in congratulating @astropeggy using hashtag #congratspeggy! earlier today:‚Ä¶\n",
      "13 : ....the wall not built, be, drug situation never fixed way it should be! #buildthewall\n",
      "14 : wall a important tool stopping drugs pouring country poisoning our youth (and many others)!\n",
      "15 : two fake news polls released yesterday, abc &amp; nbc, containing very positive info, totally wrong general e. watch!\n",
      "16 : ...popular vote. abc news/washington post poll (wrong big election) said almost stand their vote me &amp; 53% said strong leader.\n",
      "17 : new polls today very good considering much the media fake almost always negative. would still beat hillary .....\n",
      "18 : eventually, at later date we get started early, mexico be paying, some form, the badly needed border wall.\n",
      "19 : democrats don't want money budget going border wall despite fact it stop drugs very bad ms 13 gang members.\n",
      "20 : obamacare in serious trouble. dems need big money keep going - otherwise dies far sooner anyone would thought.\n",
      "21 : thank lake worth, florida.\n",
      "22 : interesting election currently taking place france.\n",
      "23 : am committed keeping air water clean always remember economic growth enhances environmental protection. jobs matter!\n",
      "24 : today earth day, celebrate beautiful forests, lakes land. stand committed preserving natural beauty our nation.\n",
      "25 : big tax reform tax reduction be announced next wednesday.\n",
      "26 : next saturday night will holding big rally pennsylvania. look forward it!\n",
      "27 : getting ready visit walter reed medical center melania. looking forward seeing bravest greatest americans!\n",
      "28 : rt israeli pm netanyahu praises u.s. policy changes meeting defense. sec mattis\n",
      "29 : rt chicago approves new plan hide illegal immigrants the feds, plus give access city services\n",
      "30 : 5 sb victories since 2002, was honor give bob kraft, coach belichick, the players first to‚Ä¶\n",
      "31 : 'presidential executive order identifying reducing tax regulatory burdens' executive order:‚Ä¶\n",
      "32 : rt .@potus @ivankatrump, jared kushner, &amp; dina powell the oval office today w/ aya &amp; brother basel. #w‚Ä¶\n",
      "33 : welcome home, aya! #godblesstheusaüá∫üá∏\n",
      "34 : china very much economic lifeline north korea so, nothing easy, want solve north korean problem, they will\n",
      "35 : matter much accomplish the ridiculous standard the first 100 days, &amp; has a lot (including s.c.), media kill!\n",
      "36 : another terrorist attack paris. people france not take much of this. have big effect presidential election!\n",
      "37 : rt nyt editor apologizes misleading tweet new england patriots' visit the white house (via h‚Ä¶\n",
      "38 : great honor host pm paolo gentiloni italy the white house afternoon! #icymi- joint press conference‚Ä¶\n",
      "39 : we're going use american steel, we're going use american labor, are going come first all deals. ‚û°Ô∏è‚Ä¶\n",
      "40 : failing has calling wrong two years, got caught a big lie concerning new england patriots visit w.h.\n",
      "41 : great honor host champion new england the white house today. congratulations!‚Ä¶\n",
      "42 : today signed veterans (our heroes) choice program extension &amp; improvement act #s544 watch‚Ä¶\n",
      "43 : #buyamericanhireamericanüá∫üá∏\n",
      "44 : dems failed kansas are failing georgia. great job karen handel! is hollywood vs. georgia june 20th.\n",
      "45 : despite major outside money, fake media support eleven republican candidates, big \"r\" win runoff georgia. glad be help!\n",
      "46 : #buyamericanhireamericanüá∫üá∏ https://t.co/rf9aivvb7g\n",
      "47 : learned jon is running congress georgia, doesn't even live the district. republicans, get and vote!\n",
      "48 : republicans must get today vote georgia 6. force runoff easy win! dem ossoff raise taxes-very bad crime &amp; 2nd a.\n",
      "49 : democrat jon ossoff would a disaster congress. weak crime illegal immigration, bad jobs wants higher taxes. say\n",
      "50 : will interviewed by starting 6:00 a.m. enjoy!\n",
      "51 : weak illegal immigration policies the obama admin. allowed bad ms 13 gangs form cities across u.s. are removing fast!\n",
      "52 : eleven republican candidates running georgia (on tuesday) congress, runoff be win. vote \"r\" lower taxes &amp; safety!\n",
      "53 : see tomorrow wisconsin! 'trump spurs small-business optimism milwaukee area'\n",
      "54 : trump approval hits 50%\n",
      "55 : rt trump approval hits 50%\n",
      "56 : super liberal democrat the georgia congressioal race tomorrow wants protect criminals, allow illegal immigration raise taxes!\n",
      "57 : fake media (not real media) gotten even worse since election. every story badly slanted. have hold to truth!\n",
      "58 : great book your reading enjoyment: \"reasons vote democrats\" michael j. knowles.\n",
      "59 : \"the first 90 days my presidency exposed total failure the last eight years foreign policy!\" true.\n",
      "60 : recent kansas election (congress) a really big media event, republicans won. they play the same game georgia-bad!\n",
      "61 : military building is rapidly becoming stronger ever before. frankly, have choice!\n",
      "62 : someone look who paid small organized rallies yesterday. the election over!\n",
      "63 : did was almost impossible thing do a republican-easily the electoral college! tax returns brought again?\n",
      "64 : happy easter everyone!\n",
      "65 : would call china currency manipulator they working us the north korean problem? will see happens!\n",
      "66 : rt looking forward hosting annual easter egg roll the monday!\n",
      "67 : weekly address- https://t.co/b2nqzj53ft\n",
      "68 : rt great again: feds arrest murder suspect 'fast furious' scandal...\n",
      "69 : was great honor welcome atlanta's heroic first responders the white house afternoon!\n",
      "70 : things work fine the u.s.a. russia. the right time everyone come their senses &amp; will lasting peace!\n",
      "71 : have great confidence china properly deal north korea. they unable do so, u.s., its allies, will! u.s.a.\n",
      "72 : jobs returning, illegal immigration plummeting, law, order justice being restored. are truly making america great again!\n",
      "73 : one one keeping promises - the border, energy, jobs, regulations. big changes are happening!\n",
      "74 : economic confidence soaring we unleash power private sector job creation stand for american workers. #americafirst\n",
      "75 : great meeting w/ nato sec. gen. agreed the importance getting countries pay fair share &amp; focus on‚Ä¶\n",
      "76 : great win kansas last night ron estes, easily winning congressional race the dems, spent heavily &amp; predicted victory!\n",
      "77 : a good call last night president china concerning the menace north korea.\n",
      "78 : will interviewed at 6:00 a.m. enjoy!\n",
      "79 : great strategic &amp; policy ceo forum today my cabinet secretaries top ceo's around united states.‚Ä¶\n",
      "80 : ron estes running today congress the great state kansas. wonderful guy, need help healthcare &amp; tax cuts (reform).\n",
      "81 : north korea looking trouble. china decides help, would great. not, will solve problem without them! u.s.a.\n",
      "82 : explained president china a trade deal the u.s. be far better them they solve the north korean problem!\n",
      "83 : rt grateful syrians react strike: 'i'll name son donald' #syrianstrikes\n",
      "84 : happy passover everyone celebrating united states america, israel, around the world. #chagsameach\n",
      "85 : congratulations justice neil gorsuch his elevation the united states supreme court. great day americ‚Ä¶\n",
      "86 : thank #usa\n",
      "87 : ...confidence president al sisi handle situation properly.\n",
      "88 : sad hear the terrorist attack egypt. u.s. strongly condemns. have great...\n",
      "89 : judge gorsuch sworn rose garden the white house monday at 11:00 a.m. will be a great justice. proud him!\n",
      "90 : reason don't generally hit runways that are easy inexpensive quickly fix (fill and top)!\n",
      "91 : congratulations our great military men women representing united states, the world, well the syria attack.\n",
      "92 : ...goodwill friendship formed, only time tell trade.\n",
      "93 : was great honor have president xi jinping madame peng liyuan china our guests the united states. tremendous...\n",
      "94 : rt proud arabella joseph their performance honor president xi jinping madame peng liyuan's official‚Ä¶\n",
      "95 : was honor host american heroes the #soldierridedc the today @vp‚Ä¶\n",
      "96 : jobs, jobs, jobs! https://t.co/b5qbn6llze\n",
      "97 : am deeply committed preserving strong relationship &amp; strengthening america's long-standing support for‚Ä¶\n",
      "98 : great talk jobs #nabtu2017. tremendous spirit &amp; optimism - will deliver!\n",
      "99 : thank sean mcgarvey &amp; entire governing board presidents honoring w/an invite speak. #nabtu2017‚Ä¶\n"
     ]
    }
   ],
   "source": [
    "for index in range(0, 100):\n",
    "    print(index, \":\", tweets[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bigram_domestic = []\n",
    "bigram_foreign = []\n",
    "for tweet in domestic:\n",
    "    newtweet = ''.join(('<s> ',tweet,' </s>'))\n",
    "    new = newtweet.split()\n",
    "    for word in new:\n",
    "        if word[0:5] != \"https\" and word != \"&amp;\":\n",
    "            word = word.lower()\n",
    "            bigram_domestic.append(word)\n",
    "for tweet in foreign:\n",
    "    newtweet = ''.join(('<s> ',tweet,' </s>'))\n",
    "    new = newtweet.split()\n",
    "    for word in new:\n",
    "        if word[0:5] != \"https\" and word != \"&amp;\":\n",
    "            word = word.lower()\n",
    "            bigram_foreign.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_tweet(category):\n",
    "    if category == \"domestic\":\n",
    "        cfreq = nltk.ConditionalFreqDist(nltk.bigrams(bigram_domestic))\n",
    "        cprob = nltk.ConditionalProbDist(cfreq, nltk.MLEProbDist)\n",
    "    if category == \"foreign\":\n",
    "        cfreq = nltk.ConditionalFreqDist(nltk.bigrams(bigram_foreign))\n",
    "        cprob = nltk.ConditionalProbDist(cfreq, nltk.MLEProbDist)\n",
    "    w = \"<s>\"\n",
    "    temp_tweet = \"\"\n",
    "    for index in range(50):\n",
    "        w = cprob[w].generate()\n",
    "        if w == \"</s>\":\n",
    "            break\n",
    "        else:\n",
    "            temp_tweet += w + \" \"\n",
    "    return temp_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the entire republican agenda if @repmarkmeadows, @jim_jordan and the keeping of optimism in the..... \n",
      "\n",
      "buy health plans.\" in the @wwp #soldierridedc at the jobs! jobs! jobs! jobs! \n",
      "\n",
      "honored true american heroes on the reason you @exxonmobil. \n",
      "\n",
      "met with a beautiful picture! \n",
      "\n",
      "today for immunity in the..... \n",
      "\n",
      "'president trump tower just prior to speak. #nabtu2017_ \n",
      "\n",
      "if @repmarkmeadows, @jim_jordan and top)! \n",
      "\n",
      "it true american hire american are being removed! \n",
      "\n",
      "thank you believe @unionleader newspaper n.h. sent trump organization letter them, received: \n",
      "\n",
      ".@macys one else. #dem leaders can_t compete! \n",
      "\n",
      "the jobs! thank you @jclayfield -- will be president.\" big election loss), by ford today. major things done! \n",
      "\n",
      "we must find leaker now! \n",
      "\n",
      "the boring, rambling non-substantive have saved planned parenthood ocare! \n",
      "\n",
      "jobs, jobs! \n",
      "\n",
      "45,000 construction manufacturing construction jobs in 2018! \n",
      "\n",
      "terrible! just like one program, price will be a serious problem \n",
      "\n",
      "hank greenberg, formerly aig, gave $10 million the @whitehouse._ \n",
      "\n",
      "jobs, jobs! thank you believe @unionleader nh demanding ads? look enclosed letter asking we begin our wonderful guy, i win! \n",
      "\n",
      "if @repmarkmeadows, @jim_jordan and massive tax cut! \n",
      "\n",
      "...design or negotiations yet. when we will only just asking! \n",
      "\n",
      "met with endorsement bush. \n",
      "\n",
      "the democrats will come way i feel sure that congress, the team, fast. we honored to put #americafirst____ \n",
      "\n",
      "general kelly is merely the freedom caucus are talking to put #americafirst____ \n",
      "\n",
      ".@macys one else. #dem leaders can_t compete! \n",
      "\n",
      "despite what you @exxonmobil. \n",
      "\n",
      ".@whitehouse #ceotownhall \n",
      "\n",
      "wow, @foxnews just another terrible decision! \n",
      "\n",
      "the jobs! thank you hear in at the government originally thought, but i feel sure that be president.\" big tax cut! \n",
      "\n",
      "there will be president.\" big tax cut! \n",
      "\n",
      "#thankaveteran_ \n",
      "\n",
      "ron estes is a deal with this legislation, we are being removed! \n",
      "\n",
      "was an election? turned down by media dems, in the_ \n",
      "\n",
      "is unprecedented.\" @fbi \n",
      "\n",
      "thank you @jclayfield -- will hurt the u.s. jobs! jobs! \n",
      "\n",
      "jeb bush, did poorly last night the battlefield. just reporting big tax cuts reform. \n",
      "\n",
      "ted gop. great healthcare is big election loss), by ford today. major investment that president to check server or dems. \n",
      "\n",
      "talks on healthcare rollout. @foxandfriends \n",
      "\n",
      "thank you to russia lifted? did hillary clinton - not long. do not even better as a great state lines, which is: jobs, jobs! \n",
      "\n",
      "for eight years russia lifted? did poorly last year, plunging 46%. disloyal company. another win trump! boycott. \n",
      "\n",
      "good things will come together and astronauts in trump congratulates exxon mobil for americ_ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    tweet = generate_tweet(\"domestic\")\n",
    "    if len(tweet) <= 140:\n",
    "        print(tweet + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after the historic partnership betwe_ \n",
      "\n",
      "spoke to japan. l \n",
      "\n",
      "when a long and i look forward to go to compete), heavily tax our very nice! \n",
      "\n",
      "countries charge u.s. days of refugees admitted into the middle of china will forget! \n",
      "\n",
      "what about searching for representing the syria attack. my prayers and his representatives, at mar-a-lago in germany a hoax. #maga! \n",
      "\n",
      "man shot inside paris police station. announced terror attack in the u.s. charges them nothing or tariffs while the u.s. \n",
      "\n",
      "you to the contact with prime minister shinzo abe is looking for u.s. strongly condemns. i have more flexibility?\" @foxandfriends \n",
      "\n",
      "china ask us if certain people are hosting prime minister abe on behalf of the truth about our very difficult one in egypt. u.s. \n",
      "\n",
      "having a great american, kurt cochran, was formed, but only time will create great honor to the problem without them! u.s.a. \n",
      "\n",
      "a wonderful couple! \n",
      "\n",
      "great time. japan and condolences are weak. will create great incentive others! \n",
      "\n",
      "our products going into.. \n",
      "\n",
      "why isn't the capture our country. new radical islam attacks its people are allowed in the obama white house (mar-a-lago). very nice! \n",
      "\n",
      "why isn't the meeting next week with china will be a terrible campaign. study the middle of our great incentive others! \n",
      "\n",
      "north korea. \n",
      "\n",
      "north korea. \n",
      "\n",
      "the election i'll have president al sisi will deliver! \n",
      "\n",
      "..not associated with his representatives, at mar-a-lago in unprecedented act. \n",
      "\n",
      "watched jeb's ad he was ok to devalue their country anymore! \n",
      "\n",
      "we will deliver! \n",
      "\n",
      "a very well. \n",
      "\n",
      "the problem without them! u.s.a. \n",
      "\n",
      "spoke to devalue their country (the u.s. charges them keep it! \n",
      "\n",
      "our guests in the @whitehouse yes_ \n",
      "\n",
      "melania and i have great... \n",
      "\n",
      "the meeting next week with isis, chinese with prime minister abe of very good talks! \n",
      "\n",
      "#icymi: joint statement with north korea is in palm beach, fla. they charge us! \n",
      "\n",
      "just attacked in germany said just attacked in electoral college lost! \n",
      "\n",
      "iran would be far better for telling the u.s. it is it was killed in international waters - and friendship was sent russia. #sotu \n",
      "\n",
      "watched jeb's ad he was sent russia. trump russia story is nothing or podesta russian speech.... \n",
      "\n",
      "germany said to russian company. trump team spied on the world, so well in that it was a hoax. #maga! \n",
      "\n",
      "we will be a total mess-big crime. get uranium stockpile - never ends! strengthen borders, must change thinking! \n",
      "\n",
      "today with putin. \n",
      "\n",
      "we will we can no longer able to devalue their country is nothing nice about our companies to devalue their country humiliated. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    tweet = generate_tweet(\"foreign\")\n",
    "    if len(tweet) <= 140:\n",
    "        print(tweet + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
