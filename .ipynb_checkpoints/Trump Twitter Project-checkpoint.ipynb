{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keeping Trump on Topic: LIN353C Final Project\n",
    "\n",
    "## By Hannah Brinsko and Aditya Kharosekar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Preprocessing\n",
    "\n",
    "### This involves getting his tweets and cleaning them up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing tweets from the CSV file - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3218, 28)\n",
      "5     2016-09-27T22:13:24\n",
      "8     2016-09-27T21:08:22\n",
      "11    2016-09-27T20:31:14\n",
      "12    2016-09-27T20:14:33\n",
      "13    2016-09-27T20:06:25\n",
      "Name: time, dtype: object\n"
     ]
    }
   ],
   "source": [
    "tweets_csv = pd.read_csv(\"tweets.csv\")\n",
    "trump_tweets = tweets_csv[tweets_csv['handle']==\"realDonaldTrump\"]\n",
    "print(trump_tweets.shape)\n",
    "print(trump_tweets['time'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the CSV file, we see that it contains tweets only up to 09/27/2016. We need his more recent tweets as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting his most recent ~3200 tweets.\n",
    "\n",
    "3200 is approximately the limit to how many tweets Tweepy allows us to scrape. As it turns out, this is more than enough for our use when combined with our CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import json\n",
    "from tweepy import OAuthHandler\n",
    "import codecs\n",
    "\n",
    "consumer_key = \"i387QW7Eqgh12UHmK3VoQO9K5\"\n",
    "consumer_secret = \"BQI8c5eKale4etdA21mawnFqOmAziDQpnThm679V7UtLjbWlMG\"\n",
    "access_token = \"816857419338764288-S8Ay111O2Mo32QAs88tSnv5uKvmGCkF\"\n",
    "access_secret = \"HVU19yLuV0klltJl1fsDibAi7Hiq1U4GwsEV9kozTAc1m\"\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "all_tweets = []\n",
    "\n",
    "new_tweets = api.user_timeline(screen_name=\"realDonaldTrump\", count=200)\n",
    "\n",
    "\n",
    "all_tweets.extend(new_tweets)\n",
    "oldest = all_tweets[-1].id-1\n",
    "\n",
    "t = new_tweets[0];\n",
    "\n",
    "while len(new_tweets) > 0:\n",
    "   new_tweets = api.user_timeline(screen_name = \"realDonaldTrump\", count=200, max_id = oldest)\n",
    "   all_tweets.extend(new_tweets)\n",
    "   oldest = all_tweets[-1].id-1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3222\n"
     ]
    }
   ],
   "source": [
    "print(len(all_tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have his most recent tweets.\n",
    "\n",
    "However, there is significant overlap between the tweets that we have scraped from his account and the tweets that are in the CSV file.\n",
    "\n",
    "The latest tweet in the CSV file was posted on September 27, 2016 at 22:13:24. So, we need to keep any scraped tweets which were posted after this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets = []\n",
    "i = 0\n",
    "while (all_tweets[i].created_at!=datetime.datetime(2016, 9, 27, 22, 13, 24)):\n",
    "    tweets.append(all_tweets[i].text)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "tweets1 = trump_tweets['text']\n",
    "tweets1 = tweets1.tolist()\n",
    "for t in tweets1:\n",
    "    tweets.append(t)\n",
    "print(type(tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making distributional vectors from each tweet\n",
    "\n",
    "But to do that, we need to - \n",
    "1. Remove any twitter links and image links\n",
    "2. Remove any stopwords\n",
    "3. Make sure that we have a list of tweets where each tweet is a string\n",
    "4. Then use CountVectorizer http://scikit-learn.org/stable/modules/feature_extraction.html#common-vectorizer-usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 'are', 'taking', 'action', 'to', '#repealandreplace', '#obamacare!', 'contact', 'your', 'rep', '&amp;', 'tell', 'them', 'you', 'support', '#ahca.', '#passthebill…', 'https://t.co/5lsrdjxfzb']\n",
      "['we', 'are', 'taking', 'action', 'to', '#repealandreplace', '#obamacare!', 'contact', 'your', 'rep', '&amp;', 'tell', 'them', 'you', 'support', '#ahca.', '#passthebill…']\n"
     ]
    }
   ],
   "source": [
    "temp_tweets = []\n",
    "for t in tweets:\n",
    "    temp_tweets.append(t.lower().split())\n",
    "\n",
    "print(temp_tweets[3])\n",
    "for t in temp_tweets:\n",
    "    for w in t:\n",
    "        if w[0:4]==\"http\":\n",
    "            t.remove(w)\n",
    "print(temp_tweets[3]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['taking', 'action', '#repealandreplace', '#obamacare!', 'contact', 'rep', '&amp;', 'tell', 'support', '#ahca.', '#passthebill…']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "for t in temp_tweets:\n",
    "    for w in t:\n",
    "        if w in stop:\n",
    "            t.remove(w)\n",
    "\n",
    "print(temp_tweets[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Succesfully removed stopwords. At this point, each tweet is a list of words and temp_tweets is a list. What we need to use CountVectorizer is a list where each element is a string.\n",
    "\n",
    "Therefore, we need to convert each tweet from a lists of words to a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taking action #repealandreplace #obamacare! contact rep &amp; tell support #ahca. #passthebill…\n"
     ]
    }
   ],
   "source": [
    "tweets = []\n",
    "for t in temp_tweets:\n",
    "    tweets.append(' '.join(t))\n",
    "print(tweets[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##This is where I left off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tweets2 = []\n",
    "# for t in tweets:\n",
    "#     tweets2.append(t.split())\n",
    "\n",
    "# tweets = tweets2 #just for sake of notation\n",
    "\n",
    "# #at this point, tweets is a list. Each element of tweets is a list of words\n",
    "\n",
    "# for tweet in tweets:\n",
    "#     tweet.insert(0, '<s>')\n",
    "#     tweet.append('</s>')\n",
    "    \n",
    "# words = []\n",
    "# for i in range(len(tweets)):\n",
    "#     for word in tweets[i]:\n",
    "#         words.append(word)\n",
    "\n",
    "# # #now, words is a list which consists of all the words Trump has used in his tweets\n",
    "\n",
    "# words = [w.lower() for w in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying his existing tweets into the following categories - \n",
    "1. Foreign policy / international news\n",
    "2. Domestic policy / national news\n",
    "3. Tweets against the media\n",
    "4. Attack tweets\n",
    "5. Agenda / schedule tweets\n",
    "6. Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "freq = nltk.FreqDist(words)\n",
    "print(freq.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cfreq = nltk.ConditionalFreqDist(nltk.bigrams(words))\n",
    "cprob = nltk.ConditionalProbDist(cfreq, nltk.MLEProbDist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w = \"<s>\"\n",
    "for index in range(50):\n",
    "    w = cprob[w].generate()\n",
    "    if w=='</s>':\n",
    "        break\n",
    "    print(w, end = \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying existing tweets into five chosen topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "relevant_tweets = just_text\n",
    "foreign = 'china', 'russia','nato', 'immigrants', 'mexico', 'jobs']\n",
    "domestic = ['democrat', 'democrats', 'senate', 'healthcare', 'jobs', 'tax', 'economy']\n",
    "media = ['fake', 'news', 'cnn', 'reporter']\n",
    "attack = ['sad', 'terrible', 'hillary', 'obama']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = relevant_tweets[24]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(relevant_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.similarity(domestic, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
