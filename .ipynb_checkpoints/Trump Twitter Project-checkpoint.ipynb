{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Keeping Trump on Topic: LIN353C Final Project\n",
    "\n",
    "By Hannah Brinsko and Aditya Kharosekar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aditya Kharosekar\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import csv\n",
    "import datetime\n",
    "import gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocessing - Scraping tweets, and cleaning them up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Importing tweets from the CSV file - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3218, 28)\n",
      "5     2016-09-27T22:13:24\n",
      "8     2016-09-27T21:08:22\n",
      "11    2016-09-27T20:31:14\n",
      "12    2016-09-27T20:14:33\n",
      "13    2016-09-27T20:06:25\n",
      "Name: time, dtype: object\n"
     ]
    }
   ],
   "source": [
    "tweets_csv = pd.read_csv(\"tweets.csv\")\n",
    "trump_tweets = tweets_csv[tweets_csv['handle']==\"realDonaldTrump\"]\n",
    "print(trump_tweets.shape)\n",
    "print(trump_tweets['time'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Looking at the CSV file, we see that it contains tweets only up to 09/27/2016. We need his more recent tweets as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Getting his most recent ~3200 tweets.\n",
    "\n",
    "3200 is approximately the limit to how many tweets Tweepy allows us to scrape. As it turns out, this is more than enough for our use when combined with our CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import json\n",
    "from tweepy import OAuthHandler\n",
    "import codecs\n",
    "\n",
    "consumer_key = \"i387QW7Eqgh12UHmK3VoQO9K5\"\n",
    "consumer_secret = \"BQI8c5eKale4etdA21mawnFqOmAziDQpnThm679V7UtLjbWlMG\"\n",
    "access_token = \"816857419338764288-S8Ay111O2Mo32QAs88tSnv5uKvmGCkF\"\n",
    "access_secret = \"HVU19yLuV0klltJl1fsDibAi7Hiq1U4GwsEV9kozTAc1m\"\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "all_tweets = []\n",
    "\n",
    "new_tweets = api.user_timeline(screen_name=\"realDonaldTrump\", count=200)\n",
    "\n",
    "\n",
    "all_tweets.extend(new_tweets)\n",
    "oldest = all_tweets[-1].id-1\n",
    "\n",
    "t = new_tweets[0];\n",
    "\n",
    "while len(new_tweets) > 0:\n",
    "   new_tweets = api.user_timeline(screen_name = \"realDonaldTrump\", count=200, max_id = oldest)\n",
    "   all_tweets.extend(new_tweets)\n",
    "   oldest = all_tweets[-1].id-1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3244\n"
     ]
    }
   ],
   "source": [
    "print(len(all_tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We now have his most recent tweets.\n",
    "\n",
    "However, there is significant overlap between the tweets that we have scraped from his account and the tweets that are in the CSV file.\n",
    "\n",
    "The latest tweet in the CSV file was posted on September 27, 2016 at 22:13:24. So, we need to keep any scraped tweets which were posted after this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tweets = []\n",
    "i = 0\n",
    "while (all_tweets[i].created_at!=datetime.datetime(2016, 9, 27, 22, 13, 24)):\n",
    "    tweets.append(all_tweets[i].text)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "We have  4694 tweets to work with\n"
     ]
    }
   ],
   "source": [
    "tweets1 = trump_tweets['text']\n",
    "tweets1 = tweets1.tolist()\n",
    "for t in tweets1:\n",
    "    tweets.append(t)\n",
    "print(type(tweets))\n",
    "print(\"We have \", len(tweets), \"tweets to work with\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Making distributional vectors from each tweet\n",
    "\n",
    "But to do that, we need to - \n",
    "1. Remove any twitter links and image links\n",
    "2. Remove any stopwords\n",
    "3. Make sure that we have a list of tweets where each tweet is a string\n",
    "4. Then use CountVectorizer http://scikit-learn.org/stable/modules/feature_extraction.html#common-vectorizer-usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Removing links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['no', 'matter', 'how', 'much', 'i', 'accomplish', 'during', 'the', 'ridiculous', 'standard', 'of', 'the', 'first', '100', 'days,', '&amp;', 'it', 'has', 'been', 'a', 'lot', '(including', 's.c.),', 'media', 'will', 'kill!']\n",
      "['no', 'matter', 'how', 'much', 'i', 'accomplish', 'during', 'the', 'ridiculous', 'standard', 'of', 'the', 'first', '100', 'days,', '&amp;', 'it', 'has', 'been', 'a', 'lot', '(including', 's.c.),', 'media', 'will', 'kill!']\n"
     ]
    }
   ],
   "source": [
    "temp_tweets = []\n",
    "for t in tweets:\n",
    "    temp_tweets.append(t.lower().split())\n",
    "\n",
    "print(temp_tweets[1])\n",
    "for t in temp_tweets:\n",
    "    for w in t:\n",
    "        if \"http\" in w or \"@\" in w: #I've removed any instances where he tags anyone in his tweets. \n",
    "                                    #I thought the word vectors might be too sparse if I left those in.\n",
    "            t.remove(w)\n",
    "print(temp_tweets[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## NOTE: This link removal is not working properly. Have to fix later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "for t in temp_tweets:\n",
    "    for w in t:\n",
    "        if w in stop:\n",
    "            t.remove(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Succesfully removed stopwords. At this point, each tweet is a list of words and temp_tweets is a list. What we need to use CountVectorizer is a list where each element is a string.\n",
    "\n",
    "Therefore, we need to convert each tweet from a lists of words to a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = []\n",
    "for t in temp_tweets:\n",
    "    tweets.append(' '.join(t))\n",
    "type(tweets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Our methodology for classifying tweets\n",
    "\n",
    "Step 0 - Download a pre-trained Word2Vec model. We tried training our own model, but we did not have enough data.\n",
    "\n",
    "Step 1 - Hand tag some number of tweets (we ended up tagging about 280 tweets) and classify them into the following categories - \n",
    "1. Foreign Policy / International News\n",
    "2. Domestic Policy / domestic news\n",
    "3. Tweets about the media\n",
    "4. Attack tweets\n",
    "5. Other tweets\n",
    "6. Tweets about the election\n",
    "\n",
    "Step 2 - From our hand-tagged corpus, and for each category, create a list of words used.\n",
    "\n",
    "Step 3 - Create a word vector for each category by summing up the individual word vectors\n",
    "\n",
    "Step 4 - For each subsequent tweet, find cosine similarity between it and each category vector. Assign that tweet to the category it is most similar to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Step 0 - Downloading a pre-trained Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 400000  words loaded\n"
     ]
    }
   ],
   "source": [
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r', encoding = 'utf8')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = [float(val) for val in splitLine[1:]]\n",
    "        model[word] = embedding\n",
    "    print(\"Done.\",len(model),\" words loaded\")\n",
    "    return model\n",
    "\n",
    "glove_model = loadGloveModel('glove.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.14675,\n",
       " 1.1692,\n",
       " 0.69416,\n",
       " -0.061429,\n",
       " -0.13677,\n",
       " 0.42015,\n",
       " -0.716,\n",
       " 0.019014,\n",
       " -0.52896,\n",
       " -0.83643,\n",
       " -1.8561,\n",
       " -0.18324,\n",
       " 0.057648,\n",
       " -0.31188,\n",
       " 0.024997,\n",
       " 0.045878,\n",
       " -0.098728,\n",
       " -0.21451,\n",
       " 0.14298,\n",
       " -0.0080809,\n",
       " -0.14569,\n",
       " 0.38326,\n",
       " 0.63811,\n",
       " -0.46426,\n",
       " 1.0953,\n",
       " -2.15,\n",
       " -0.18462,\n",
       " 0.1738,\n",
       " -0.50607,\n",
       " 0.00057719,\n",
       " 0.52828,\n",
       " 0.6685,\n",
       " -0.89692,\n",
       " -0.34346,\n",
       " -0.15456,\n",
       " -0.97313,\n",
       " -0.69441,\n",
       " 0.59201,\n",
       " -1.2194,\n",
       " -1.3469,\n",
       " -0.25691,\n",
       " 0.34537,\n",
       " -0.43824,\n",
       " -0.096233,\n",
       " 0.29882,\n",
       " -0.29174,\n",
       " -0.47201,\n",
       " -0.32221,\n",
       " 0.079279,\n",
       " 0.59419]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_model['hillary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "google_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.24023438, -0.046875  , -0.05786133, -0.17285156,  0.13476562,\n",
       "       -0.03466797,  0.05957031, -0.02209473,  0.00334167, -0.03564453,\n",
       "       -0.04589844,  0.04248047, -0.09570312,  0.21582031, -0.12597656,\n",
       "       -0.06835938,  0.15332031,  0.17773438, -0.03662109,  0.03515625,\n",
       "        0.04418945,  0.28320312,  0.05297852, -0.01953125, -0.27929688,\n",
       "       -0.23828125,  0.00238037, -0.04345703,  0.26367188,  0.06591797,\n",
       "       -0.02624512,  0.03369141,  0.02880859, -0.15332031,  0.11083984,\n",
       "       -0.046875  , -0.02355957,  0.01000977,  0.23632812, -0.07421875,\n",
       "        0.27734375, -0.14746094,  0.02478027,  0.10351562, -0.33007812,\n",
       "        0.0050354 , -0.04736328,  0.16699219,  0.015625  ,  0.30859375,\n",
       "        0.15039062, -0.09472656,  0.08349609,  0.05883789, -0.17578125,\n",
       "       -0.00273132, -0.04101562, -0.30859375, -0.15332031, -0.05200195,\n",
       "       -0.19140625,  0.13476562, -0.28515625, -0.06445312, -0.00058365,\n",
       "        0.01348877, -0.00527954,  0.10498047,  0.20605469,  0.01538086,\n",
       "        0.06445312,  0.13574219,  0.05737305, -0.05541992,  0.02648926,\n",
       "        0.02868652,  0.33007812,  0.0246582 , -0.08642578, -0.0625    ,\n",
       "       -0.05834961, -0.25390625,  0.01055908, -0.20019531,  0.02770996,\n",
       "        0.15820312, -0.38867188, -0.06005859,  0.24414062,  0.09472656,\n",
       "        0.12695312, -0.14746094, -0.08300781, -0.10253906, -0.03540039,\n",
       "       -0.2421875 ,  0.03637695,  0.0057373 ,  0.265625  ,  0.31835938,\n",
       "       -0.25976562, -0.15429688, -0.06079102,  0.14941406,  0.00765991,\n",
       "       -0.09716797, -0.07226562, -0.01306152, -0.03955078, -0.01245117,\n",
       "       -0.140625  , -0.13964844,  0.01080322,  0.10253906,  0.11816406,\n",
       "       -0.31640625, -0.05371094,  0.06225586, -0.01977539,  0.15039062,\n",
       "       -0.05664062,  0.03564453,  0.11669922,  0.04956055, -0.171875  ,\n",
       "        0.11621094,  0.16601562, -0.0378418 , -0.03149414, -0.10986328,\n",
       "        0.03515625, -0.18359375,  0.03759766, -0.36914062, -0.18847656,\n",
       "        0.03833008,  0.03588867,  0.07763672, -0.01123047, -0.01904297,\n",
       "        0.09912109,  0.11328125,  0.02050781, -0.12792969,  0.16015625,\n",
       "       -0.23242188,  0.15722656, -0.13867188, -0.12792969,  0.0234375 ,\n",
       "        0.2734375 , -0.078125  ,  0.30078125, -0.11914062,  0.35742188,\n",
       "       -0.0703125 , -0.04394531, -0.20507812,  0.16308594, -0.10888672,\n",
       "       -0.05419922, -0.00585938,  0.27734375,  0.22363281, -0.02185059,\n",
       "       -0.33984375,  0.21386719, -0.28125   ,  0.19824219, -0.06982422,\n",
       "        0.03930664, -0.12695312,  0.04272461, -0.23925781,  0.08398438,\n",
       "        0.09912109,  0.04467773,  0.02246094, -0.06835938,  0.02282715,\n",
       "       -0.07373047,  0.04589844, -0.15234375,  0.08154297,  0.06884766,\n",
       "       -0.13183594, -0.29296875,  0.13085938, -0.03051758,  0.07324219,\n",
       "       -0.09375   ,  0.00778198,  0.22558594,  0.296875  , -0.21679688,\n",
       "        0.06884766, -0.01019287,  0.22167969, -0.10302734,  0.14941406,\n",
       "       -0.03417969,  0.22460938,  0.09472656, -0.06494141,  0.24414062,\n",
       "        0.04321289,  0.08154297, -0.06347656, -0.05981445, -0.02392578,\n",
       "       -0.08642578,  0.03686523, -0.24414062,  0.12402344,  0.11816406,\n",
       "        0.00180817, -0.01470947,  0.12109375, -0.15039062, -0.02612305,\n",
       "       -0.10058594,  0.07128906,  0.29492188,  0.02038574,  0.19238281,\n",
       "        0.25195312,  0.05224609,  0.16015625,  0.140625  , -0.13964844,\n",
       "        0.04321289,  0.12060547,  0.01098633, -0.09423828, -0.06542969,\n",
       "        0.01977539, -0.03222656,  0.1484375 ,  0.05737305, -0.30859375,\n",
       "       -0.1171875 , -0.03491211,  0.06835938, -0.03857422, -0.06347656,\n",
       "        0.03491211,  0.00891113,  0.17382812,  0.05151367,  0.03466797,\n",
       "       -0.04736328, -0.13964844, -0.12207031, -0.13085938,  0.02844238,\n",
       "       -0.04736328,  0.16503906,  0.30664062,  0.07958984, -0.2890625 ,\n",
       "        0.02941895,  0.04614258,  0.07519531,  0.13378906, -0.30664062,\n",
       "       -0.04174805, -0.23535156, -0.13964844, -0.23535156,  0.15722656,\n",
       "        0.01623535, -0.03051758,  0.0703125 , -0.13085938, -0.05664062,\n",
       "       -0.12988281,  0.23046875,  0.03881836, -0.0559082 , -0.0201416 ,\n",
       "       -0.08398438, -0.1953125 ,  0.09277344,  0.05444336, -0.03442383,\n",
       "       -0.11474609, -0.23046875,  0.10058594,  0.26953125, -0.18066406,\n",
       "       -0.18554688, -0.22363281, -0.06054688, -0.25195312,  0.02832031,\n",
       "       -0.28710938,  0.25      , -0.14355469, -0.140625  ,  0.03222656], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_model['campaign']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "google_model is our pre-trained model which we will be using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Step 1 - Hand tagging tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Using a .csv file of a number of hand tagged tweets, we place the tweets into the preselected categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "foreign = []\n",
    "domestic =[]\n",
    "media =[]\n",
    "attack = []\n",
    "election = []\n",
    "other = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "f = open('tagged_tweets.csv')\n",
    "csv_f = csv.reader(f)\n",
    "\n",
    "for row in csv_f:\n",
    "    tweet = row[0]\n",
    "    cats = row[1]\n",
    "    if \"1\" in cats:\n",
    "        foreign.append(tweet)\n",
    "    elif \"2\" in cats:\n",
    "        domestic.append(tweet)\n",
    "    elif \"3\" in cats:\n",
    "        media.append(tweet)\n",
    "    elif \"5\" in cats:\n",
    "        other.append(tweet)\n",
    "    elif \"6\" in cats:\n",
    "        election.append(tweet)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domestic:  104\n",
      "Foreign:  56\n",
      "Media:  83\n",
      "Other:  95\n",
      "Election:  79\n"
     ]
    }
   ],
   "source": [
    "print(\"Domestic: \",len(domestic))\n",
    "print(\"Foreign: \", len(foreign))\n",
    "print(\"Media: \", len(media))\n",
    "print(\"Other: \",len(other))\n",
    "print(\"Election: \",len(election))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Step 2 - Making a list of words used in each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Each tweet is a string right now.\n",
    "#This function will split up the string into individual words, remove any words which start with @\n",
    "#(i.e our generated tweets won't have any tags) and remove any punctuation\n",
    "\n",
    "def clean_up(tweets):\n",
    "    tweets1 = []\n",
    "    for t in tweets:\n",
    "        tweets1.append(t.split())\n",
    "        \n",
    "    tweets_words = []\n",
    "    for t in tweets1:\n",
    "        for w in t:\n",
    "            tweets_words.append(w)\n",
    "    tweets_words = tweets_words\n",
    "    \n",
    "    #removing '@' from any word which has it. The google_model does not have any words which start with @\n",
    "    temp_words = []\n",
    "    for word in tweets_words:\n",
    "        if word[0]=='@':\n",
    "            temp_words.append(word[1:])\n",
    "        else:\n",
    "            temp_words.append(word)\n",
    "    return temp_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "domestic_words = clean_up(domestic)\n",
    "foreign_words = clean_up(foreign)\n",
    "media_words = clean_up(media)\n",
    "election_words = clean_up(election)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(words):\n",
    "    x = [''.join(c for c in s if c not in string.punctuation) for s in words]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "domestic_words = remove_punctuation(domestic_words)\n",
    "foreign_words = remove_punctuation(foreign_words)\n",
    "media_words = remove_punctuation(media_words)\n",
    "election_words = remove_punctuation(election_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def remove_short_words(words):\n",
    "    temp_words = []\n",
    "    for word in words:\n",
    "        if len(word) >=4:\n",
    "            temp_words.append(word)\n",
    "    return temp_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "d_short_words = remove_short_words(domestic_words)\n",
    "f_short_words = remove_short_words(foreign_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thank',\n",
       " 'to',\n",
       " 'law',\n",
       " 'enforcement',\n",
       " 'officers',\n",
       " 'lesm',\n",
       " 'trump2016',\n",
       " 'jeb',\n",
       " 'bush',\n",
       " 'did',\n",
       " 'poorly',\n",
       " 'last',\n",
       " 'night',\n",
       " 'the',\n",
       " 'debate',\n",
       " 'whose',\n",
       " 'chances',\n",
       " 'winning',\n",
       " 'zero',\n",
       " 'got',\n",
       " 'graham',\n",
       " 'endorsement',\n",
       " 'graham',\n",
       " 'quit',\n",
       " 'o',\n",
       " 'sen',\n",
       " 'lindsey',\n",
       " 'graham',\n",
       " 'embarrassed',\n",
       " 'with',\n",
       " 'failed',\n",
       " 'run',\n",
       " 'president',\n",
       " 'now',\n",
       " 'embarrasses',\n",
       " 'with',\n",
       " 'endorsement',\n",
       " 'bush',\n",
       " 'will',\n",
       " 'the',\n",
       " 'greatest',\n",
       " 'jobproducing',\n",
       " 'president',\n",
       " 'american',\n",
       " 'history',\n",
       " 'trump2016',\n",
       " 'votetrump',\n",
       " 'httpstcooc480lwvqg',\n",
       " 'How',\n",
       " 'low',\n",
       " 'has',\n",
       " 'President',\n",
       " 'Obama',\n",
       " 'gone',\n",
       " 'to',\n",
       " 'tapp',\n",
       " 'my',\n",
       " 'phones',\n",
       " 'during',\n",
       " 'the',\n",
       " 'very',\n",
       " 'sacred',\n",
       " 'election',\n",
       " 'process',\n",
       " 'This',\n",
       " 'is',\n",
       " 'NixonWatergate',\n",
       " 'Bad',\n",
       " 'or',\n",
       " 'sick',\n",
       " 'guy',\n",
       " 'Id',\n",
       " 'bet',\n",
       " 'a',\n",
       " 'good',\n",
       " 'lawyer',\n",
       " 'could',\n",
       " 'make',\n",
       " 'a',\n",
       " 'great',\n",
       " 'case',\n",
       " 'out',\n",
       " 'of',\n",
       " 'the',\n",
       " 'fact',\n",
       " 'that',\n",
       " 'President',\n",
       " 'Obama',\n",
       " 'was',\n",
       " 'tapping',\n",
       " 'my',\n",
       " 'phones',\n",
       " 'in',\n",
       " 'October',\n",
       " 'just',\n",
       " 'prior',\n",
       " 'to',\n",
       " 'Election',\n",
       " 'Is',\n",
       " 'it',\n",
       " 'legal',\n",
       " 'for',\n",
       " 'a',\n",
       " 'sitting',\n",
       " 'President',\n",
       " 'to',\n",
       " 'be',\n",
       " 'wire',\n",
       " 'tapping',\n",
       " 'a',\n",
       " 'race',\n",
       " 'for',\n",
       " 'president',\n",
       " 'prior',\n",
       " 'to',\n",
       " 'an',\n",
       " 'election',\n",
       " 'Turned',\n",
       " 'down',\n",
       " 'by',\n",
       " 'court',\n",
       " 'earlier',\n",
       " 'A',\n",
       " 'NEW',\n",
       " 'LOW',\n",
       " 'tedcruz',\n",
       " 'eligibility',\n",
       " 'be',\n",
       " 'president',\n",
       " 'settled',\n",
       " 'law',\n",
       " 'says',\n",
       " 'cruz',\n",
       " 'constitutional',\n",
       " 'law',\n",
       " 'professor',\n",
       " 'laurencetribe',\n",
       " 'highly',\n",
       " 'respected',\n",
       " 'constitutional',\n",
       " 'law',\n",
       " 'professor',\n",
       " 'mary',\n",
       " 'brigid',\n",
       " 'mcmanamon',\n",
       " 'just',\n",
       " 'stated',\n",
       " 'ted',\n",
       " 'cruz',\n",
       " 'not',\n",
       " 'eligible',\n",
       " 'be',\n",
       " 'president',\n",
       " 'big',\n",
       " 'problem',\n",
       " 'united',\n",
       " 'states',\n",
       " 'looks',\n",
       " 'and',\n",
       " 'like',\n",
       " 'paper',\n",
       " 'tiger',\n",
       " 'wont',\n",
       " 'that',\n",
       " 'way',\n",
       " 'i',\n",
       " 'win',\n",
       " 'hank',\n",
       " 'greenberg',\n",
       " 'formerly',\n",
       " 'aig',\n",
       " 'gave',\n",
       " '10',\n",
       " 'million',\n",
       " 'the',\n",
       " 'jebbush',\n",
       " 'campaign',\n",
       " '3',\n",
       " 'months',\n",
       " 'ago',\n",
       " 'is',\n",
       " 'happy',\n",
       " 'total',\n",
       " 'waste',\n",
       " 'money',\n",
       " 'sotu',\n",
       " 'speech',\n",
       " 'really',\n",
       " 'boring',\n",
       " 'slow',\n",
       " 'lethargic',\n",
       " '',\n",
       " 'hard',\n",
       " 'watch',\n",
       " 'state',\n",
       " 'the',\n",
       " 'union',\n",
       " 'speech',\n",
       " 'one',\n",
       " 'the',\n",
       " 'boring',\n",
       " 'rambling',\n",
       " 'nonsubstantive',\n",
       " 'have',\n",
       " 'heard',\n",
       " 'a',\n",
       " 'long',\n",
       " 'time',\n",
       " 'new',\n",
       " 'leadership',\n",
       " 'fast',\n",
       " 'Why',\n",
       " 'doesnt',\n",
       " 'Fake',\n",
       " 'News',\n",
       " 'talk',\n",
       " 'about',\n",
       " 'Podesta',\n",
       " 'ties',\n",
       " 'to',\n",
       " 'Russia',\n",
       " 'as',\n",
       " 'covered',\n",
       " 'by',\n",
       " 'FoxNews',\n",
       " 'or',\n",
       " 'money',\n",
       " 'from',\n",
       " 'Russia',\n",
       " 'to',\n",
       " 'Clinton',\n",
       " '',\n",
       " 'sale',\n",
       " 'of',\n",
       " 'Uranium',\n",
       " 'Despite',\n",
       " 'what',\n",
       " 'you',\n",
       " 'hear',\n",
       " 'in',\n",
       " 'the',\n",
       " 'press',\n",
       " 'healthcare',\n",
       " 'is',\n",
       " 'coming',\n",
       " 'along',\n",
       " 'great',\n",
       " 'We',\n",
       " 'are',\n",
       " 'talking',\n",
       " 'to',\n",
       " 'many',\n",
       " 'groups',\n",
       " 'and',\n",
       " 'it',\n",
       " 'will',\n",
       " 'end',\n",
       " 'in',\n",
       " 'a',\n",
       " 'beautiful',\n",
       " 'picture',\n",
       " 'Dont',\n",
       " 'let',\n",
       " 'the',\n",
       " 'FAKE',\n",
       " 'NEWS',\n",
       " 'tell',\n",
       " 'you',\n",
       " 'that',\n",
       " 'there',\n",
       " 'is',\n",
       " 'big',\n",
       " 'infighting',\n",
       " 'in',\n",
       " 'the',\n",
       " 'Trump',\n",
       " 'Admin',\n",
       " 'We',\n",
       " 'are',\n",
       " 'getting',\n",
       " 'along',\n",
       " 'great',\n",
       " 'and',\n",
       " 'getting',\n",
       " 'major',\n",
       " 'things',\n",
       " 'done',\n",
       " 'Dont',\n",
       " 'worry',\n",
       " 'getting',\n",
       " 'rid',\n",
       " 'of',\n",
       " 'state',\n",
       " 'lines',\n",
       " 'which',\n",
       " 'will',\n",
       " 'promote',\n",
       " 'competition',\n",
       " 'will',\n",
       " 'be',\n",
       " 'in',\n",
       " 'phase',\n",
       " '2',\n",
       " 'amp',\n",
       " '3',\n",
       " 'of',\n",
       " 'healthcare',\n",
       " 'rollout',\n",
       " 'foxandfriends',\n",
       " 'For',\n",
       " 'eight',\n",
       " 'years',\n",
       " 'Russia',\n",
       " 'ran',\n",
       " 'over',\n",
       " 'President',\n",
       " 'Obama',\n",
       " 'got',\n",
       " 'stronger',\n",
       " 'and',\n",
       " 'stronger',\n",
       " 'pickedoff',\n",
       " 'Crimea',\n",
       " 'and',\n",
       " 'added',\n",
       " 'missiles',\n",
       " 'Weak',\n",
       " 'foxandfriends',\n",
       " 'Terrible',\n",
       " 'Just',\n",
       " 'found',\n",
       " 'out',\n",
       " 'that',\n",
       " 'Obama',\n",
       " 'had',\n",
       " 'my',\n",
       " 'wires',\n",
       " 'tapped',\n",
       " 'in',\n",
       " 'Trump',\n",
       " 'Tower',\n",
       " 'just',\n",
       " 'before',\n",
       " 'the',\n",
       " 'victory',\n",
       " 'Nothing',\n",
       " 'found',\n",
       " 'This',\n",
       " 'is',\n",
       " 'McCarthyism',\n",
       " 'The',\n",
       " 'first',\n",
       " 'meeting',\n",
       " 'Jeff',\n",
       " 'Sessions',\n",
       " 'had',\n",
       " 'with',\n",
       " 'the',\n",
       " 'Russian',\n",
       " 'Amb',\n",
       " 'was',\n",
       " 'set',\n",
       " 'up',\n",
       " 'by',\n",
       " 'the',\n",
       " 'Obama',\n",
       " 'Administration',\n",
       " 'under',\n",
       " 'education',\n",
       " 'program',\n",
       " 'for',\n",
       " '100',\n",
       " 'Ambs',\n",
       " 'today',\n",
       " 'honored',\n",
       " 'true',\n",
       " 'american',\n",
       " 'heroes',\n",
       " 'the',\n",
       " 'firstever',\n",
       " 'national',\n",
       " 'vietnam',\n",
       " 'war',\n",
       " 'veterans',\n",
       " 'day',\n",
       " 'thankaveteran',\n",
       " 'you',\n",
       " 'believe',\n",
       " 'unionleader',\n",
       " 'nh',\n",
       " 'demanding',\n",
       " 'ads',\n",
       " 'look',\n",
       " 'enclosed',\n",
       " 'letter',\n",
       " 'them',\n",
       " 'received',\n",
       " 'failing',\n",
       " 'unionleader',\n",
       " 'newspaper',\n",
       " 'nh',\n",
       " 'sent',\n",
       " 'trump',\n",
       " 'organization',\n",
       " 'letter',\n",
       " 'asking',\n",
       " 'we',\n",
       " 'take',\n",
       " 'ads',\n",
       " 'stupid',\n",
       " 'desperate',\n",
       " 'macys',\n",
       " 'one',\n",
       " 'the',\n",
       " 'worst',\n",
       " 'performing',\n",
       " 'stocks',\n",
       " 'the',\n",
       " 'sampp',\n",
       " 'last',\n",
       " 'year',\n",
       " 'plunging',\n",
       " '46',\n",
       " 'disloyal',\n",
       " 'company',\n",
       " 'another',\n",
       " 'win',\n",
       " 'trump',\n",
       " 'boycott',\n",
       " 'was',\n",
       " 'very',\n",
       " 'wise',\n",
       " 'move',\n",
       " 'ted',\n",
       " 'cruz',\n",
       " 'renounced',\n",
       " 'canadian',\n",
       " 'citizenship',\n",
       " '18',\n",
       " 'months',\n",
       " 'ago',\n",
       " 'senator',\n",
       " 'john',\n",
       " 'mccain',\n",
       " 'certainly',\n",
       " 'friend',\n",
       " 'ted',\n",
       " 'sentedcruz',\n",
       " 'tedfree',\n",
       " 'legal',\n",
       " 'advice',\n",
       " 'how',\n",
       " 'preempt',\n",
       " 'dems',\n",
       " 'citizen',\n",
       " 'issue',\n",
       " 'go',\n",
       " 'court',\n",
       " 'amp',\n",
       " 'seek',\n",
       " 'declaratory',\n",
       " 'judgmentyou',\n",
       " 'win',\n",
       " 'good',\n",
       " 'news',\n",
       " 'jeb',\n",
       " 'bush',\n",
       " 'hope',\n",
       " 'workers',\n",
       " 'demand',\n",
       " 'their',\n",
       " 'teamsters',\n",
       " 'reps',\n",
       " 'endorse',\n",
       " 'donald',\n",
       " 'j',\n",
       " 'trump',\n",
       " 'nobody',\n",
       " 'knows',\n",
       " 'jobs',\n",
       " 'like',\n",
       " 'do',\n",
       " 'dont',\n",
       " 'let',\n",
       " 'sell',\n",
       " 'out',\n",
       " 'love',\n",
       " 'seeing',\n",
       " 'union',\n",
       " 'amp',\n",
       " 'nonunion',\n",
       " 'members',\n",
       " 'alike',\n",
       " 'defecting',\n",
       " 'trump',\n",
       " 'will',\n",
       " 'create',\n",
       " 'jobs',\n",
       " 'like',\n",
       " 'one',\n",
       " 'else',\n",
       " 'dem',\n",
       " 'leaders',\n",
       " 'cant',\n",
       " 'compete',\n",
       " 'constitutional',\n",
       " 'law',\n",
       " 'expert',\n",
       " 'laurence',\n",
       " 'tribe',\n",
       " 'harvard',\n",
       " 'says',\n",
       " 'wrong',\n",
       " 'say',\n",
       " 'natural',\n",
       " 'born',\n",
       " 'citizen',\n",
       " 'a',\n",
       " 'settled',\n",
       " 'matterit',\n",
       " 'isnt',\n",
       " 'settled',\n",
       " 'stuartpstevens',\n",
       " 'horrible',\n",
       " 'advise',\n",
       " 'mitt',\n",
       " 'romney',\n",
       " 'made',\n",
       " 'victory',\n",
       " 'impossibility',\n",
       " 'dont',\n",
       " 'blame',\n",
       " 'mitt',\n",
       " 'stevens',\n",
       " 'cant',\n",
       " 'get',\n",
       " 'job',\n",
       " 'a',\n",
       " 'serious',\n",
       " 'problem',\n",
       " 'ted',\n",
       " 'amp',\n",
       " 'gop',\n",
       " 'great',\n",
       " 'doubt',\n",
       " 'dems',\n",
       " 'sue',\n",
       " 'lets',\n",
       " 'work',\n",
       " 'together',\n",
       " 'solve',\n",
       " 'problem',\n",
       " 'freejessejames',\n",
       " 'read',\n",
       " 'complete',\n",
       " 'statement',\n",
       " 'are',\n",
       " 'amazing',\n",
       " 'guy',\n",
       " 'amp',\n",
       " 'really',\n",
       " 'appreciate',\n",
       " 'words',\n",
       " 'amp',\n",
       " 'support',\n",
       " 'will',\n",
       " 'see',\n",
       " 'soon',\n",
       " 'ted',\n",
       " 'cruz',\n",
       " 'complains',\n",
       " 'my',\n",
       " 'views',\n",
       " 'eminent',\n",
       " 'domain',\n",
       " 'without',\n",
       " 'we',\n",
       " 'wouldnt',\n",
       " 'roads',\n",
       " 'highways',\n",
       " 'airports',\n",
       " 'schools',\n",
       " 'even',\n",
       " 'pipelines',\n",
       " 'Ron',\n",
       " 'Estes',\n",
       " 'is',\n",
       " 'running',\n",
       " 'TODAY',\n",
       " 'for',\n",
       " 'Congress',\n",
       " 'in',\n",
       " 'the',\n",
       " 'Great',\n",
       " 'State',\n",
       " 'of',\n",
       " 'Kansas',\n",
       " 'A',\n",
       " 'wonderful',\n",
       " 'guy',\n",
       " 'I',\n",
       " 'need',\n",
       " 'his',\n",
       " 'help',\n",
       " 'on',\n",
       " 'Healthcare',\n",
       " 'amp',\n",
       " 'Tax',\n",
       " 'Cuts',\n",
       " 'Reform',\n",
       " 'Congratulations',\n",
       " 'to',\n",
       " 'Justice',\n",
       " 'Neil',\n",
       " 'Gorsuch',\n",
       " 'on',\n",
       " 'his',\n",
       " 'elevation',\n",
       " 'to',\n",
       " 'the',\n",
       " 'United',\n",
       " 'States',\n",
       " 'Supreme',\n",
       " 'Court',\n",
       " 'A',\n",
       " 'great',\n",
       " 'day',\n",
       " 'for',\n",
       " 'Americ',\n",
       " 'httpstcoRM9lfTaePS',\n",
       " 'Thank',\n",
       " 'you',\n",
       " 'USNavy',\n",
       " 'USA',\n",
       " 'httpstcooD7L8vPgjq',\n",
       " 'Judge',\n",
       " 'Gorsuch',\n",
       " 'will',\n",
       " 'be',\n",
       " 'sworn',\n",
       " 'in',\n",
       " 'at',\n",
       " 'the',\n",
       " 'Rose',\n",
       " 'Garden',\n",
       " 'of',\n",
       " 'the',\n",
       " 'White',\n",
       " 'House',\n",
       " 'on',\n",
       " 'Monday',\n",
       " 'at',\n",
       " '1100',\n",
       " 'AM',\n",
       " 'He',\n",
       " 'will',\n",
       " 'be',\n",
       " 'a',\n",
       " 'great',\n",
       " 'Justice',\n",
       " 'Very',\n",
       " 'proud',\n",
       " 'of',\n",
       " 'him',\n",
       " 'The',\n",
       " 'reason',\n",
       " 'you',\n",
       " 'dont',\n",
       " 'generally',\n",
       " 'hit',\n",
       " 'runways',\n",
       " 'is',\n",
       " 'that',\n",
       " 'they',\n",
       " 'are',\n",
       " 'easy',\n",
       " 'and',\n",
       " 'inexpensive',\n",
       " 'to',\n",
       " 'quickly',\n",
       " 'fix',\n",
       " 'fill',\n",
       " 'in',\n",
       " 'and',\n",
       " 'top',\n",
       " 'It',\n",
       " 'was',\n",
       " 'an',\n",
       " 'honor',\n",
       " 'to',\n",
       " 'host',\n",
       " 'our',\n",
       " 'American',\n",
       " 'heroes',\n",
       " 'from',\n",
       " 'the',\n",
       " 'WWP',\n",
       " 'SoldierRideDC',\n",
       " 'at',\n",
       " 'the',\n",
       " 'WhiteHouse',\n",
       " 'today',\n",
       " 'with',\n",
       " 'FLOTUS',\n",
       " 'VP',\n",
       " 'httpstcou5AI1pupVV',\n",
       " 'JOBS',\n",
       " 'JOBS',\n",
       " 'JOBS',\n",
       " 'Thank',\n",
       " 'you',\n",
       " 'Sean',\n",
       " 'McGarvey',\n",
       " 'amp',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'Governing',\n",
       " 'Board',\n",
       " 'of',\n",
       " 'Presidents',\n",
       " 'for',\n",
       " 'honoring',\n",
       " 'me',\n",
       " 'wan',\n",
       " 'invite',\n",
       " 'to',\n",
       " 'speak',\n",
       " 'NABTU2017',\n",
       " 'httpstcodJlZvlq6Tj',\n",
       " 'WhiteHouse',\n",
       " 'CEOTownHall',\n",
       " 'Looking',\n",
       " 'forward',\n",
       " 'to',\n",
       " 'hosting',\n",
       " 'our',\n",
       " 'heroes',\n",
       " 'from',\n",
       " 'the',\n",
       " 'Wounded',\n",
       " 'Warrior',\n",
       " 'Project',\n",
       " 'WWP',\n",
       " 'Soldier',\n",
       " 'Ride',\n",
       " 'to',\n",
       " 'the',\n",
       " 'WhiteHouse',\n",
       " 'on',\n",
       " 'Th',\n",
       " 'httpstcoQLC0qFD94x',\n",
       " 'FoxNews',\n",
       " 'from',\n",
       " 'multiple',\n",
       " 'sources',\n",
       " 'There',\n",
       " 'was',\n",
       " 'electronic',\n",
       " 'surveillance',\n",
       " 'of',\n",
       " 'Trump',\n",
       " 'and',\n",
       " 'people',\n",
       " 'close',\n",
       " 'to',\n",
       " 'Trump',\n",
       " 'This',\n",
       " 'is',\n",
       " 'unprecedented',\n",
       " 'FBI',\n",
       " 'Did',\n",
       " 'Hillary',\n",
       " 'Clinton',\n",
       " 'ever',\n",
       " 'apologize',\n",
       " 'for',\n",
       " 'receiving',\n",
       " 'the',\n",
       " 'answers',\n",
       " 'to',\n",
       " 'the',\n",
       " 'debate',\n",
       " 'Just',\n",
       " 'asking',\n",
       " 'Was',\n",
       " 'the',\n",
       " 'brother',\n",
       " 'of',\n",
       " 'John',\n",
       " 'Podesta',\n",
       " 'paid',\n",
       " 'big',\n",
       " 'money',\n",
       " 'to',\n",
       " 'get',\n",
       " 'the',\n",
       " 'sanctions',\n",
       " 'on',\n",
       " 'Russia',\n",
       " 'lifted',\n",
       " 'Did',\n",
       " 'Hillary',\n",
       " 'know',\n",
       " 'The',\n",
       " 'real',\n",
       " 'story',\n",
       " 'turns',\n",
       " 'out',\n",
       " 'to',\n",
       " 'be',\n",
       " 'SURVEILLANCE',\n",
       " 'and',\n",
       " 'LEAKING',\n",
       " 'Find',\n",
       " 'the',\n",
       " 'leakers',\n",
       " 'Talks',\n",
       " 'on',\n",
       " 'Repealing',\n",
       " 'and',\n",
       " 'Replacing',\n",
       " 'ObamaCare',\n",
       " 'are',\n",
       " 'and',\n",
       " 'have',\n",
       " 'been',\n",
       " 'going',\n",
       " 'on',\n",
       " 'and',\n",
       " 'will',\n",
       " 'continue',\n",
       " 'until',\n",
       " 'such',\n",
       " 'time',\n",
       " 'as',\n",
       " 'a',\n",
       " 'deal',\n",
       " 'is',\n",
       " 'hopefully',\n",
       " 'struck',\n",
       " 'Thank',\n",
       " 'you',\n",
       " 'JCLayfield',\n",
       " '',\n",
       " 'will',\n",
       " 'get',\n",
       " 'even',\n",
       " 'better',\n",
       " 'as',\n",
       " 'my',\n",
       " 'Administration',\n",
       " 'continues',\n",
       " 'to',\n",
       " 'put',\n",
       " 'AmericaFirst',\n",
       " 'httpstcoAQQzmt10x7',\n",
       " 'Wow',\n",
       " 'FoxNews',\n",
       " 'just',\n",
       " 'reporting',\n",
       " 'big',\n",
       " 'news',\n",
       " 'Source',\n",
       " 'Official',\n",
       " 'behind',\n",
       " 'unmasking',\n",
       " 'is',\n",
       " 'high',\n",
       " 'up',\n",
       " 'Known',\n",
       " 'Intel',\n",
       " 'official',\n",
       " 'is',\n",
       " 'responsible',\n",
       " 'Some',\n",
       " 'unmasked',\n",
       " 'use',\n",
       " 'subsidies',\n",
       " 'to',\n",
       " 'buy',\n",
       " 'health',\n",
       " 'plans',\n",
       " 'In',\n",
       " 'other',\n",
       " 'words',\n",
       " 'Ocare',\n",
       " 'is',\n",
       " 'dead',\n",
       " 'Good',\n",
       " 'things',\n",
       " 'will',\n",
       " 'happen',\n",
       " 'however',\n",
       " 'either',\n",
       " 'with',\n",
       " 'Republicans',\n",
       " 'or',\n",
       " 'Dems',\n",
       " 'WeeklyAddress',\n",
       " 'ConfirmGorsuch',\n",
       " 'httpstcotP4bkvTOBq',\n",
       " 'ConfirmGorsuch',\n",
       " 'SCOTUS',\n",
       " 'httpstcoWkqHYMcYa3',\n",
       " 'Great',\n",
       " 'meeting',\n",
       " 'with',\n",
       " 'a',\n",
       " 'wonderful',\n",
       " 'woman',\n",
       " 'today',\n",
       " 'former',\n",
       " 'Secretary',\n",
       " 'of',\n",
       " 'State',\n",
       " 'Condoleezza',\n",
       " 'Rice',\n",
       " 'USA',\n",
       " 'httpstcoZuriIC3YwG',\n",
       " 'Mike',\n",
       " 'Flynn',\n",
       " 'should',\n",
       " 'ask',\n",
       " 'for',\n",
       " 'immunity',\n",
       " 'in',\n",
       " 'that',\n",
       " 'this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'witch',\n",
       " 'hunt',\n",
       " 'excuse',\n",
       " 'for',\n",
       " 'big',\n",
       " 'election',\n",
       " 'loss',\n",
       " 'by',\n",
       " 'media',\n",
       " 'amp',\n",
       " 'Dems',\n",
       " 'of',\n",
       " 'historic',\n",
       " 'proportion',\n",
       " 'and',\n",
       " 'job',\n",
       " 'losses',\n",
       " 'American',\n",
       " 'companies',\n",
       " 'must',\n",
       " 'be',\n",
       " 'prepared',\n",
       " 'to',\n",
       " 'look',\n",
       " 'at',\n",
       " 'other',\n",
       " 'alternatives',\n",
       " 'Where',\n",
       " 'are',\n",
       " 'RepMarkMeadows',\n",
       " 'JimJordan',\n",
       " 'and',\n",
       " 'RaulLabrador',\n",
       " 'RepealANDReplace',\n",
       " 'Obamacare',\n",
       " 'If',\n",
       " 'RepMarkMeadows',\n",
       " 'JimJordan',\n",
       " 'and',\n",
       " 'RaulLabrador',\n",
       " 'would',\n",
       " 'get',\n",
       " 'on',\n",
       " 'board',\n",
       " 'we',\n",
       " 'would',\n",
       " 'have',\n",
       " 'both',\n",
       " 'great',\n",
       " 'healthcare',\n",
       " 'and',\n",
       " 'massive',\n",
       " 'tax',\n",
       " 'cuts',\n",
       " 'amp',\n",
       " 'reform',\n",
       " 'Great',\n",
       " 'oped',\n",
       " 'from',\n",
       " 'RepKenBuck',\n",
       " 'Looks',\n",
       " 'like',\n",
       " 'some',\n",
       " 'in',\n",
       " 'the',\n",
       " 'Freedom',\n",
       " 'Caucus',\n",
       " 'are',\n",
       " 'helping',\n",
       " 'me',\n",
       " ...]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domestic_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Step 3 - Create a category vector by adding up individual word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_category_vector(words):\n",
    "    vector = np.ones(300)\n",
    "    for word in words:\n",
    "        try:\n",
    "            vector = vector + google_model[word]\n",
    "        except KeyError: #some words are not in model. I don't want to pre-process everything so I'm just handling each exception\n",
    "            pass\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "domestic_vector = create_category_vector(domestic_words)\n",
    "foreign_vector = create_category_vector(foreign_words)\n",
    "media_vector = create_category_vector(media_words)\n",
    "election_vector = create_category_vector(election_words)\n",
    "obamacare = create_category_vector(\"Health\")\n",
    "isis = create_category_vector(\"Russia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'military building is rapidly becoming stronger ever before. frankly, have choice!'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_tweet_vector(tweet):\n",
    "    vector = np.ones(300)\n",
    "    for word in tweet:\n",
    "        try:\n",
    "            vector = vector + google_model[word]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def calc_cosine_similarity(tweet_vector, category_vector):\n",
    "    return cosine_similarity(tweet_vector, category_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "specific_foreign = []\n",
    "specific_domestic = []\n",
    "for word in foreign_words:\n",
    "    if word not in domestic_words:\n",
    "        specific_foreign.append(word)\n",
    "for word in domestic_words:\n",
    "    if word not in foreign_words:\n",
    "        specific_domestic.append(word)\n",
    "        \n",
    "specific_dshort = []\n",
    "specific_fshort = []\n",
    "for word in d_short_words:\n",
    "    if word not in f_short_words:\n",
    "        specific_fshort.append(word)\n",
    "for word in f_short_words:\n",
    "    if word not in d_short_words:\n",
    "        specific_dshort.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def calc_scores(tweet):\n",
    "#     score = calc_cosine_similarity(create_tweet_vector(tweet), specific_domestic_vector)\n",
    "#     print(\"Domestic:\", score)\n",
    "    score = calc_cosine_similarity(create_tweet_vector(tweet), dshort_vector)\n",
    "    print(\"Domestic Long words:\", score)\n",
    "#     score = calc_cosine_similarity(create_tweet_vector(tweet), specific_foreign_vector)\n",
    "#     print(\"Foreign: \",score)\n",
    "    score = calc_cosine_similarity(create_tweet_vector(tweet), fshort_vector)\n",
    "    print(\"Foreign Long words:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "specific_foreign = clean_up(specific_foreign)\n",
    "specific_domestic = clean_up(specific_domestic)\n",
    "\n",
    "specific_foreign = remove_punctuation(specific_foreign)\n",
    "specific_domestic = remove_punctuation(specific_domestic)\n",
    "\n",
    "specific_foreign_vector = create_category_vector(specific_foreign)\n",
    "specific_domestic_vector = create_category_vector(specific_domestic)\n",
    "dshort_vector = create_category_vector(specific_dshort)\n",
    "fshort_vector = create_category_vector(specific_fshort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matter much accomplish the ridiculous standard the first 100 days, &amp; has a lot (including s.c.), media kill!\n",
      "Domestic Long words: [[ 0.29918605]]\n",
      "Foreign Long words: [[ 0.29549825]]\n",
      "None\n",
      "\n",
      "another terrorist attack paris. people france not take much of this. have big effect presidential election!\n",
      "Domestic Long words: [[ 0.29141667]]\n",
      "Foreign Long words: [[ 0.29708134]]\n",
      "None\n",
      "\n",
      "rt nyt editor apologizes misleading tweet new england patriots' visit the white house (via h…\n",
      "Domestic Long words: [[ 0.2990066]]\n",
      "Foreign Long words: [[ 0.30049044]]\n",
      "None\n",
      "\n",
      "great honor host pm paolo gentiloni italy the white house afternoon! #icymi- joint press conference…\n",
      "Domestic Long words: [[ 0.28118862]]\n",
      "Foreign Long words: [[ 0.28845017]]\n",
      "None\n",
      "\n",
      "we're going use american steel, we're going use american labor, are going come first all deals. ➡️…\n",
      "Domestic Long words: [[ 0.27667859]]\n",
      "Foreign Long words: [[ 0.28249208]]\n",
      "None\n",
      "\n",
      "failing has calling wrong two years, got caught a big lie concerning new england patriots visit w.h.\n",
      "Domestic Long words: [[ 0.27259411]]\n",
      "Foreign Long words: [[ 0.28455217]]\n",
      "None\n",
      "\n",
      "great honor host champion new england the white house today. congratulations!…\n",
      "Domestic Long words: [[ 0.27786309]]\n",
      "Foreign Long words: [[ 0.2831806]]\n",
      "None\n",
      "\n",
      "today signed veterans (our heroes) choice program extension &amp; improvement act #s544 watch…\n",
      "Domestic Long words: [[ 0.29590612]]\n",
      "Foreign Long words: [[ 0.29886312]]\n",
      "None\n",
      "\n",
      "#buyamericanhireamerican🇺🇸\n",
      "Domestic Long words: [[ 0.25435742]]\n",
      "Foreign Long words: [[ 0.25202713]]\n",
      "None\n",
      "\n",
      "dems failed kansas are failing georgia. great job karen handel! is hollywood vs. georgia june 20th.\n",
      "Domestic Long words: [[ 0.27802266]]\n",
      "Foreign Long words: [[ 0.28664618]]\n",
      "None\n",
      "\n",
      "despite major outside money, fake media support eleven republican candidates, big \"r\" win runoff georgia. glad be help!\n",
      "Domestic Long words: [[ 0.290282]]\n",
      "Foreign Long words: [[ 0.29759387]]\n",
      "None\n",
      "\n",
      "#buyamericanhireamerican🇺🇸 https://t.co/rf9aivvb7g\n",
      "Domestic Long words: [[ 0.29491603]]\n",
      "Foreign Long words: [[ 0.29409241]]\n",
      "None\n",
      "\n",
      "learned jon is running congress georgia, doesn't even live the district. republicans, get and vote!\n",
      "Domestic Long words: [[ 0.2893886]]\n",
      "Foreign Long words: [[ 0.29766572]]\n",
      "None\n",
      "\n",
      "republicans must get today vote georgia 6. force runoff easy win! dem ossoff raise taxes-very bad crime &amp; 2nd a.\n",
      "Domestic Long words: [[ 0.29968303]]\n",
      "Foreign Long words: [[ 0.29975131]]\n",
      "None\n",
      "\n",
      "democrat jon ossoff would a disaster congress. weak crime illegal immigration, bad jobs wants higher taxes. say\n",
      "Domestic Long words: [[ 0.28631866]]\n",
      "Foreign Long words: [[ 0.28814971]]\n",
      "None\n",
      "\n",
      "will interviewed by starting 6:00 a.m. enjoy!\n",
      "Domestic Long words: [[ 0.28524549]]\n",
      "Foreign Long words: [[ 0.28604789]]\n",
      "None\n",
      "\n",
      "weak illegal immigration policies the obama admin. allowed bad ms 13 gangs form cities across u.s. are removing fast!\n",
      "Domestic Long words: [[ 0.29437532]]\n",
      "Foreign Long words: [[ 0.29504404]]\n",
      "None\n",
      "\n",
      "eleven republican candidates running georgia (on tuesday) congress, runoff be win. vote \"r\" lower taxes &amp; safety!\n",
      "Domestic Long words: [[ 0.29058689]]\n",
      "Foreign Long words: [[ 0.29951272]]\n",
      "None\n",
      "\n",
      "see tomorrow wisconsin! 'trump spurs small-business optimism milwaukee area'\n",
      "Domestic Long words: [[ 0.30325642]]\n",
      "Foreign Long words: [[ 0.30247403]]\n",
      "None\n",
      "\n",
      "trump approval hits 50%\n",
      "Domestic Long words: [[ 0.27318544]]\n",
      "Foreign Long words: [[ 0.26393235]]\n",
      "None\n",
      "\n",
      "rt trump approval hits 50%\n",
      "Domestic Long words: [[ 0.28050039]]\n",
      "Foreign Long words: [[ 0.26968777]]\n",
      "None\n",
      "\n",
      "super liberal democrat the georgia congressioal race tomorrow wants protect criminals, allow illegal immigration raise taxes!\n",
      "Domestic Long words: [[ 0.28322914]]\n",
      "Foreign Long words: [[ 0.28574324]]\n",
      "None\n",
      "\n",
      "fake media (not real media) gotten even worse since election. every story badly slanted. have hold to truth!\n",
      "Domestic Long words: [[ 0.29252476]]\n",
      "Foreign Long words: [[ 0.29518089]]\n",
      "None\n",
      "\n",
      "great book your reading enjoyment: \"reasons vote democrats\" michael j. knowles.\n",
      "Domestic Long words: [[ 0.27697612]]\n",
      "Foreign Long words: [[ 0.28531241]]\n",
      "None\n",
      "\n",
      "\"the first 90 days my presidency exposed total failure the last eight years foreign policy!\" true.\n",
      "Domestic Long words: [[ 0.29347808]]\n",
      "Foreign Long words: [[ 0.29198024]]\n",
      "None\n",
      "\n",
      "recent kansas election (congress) a really big media event, republicans won. they play the same game georgia-bad!\n",
      "Domestic Long words: [[ 0.28003035]]\n",
      "Foreign Long words: [[ 0.28761966]]\n",
      "None\n",
      "\n",
      "military building is rapidly becoming stronger ever before. frankly, have choice!\n",
      "Domestic Long words: [[ 0.28049151]]\n",
      "Foreign Long words: [[ 0.28638869]]\n",
      "None\n",
      "\n",
      "someone look who paid small organized rallies yesterday. the election over!\n",
      "Domestic Long words: [[ 0.27750409]]\n",
      "Foreign Long words: [[ 0.28224203]]\n",
      "None\n",
      "\n",
      "did was almost impossible thing do a republican-easily the electoral college! tax returns brought again?\n",
      "Domestic Long words: [[ 0.28782002]]\n",
      "Foreign Long words: [[ 0.29104534]]\n",
      "None\n",
      "\n",
      "happy easter everyone!\n",
      "Domestic Long words: [[ 0.2351699]]\n",
      "Foreign Long words: [[ 0.23442249]]\n",
      "None\n",
      "\n",
      "would call china currency manipulator they working us the north korean problem? will see happens!\n",
      "Domestic Long words: [[ 0.27697189]]\n",
      "Foreign Long words: [[ 0.2895278]]\n",
      "None\n",
      "\n",
      "rt looking forward hosting annual easter egg roll the monday!\n",
      "Domestic Long words: [[ 0.26543814]]\n",
      "Foreign Long words: [[ 0.27176402]]\n",
      "None\n",
      "\n",
      "weekly address- https://t.co/b2nqzj53ft\n",
      "Domestic Long words: [[ 0.27973115]]\n",
      "Foreign Long words: [[ 0.2793935]]\n",
      "None\n",
      "\n",
      "rt great again: feds arrest murder suspect 'fast furious' scandal...\n",
      "Domestic Long words: [[ 0.29890021]]\n",
      "Foreign Long words: [[ 0.29133028]]\n",
      "None\n",
      "\n",
      "was great honor welcome atlanta's heroic first responders the white house afternoon!\n",
      "Domestic Long words: [[ 0.28518963]]\n",
      "Foreign Long words: [[ 0.2864684]]\n",
      "None\n",
      "\n",
      "things work fine the u.s.a. russia. the right time everyone come their senses &amp; will lasting peace!\n",
      "Domestic Long words: [[ 0.29754482]]\n",
      "Foreign Long words: [[ 0.29941869]]\n",
      "None\n",
      "\n",
      "have great confidence china properly deal north korea. they unable do so, u.s., its allies, will! u.s.a.\n",
      "Domestic Long words: [[ 0.28468296]]\n",
      "Foreign Long words: [[ 0.29297202]]\n",
      "None\n",
      "\n",
      "jobs returning, illegal immigration plummeting, law, order justice being restored. are truly making america great again!\n",
      "Domestic Long words: [[ 0.28548011]]\n",
      "Foreign Long words: [[ 0.28926166]]\n",
      "None\n",
      "\n",
      "one one keeping promises - the border, energy, jobs, regulations. big changes are happening!\n",
      "Domestic Long words: [[ 0.26805208]]\n",
      "Foreign Long words: [[ 0.28067815]]\n",
      "None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in tweets[1:40]:\n",
    "    print(t)\n",
    "    print(calc_scores(t))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o\n",
      "Domestic Long words: [[ 0.01014544]]\n",
      "Foreign Long words: [[-0.00438806]]\n",
      "None\n",
      "\n",
      "sen\n",
      "Domestic Long words: [[ 0.05965827]]\n",
      "Foreign Long words: [[ 0.04615618]]\n",
      "None\n",
      "\n",
      "lindsey\n",
      "Domestic Long words: [[ 0.14880492]]\n",
      "Foreign Long words: [[ 0.13815822]]\n",
      "None\n",
      "\n",
      "graham\n",
      "Domestic Long words: [[ 0.08706938]]\n",
      "Foreign Long words: [[ 0.06181411]]\n",
      "None\n",
      "\n",
      "embarrassed\n",
      "Domestic Long words: [[ 0.18554745]]\n",
      "Foreign Long words: [[ 0.1607589]]\n",
      "None\n",
      "\n",
      "failed\n",
      "Domestic Long words: [[ 0.13266373]]\n",
      "Foreign Long words: [[ 0.11580782]]\n",
      "None\n",
      "\n",
      "run\n",
      "Domestic Long words: [[ 0.0693841]]\n",
      "Foreign Long words: [[ 0.05718254]]\n",
      "None\n",
      "\n",
      "president\n",
      "Domestic Long words: [[ 0.19738317]]\n",
      "Foreign Long words: [[ 0.18692844]]\n",
      "None\n",
      "\n",
      "embarrasses\n",
      "Domestic Long words: [[ 0.17816068]]\n",
      "Foreign Long words: [[ 0.15297143]]\n",
      "None\n",
      "\n",
      "endorsement\n",
      "Domestic Long words: [[ 0.20781486]]\n",
      "Foreign Long words: [[ 0.19515333]]\n",
      "None\n",
      "\n",
      "bush\n",
      "Domestic Long words: [[ 0.10439766]]\n",
      "Foreign Long words: [[ 0.08620976]]\n",
      "None\n",
      "\n",
      "greatest\n",
      "Domestic Long words: [[ 0.16548235]]\n",
      "Foreign Long words: [[ 0.14669458]]\n",
      "None\n",
      "\n",
      "jobproducing\n",
      "Domestic Long words: [[ 0.19836624]]\n",
      "Foreign Long words: [[ 0.20385969]]\n",
      "None\n",
      "\n",
      "president\n",
      "Domestic Long words: [[ 0.19738317]]\n",
      "Foreign Long words: [[ 0.18692844]]\n",
      "None\n",
      "\n",
      "american\n",
      "Domestic Long words: [[ 0.13431886]]\n",
      "Foreign Long words: [[ 0.11977673]]\n",
      "None\n",
      "\n",
      "history\n",
      "Domestic Long words: [[ 0.15587868]]\n",
      "Foreign Long words: [[ 0.13750003]]\n",
      "None\n",
      "\n",
      "trump2016\n",
      "Domestic Long words: [[ 0.18960783]]\n",
      "Foreign Long words: [[ 0.17248241]]\n",
      "None\n",
      "\n",
      "votetrump\n",
      "Domestic Long words: [[ 0.22195072]]\n",
      "Foreign Long words: [[ 0.20550801]]\n",
      "None\n",
      "\n",
      "httpstcooc480lwvqg\n",
      "Domestic Long words: [[ 0.24437544]]\n",
      "Foreign Long words: [[ 0.24349128]]\n",
      "None\n",
      "\n",
      "How\n",
      "Domestic Long words: [[ 0.06339867]]\n",
      "Foreign Long words: [[ 0.05576682]]\n",
      "None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in specific_domestic[20:40]:\n",
    "    print(t)\n",
    "    print(calc_scores(t))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
