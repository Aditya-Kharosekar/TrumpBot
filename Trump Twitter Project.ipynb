{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Keeping Trump on Topic: LIN353C Final Project\n",
    "\n",
    "By Hannah Brinsko and Aditya Kharosekar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import csv\n",
    "import datetime\n",
    "import gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocessing - Scraping tweets, and cleaning them up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Importing tweets from the CSV file - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3218, 28)\n",
      "5     2016-09-27T22:13:24\n",
      "8     2016-09-27T21:08:22\n",
      "11    2016-09-27T20:31:14\n",
      "12    2016-09-27T20:14:33\n",
      "13    2016-09-27T20:06:25\n",
      "Name: time, dtype: object\n"
     ]
    }
   ],
   "source": [
    "tweets_csv = pd.read_csv(\"tweets.csv\")\n",
    "trump_tweets = tweets_csv[tweets_csv['handle']==\"realDonaldTrump\"]\n",
    "print(trump_tweets.shape)\n",
    "print(trump_tweets['time'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Looking at the CSV file, we see that it contains tweets only up to 09/27/2016. We need his more recent tweets as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Getting his most recent ~3200 tweets.\n",
    "\n",
    "3200 is approximately the limit to how many tweets Tweepy allows us to scrape. As it turns out, this is more than enough for our use when combined with our CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import json\n",
    "from tweepy import OAuthHandler\n",
    "import codecs\n",
    "\n",
    "consumer_key = \"i387QW7Eqgh12UHmK3VoQO9K5\"\n",
    "consumer_secret = \"BQI8c5eKale4etdA21mawnFqOmAziDQpnThm679V7UtLjbWlMG\"\n",
    "access_token = \"816857419338764288-S8Ay111O2Mo32QAs88tSnv5uKvmGCkF\"\n",
    "access_secret = \"HVU19yLuV0klltJl1fsDibAi7Hiq1U4GwsEV9kozTAc1m\"\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "all_tweets = []\n",
    "\n",
    "new_tweets = api.user_timeline(screen_name=\"realDonaldTrump\", count=200)\n",
    "\n",
    "\n",
    "all_tweets.extend(new_tweets)\n",
    "oldest = all_tweets[-1].id-1\n",
    "\n",
    "t = new_tweets[0];\n",
    "\n",
    "while len(new_tweets) > 0:\n",
    "   new_tweets = api.user_timeline(screen_name = \"realDonaldTrump\", count=200, max_id = oldest)\n",
    "   all_tweets.extend(new_tweets)\n",
    "   oldest = all_tweets[-1].id-1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3221\n"
     ]
    }
   ],
   "source": [
    "print(len(all_tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We now have his most recent tweets.\n",
    "\n",
    "However, there is significant overlap between the tweets that we have scraped from his account and the tweets that are in the CSV file.\n",
    "\n",
    "The latest tweet in the CSV file was posted on September 27, 2016 at 22:13:24. So, we need to keep any scraped tweets which were posted after this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tweets = []\n",
    "i = 0\n",
    "while (all_tweets[i].created_at!=datetime.datetime(2016, 9, 27, 22, 13, 24)):\n",
    "    tweets.append(all_tweets[i].text)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "We have  4722 tweets to work with\n"
     ]
    }
   ],
   "source": [
    "tweets1 = trump_tweets['text']\n",
    "tweets1 = tweets1.tolist()\n",
    "for t in tweets1:\n",
    "    tweets.append(t)\n",
    "print(type(tweets))\n",
    "print(\"We have \", len(tweets), \"tweets to work with\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Making distributional vectors from each tweet\n",
    "\n",
    "But to do that, we need to - \n",
    "1. Remove any twitter links and image links\n",
    "2. Remove any stopwords\n",
    "3. Make sure that we have a list of tweets where each tweet is a string\n",
    "4. Then use CountVectorizer http://scikit-learn.org/stable/modules/feature_extraction.html#common-vectorizer-usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Removing links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"don't\", 'let', 'the', 'fake', 'media', 'tell', 'you', 'that', 'i', 'have', 'changed', 'my', 'position', 'on', 'the', 'wall.', 'it', 'will', 'get', 'built', 'and', 'help', 'stop', 'drugs,', 'human', 'trafficking', 'etc.']\n",
      "[\"don't\", 'let', 'the', 'fake', 'media', 'tell', 'you', 'that', 'i', 'have', 'changed', 'my', 'position', 'on', 'the', 'wall.', 'it', 'will', 'get', 'built', 'and', 'help', 'stop', 'drugs,', 'human', 'trafficking', 'etc.']\n"
     ]
    }
   ],
   "source": [
    "temp_tweets = []\n",
    "for t in tweets:\n",
    "    temp_tweets.append(t.lower().split())\n",
    "\n",
    "print(temp_tweets[1])\n",
    "for t in temp_tweets:\n",
    "    for w in t:\n",
    "        if \"http\" in w or \"@\" in w: #I've removed any instances where he tags anyone in his tweets. \n",
    "                                    #I thought the word vectors might be too sparse if I left those in.\n",
    "            t.remove(w)\n",
    "print(temp_tweets[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## NOTE: This link removal is not working properly. Have to fix later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "for t in temp_tweets:\n",
    "    for w in t:\n",
    "        if w in stop:\n",
    "            t.remove(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Succesfully removed stopwords. At this point, each tweet is a list of words and temp_tweets is a list. What we need to use CountVectorizer is a list where each element is a string.\n",
    "\n",
    "Therefore, we need to convert each tweet from a lists of words to a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = []\n",
    "for t in temp_tweets:\n",
    "    tweets.append(' '.join(t))\n",
    "type(tweets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Our methodology for classifying tweets\n",
    "\n",
    "Step 0 - Download a pre-trained Word2Vec model. We tried training our own model, but we did not have enough data.\n",
    "\n",
    "Step 1 - Hand tag some number of tweets (we ended up tagging about 280 tweets) and classify them into the following categories - \n",
    "1. Foreign Policy / International News\n",
    "2. Domestic Policy / domestic news\n",
    "3. Tweets about the media\n",
    "4. Attack tweets\n",
    "5. Other tweets\n",
    "6. Tweets about the election\n",
    "\n",
    "Step 2 - From our hand-tagged corpus, and for each category, create a list of words used.\n",
    "\n",
    "Step 3 - Create a word vector for each category by summing up the individual word vectors\n",
    "\n",
    "Step 4 - For each subsequent tweet, find cosine similarity between it and each category vector. Assign that tweet to the category it is most similar to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Step 0 - Downloading a pre-trained Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "google_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.24023438, -0.046875  , -0.05786133, -0.17285156,  0.13476562,\n",
       "       -0.03466797,  0.05957031, -0.02209473,  0.00334167, -0.03564453,\n",
       "       -0.04589844,  0.04248047, -0.09570312,  0.21582031, -0.12597656,\n",
       "       -0.06835938,  0.15332031,  0.17773438, -0.03662109,  0.03515625,\n",
       "        0.04418945,  0.28320312,  0.05297852, -0.01953125, -0.27929688,\n",
       "       -0.23828125,  0.00238037, -0.04345703,  0.26367188,  0.06591797,\n",
       "       -0.02624512,  0.03369141,  0.02880859, -0.15332031,  0.11083984,\n",
       "       -0.046875  , -0.02355957,  0.01000977,  0.23632812, -0.07421875,\n",
       "        0.27734375, -0.14746094,  0.02478027,  0.10351562, -0.33007812,\n",
       "        0.0050354 , -0.04736328,  0.16699219,  0.015625  ,  0.30859375,\n",
       "        0.15039062, -0.09472656,  0.08349609,  0.05883789, -0.17578125,\n",
       "       -0.00273132, -0.04101562, -0.30859375, -0.15332031, -0.05200195,\n",
       "       -0.19140625,  0.13476562, -0.28515625, -0.06445312, -0.00058365,\n",
       "        0.01348877, -0.00527954,  0.10498047,  0.20605469,  0.01538086,\n",
       "        0.06445312,  0.13574219,  0.05737305, -0.05541992,  0.02648926,\n",
       "        0.02868652,  0.33007812,  0.0246582 , -0.08642578, -0.0625    ,\n",
       "       -0.05834961, -0.25390625,  0.01055908, -0.20019531,  0.02770996,\n",
       "        0.15820312, -0.38867188, -0.06005859,  0.24414062,  0.09472656,\n",
       "        0.12695312, -0.14746094, -0.08300781, -0.10253906, -0.03540039,\n",
       "       -0.2421875 ,  0.03637695,  0.0057373 ,  0.265625  ,  0.31835938,\n",
       "       -0.25976562, -0.15429688, -0.06079102,  0.14941406,  0.00765991,\n",
       "       -0.09716797, -0.07226562, -0.01306152, -0.03955078, -0.01245117,\n",
       "       -0.140625  , -0.13964844,  0.01080322,  0.10253906,  0.11816406,\n",
       "       -0.31640625, -0.05371094,  0.06225586, -0.01977539,  0.15039062,\n",
       "       -0.05664062,  0.03564453,  0.11669922,  0.04956055, -0.171875  ,\n",
       "        0.11621094,  0.16601562, -0.0378418 , -0.03149414, -0.10986328,\n",
       "        0.03515625, -0.18359375,  0.03759766, -0.36914062, -0.18847656,\n",
       "        0.03833008,  0.03588867,  0.07763672, -0.01123047, -0.01904297,\n",
       "        0.09912109,  0.11328125,  0.02050781, -0.12792969,  0.16015625,\n",
       "       -0.23242188,  0.15722656, -0.13867188, -0.12792969,  0.0234375 ,\n",
       "        0.2734375 , -0.078125  ,  0.30078125, -0.11914062,  0.35742188,\n",
       "       -0.0703125 , -0.04394531, -0.20507812,  0.16308594, -0.10888672,\n",
       "       -0.05419922, -0.00585938,  0.27734375,  0.22363281, -0.02185059,\n",
       "       -0.33984375,  0.21386719, -0.28125   ,  0.19824219, -0.06982422,\n",
       "        0.03930664, -0.12695312,  0.04272461, -0.23925781,  0.08398438,\n",
       "        0.09912109,  0.04467773,  0.02246094, -0.06835938,  0.02282715,\n",
       "       -0.07373047,  0.04589844, -0.15234375,  0.08154297,  0.06884766,\n",
       "       -0.13183594, -0.29296875,  0.13085938, -0.03051758,  0.07324219,\n",
       "       -0.09375   ,  0.00778198,  0.22558594,  0.296875  , -0.21679688,\n",
       "        0.06884766, -0.01019287,  0.22167969, -0.10302734,  0.14941406,\n",
       "       -0.03417969,  0.22460938,  0.09472656, -0.06494141,  0.24414062,\n",
       "        0.04321289,  0.08154297, -0.06347656, -0.05981445, -0.02392578,\n",
       "       -0.08642578,  0.03686523, -0.24414062,  0.12402344,  0.11816406,\n",
       "        0.00180817, -0.01470947,  0.12109375, -0.15039062, -0.02612305,\n",
       "       -0.10058594,  0.07128906,  0.29492188,  0.02038574,  0.19238281,\n",
       "        0.25195312,  0.05224609,  0.16015625,  0.140625  , -0.13964844,\n",
       "        0.04321289,  0.12060547,  0.01098633, -0.09423828, -0.06542969,\n",
       "        0.01977539, -0.03222656,  0.1484375 ,  0.05737305, -0.30859375,\n",
       "       -0.1171875 , -0.03491211,  0.06835938, -0.03857422, -0.06347656,\n",
       "        0.03491211,  0.00891113,  0.17382812,  0.05151367,  0.03466797,\n",
       "       -0.04736328, -0.13964844, -0.12207031, -0.13085938,  0.02844238,\n",
       "       -0.04736328,  0.16503906,  0.30664062,  0.07958984, -0.2890625 ,\n",
       "        0.02941895,  0.04614258,  0.07519531,  0.13378906, -0.30664062,\n",
       "       -0.04174805, -0.23535156, -0.13964844, -0.23535156,  0.15722656,\n",
       "        0.01623535, -0.03051758,  0.0703125 , -0.13085938, -0.05664062,\n",
       "       -0.12988281,  0.23046875,  0.03881836, -0.0559082 , -0.0201416 ,\n",
       "       -0.08398438, -0.1953125 ,  0.09277344,  0.05444336, -0.03442383,\n",
       "       -0.11474609, -0.23046875,  0.10058594,  0.26953125, -0.18066406,\n",
       "       -0.18554688, -0.22363281, -0.06054688, -0.25195312,  0.02832031,\n",
       "       -0.28710938,  0.25      , -0.14355469, -0.140625  ,  0.03222656], dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_model['campaign']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "google_model is our pre-trained model which we will be using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Step 1 - Hand tagging tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Using a .csv file of a number of hand tagged tweets, we place the tweets into the preselected categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "foreign = []\n",
    "domestic =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "f = open('tagged_tweets.csv')\n",
    "csv_f = csv.reader(f)\n",
    "\n",
    "for row in csv_f:\n",
    "    tweet = row[0]\n",
    "    cats = row[1]\n",
    "    if \"1\" in cats:\n",
    "        foreign.append(tweet)\n",
    "    elif \"2\" in cats:\n",
    "        domestic.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domestic:  104\n",
      "Foreign:  56\n",
      "Media:  83\n",
      "Other:  95\n",
      "Election:  79\n"
     ]
    }
   ],
   "source": [
    "print(\"Domestic: \",len(domestic))\n",
    "print(\"Foreign: \", len(foreign))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Step 2 - Making a list of words used in each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Each tweet is a string right now.\n",
    "#This function will split up the string into individual words, remove any words which start with @\n",
    "#(i.e our generated tweets won't have any tags) and remove any punctuation\n",
    "\n",
    "def clean_up(tweets):\n",
    "    tweets1 = []\n",
    "    for t in tweets:\n",
    "        tweets1.append(t.split())\n",
    "        \n",
    "    tweets_words = []\n",
    "    for t in tweets1:\n",
    "        for w in t:\n",
    "            tweets_words.append(w)\n",
    "    tweets_words = tweets_words\n",
    "    \n",
    "    #removing '@' from any word which has it. The google_model does not have any words which start with @\n",
    "    temp_words = []\n",
    "    for word in tweets_words:\n",
    "        if word[0]=='@':\n",
    "            temp_words.append(word[1:])\n",
    "        else:\n",
    "            temp_words.append(word)\n",
    "    return temp_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "domestic_words = clean_up(domestic)\n",
    "foreign_words = clean_up(foreign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(words):\n",
    "    x = [''.join(c for c in s if c not in string.punctuation) for s in words]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "domestic_words = remove_punctuation(domestic_words)\n",
    "foreign_words = remove_punctuation(foreign_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we generate tweets, we will use these words as the basis for our bigram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def remove_short_words(words, length):\n",
    "    temp_words = []\n",
    "    for word in words:\n",
    "        if len(word)>=length:\n",
    "            temp_words.append(word)\n",
    "    return temp_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "d_short_words = remove_short_words(domestic_words, 4)\n",
    "f_short_words = remove_short_words(foreign_words, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Step 3 - Create a category vector by adding up individual word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_vector(words):\n",
    "    vector = np.ones(300)\n",
    "    for word in words:\n",
    "        try:\n",
    "            vector = vector + google_model[word]\n",
    "        except KeyError: #some words are not in model. I don't want to pre-process everything so I'm just handling each exception\n",
    "            pass\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "domestic_vector = create_vector(domestic_words)\n",
    "foreign_vector = create_vector(foreign_words)\n",
    "media_vector = create_vector(media_words)\n",
    "election_vector = create_vector(election_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'welcome home, aya! #godblesstheusaüá∫üá∏'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_vector(tweet):\n",
    "    vector = np.ones(300)\n",
    "    for word in tweet:\n",
    "        try:\n",
    "            vector = vector + google_model[word]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "specific_foreign = []\n",
    "specific_domestic = []\n",
    "for word in foreign_words:\n",
    "    if word not in domestic_words:\n",
    "        specific_foreign.append(word)\n",
    "for word in domestic_words:\n",
    "    if word not in foreign_words:\n",
    "        specific_domestic.append(word)\n",
    "\n",
    "specific_dshort = []\n",
    "specific_fshort = []\n",
    "for word in d_short_words:\n",
    "    #if word not in f_short_words and not in stop:\n",
    "    if word not in stop:\n",
    "        specific_fshort.append(word)\n",
    "for word in f_short_words:\n",
    "    #if word not in d_short_words:\n",
    "    if word not in stop:\n",
    "        specific_dshort.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dshort_vector = create_vector(specific_dshort)\n",
    "fshort_vector = create_vector(specific_fshort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "domestic_tags = ['press', 'media', 'election', 'healthcare', 'Obamacare', 'obamacare','american', 'immigrant', 'immigrants',\n",
    "                'Committee', 'wall','Wall', 'jobs', 'taxes', 'senate', 'congress', 'dems','drugs']\n",
    "foreign_tags = ['Russia', 'russia', 'China', 'trade', 'mexico', 'terrorist', 'terrorists', 'terrorism',\n",
    "               'migrants', 'immigration','Immigration', 'President', 'Egypt', 'Syria', 'Minister', 'Ambassador','Korea','war']\n",
    "\n",
    "def count_tag_occurrences(tweet):\n",
    "    dcount= 0\n",
    "    fcount = 0\n",
    "    tweet = tweet.split()\n",
    "    tweet = remove_punctuation(tweet)\n",
    "    for word in tweet:\n",
    "        if word in domestic_tags:\n",
    "            dcount+=1\n",
    "        if word in foreign_tags:\n",
    "            fcount+=1\n",
    "    return dcount, fcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def calc_cosine_similarity(tweet_vector, category_vector):\n",
    "    return cosine_similarity(tweet_vector, category_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def calc_scores(tweet):\n",
    "    dcount, fcount = count_tag_occurrences(tweet)\n",
    "    dscore = calc_cosine_similarity(create_tweet_vector(tweet), dshort_vector)\n",
    "    fscore = calc_cosine_similarity(create_tweet_vector(tweet), fshort_vector)\n",
    "    return dscore, fscore, dcount, fcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def model(dom, fore):\n",
    "    dcount = 0\n",
    "    fcount = 0\n",
    "    total = 0\n",
    "    for tweet in dom:\n",
    "        domestic_score, foreign_score, domestic_tag_count, foreign_tag_count = calc_scores(tweet)\n",
    "        total+=1\n",
    "        if abs(domestic_score - foreign_score) <= 0.005:\n",
    "            if domestic_tag_count >=foreign_tag_count:\n",
    "                dcount+=1\n",
    "            else:\n",
    "                fcount+=1\n",
    "        else:\n",
    "            if domestic_score > foreign_score:\n",
    "                dcount+=1\n",
    "            else:\n",
    "                fcount+=1\n",
    "\n",
    "    print(\"Number of domestic tweets = \", total)\n",
    "    print(\"Number of domestic tweets tagged as domestic = \", dcount)\n",
    "    print(\"Accuracy of domestic tweets = \", dcount / total)\n",
    "\n",
    "    dcount = 0\n",
    "    fcount = 0\n",
    "    total = 0\n",
    "    for tweet in fore:\n",
    "        domestic_score, foreign_score,domestic_tag_count, foreign_tag_count = calc_scores(tweet)\n",
    "        total+=1\n",
    "        if domestic_score > foreign_score:\n",
    "            dcount+=1\n",
    "        else:\n",
    "            fcount+=1\n",
    "        \n",
    "    print(\"\\nNumber of foreign tweets = \", total)\n",
    "    print(\"Number of foreign tweets tagged as foreign = \", fcount)\n",
    "    print(\"Accuracy of foreign tweets = \", fcount / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of domestic tweets =  104\n",
      "Number of domestic tweets tagged as domestic =  76\n",
      "Accuracy of domestic tweets =  0.7307692307692307\n",
      "\n",
      "Number of foreign tweets =  56\n",
      "Number of foreign tweets tagged as foreign =  39\n",
      "Accuracy of foreign tweets =  0.6964285714285714\n"
     ]
    }
   ],
   "source": [
    "model(domestic, foreign)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above accuracy scores are based on testing our model on our training set.\n",
    "\n",
    "We will now test our model on a small test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of domestic tweets =  23\n",
      "Number of domestic tweets tagged as domestic =  17\n",
      "Accuracy of domestic tweets =  0.7391304347826086\n",
      "\n",
      "Number of foreign tweets =  20\n",
      "Number of foreign tweets tagged as foreign =  18\n",
      "Accuracy of foreign tweets =  0.9\n"
     ]
    }
   ],
   "source": [
    "domestic_index = [1, 5, 8, 9, 11, 13, 38, 41, 42, 43, 50, 51, 54, 57, 66, 67, 68, 74, 79, 100, 105, 112, 120]\n",
    "foreign_index = [16, 28, 30, 32, 59, 64, 65, 75, 76, 82, 87, 96, 98, 123, 157, 175, 178, 224, 332, 333]\n",
    "\n",
    "dom_tweets = [tweets[i] for i in domestic_index]\n",
    "fore_tweets = [tweets[i] for i in foreign_index]\n",
    "\n",
    "model(dom_tweets, fore_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : remarks the united states holocaust memorial museum's national days remembrance. full remarks:‚Ä¶\n",
      "1 : don't let fake media tell that have changed position the wall. will get built help stop drugs, human trafficking etc.\n",
      "2 : canada made business our dairy farmers wisconsin other border states difficult. will stand this. watch!\n",
      "3 : proud for leadership these important issues. looking forward hearing speak the w20!\n",
      "4 : today, signed holocaust remembrance proclamation: #icymi- statement last night at‚Ä¶\n",
      "5 : our healthcare plan approved, see real healthcare premiums will start tumbling down. obamacare in death spiral!\n",
      "6 : join in congratulating @astropeggy using hashtag #congratspeggy! earlier today:‚Ä¶\n",
      "7 : ....the wall not built, be, drug situation never fixed way it should be! #buildthewall\n",
      "8 : wall a important tool stopping drugs pouring country poisoning our youth (and many others)!\n",
      "9 : two fake news polls released yesterday, abc &amp; nbc, containing very positive info, totally wrong general e. watch!\n",
      "10 : ...popular vote. abc news/washington post poll (wrong big election) said almost stand their vote me &amp; 53% said strong leader.\n",
      "11 : new polls today very good considering much the media fake almost always negative. would still beat hillary .....\n",
      "12 : eventually, at later date we get started early, mexico be paying, some form, the badly needed border wall.\n",
      "13 : democrats don't want money budget going border wall despite fact it stop drugs very bad ms 13 gang members.\n",
      "14 : obamacare in serious trouble. dems need big money keep going - otherwise dies far sooner anyone would thought.\n",
      "15 : thank lake worth, florida.\n",
      "16 : interesting election currently taking place france.\n",
      "17 : am committed keeping air water clean always remember economic growth enhances environmental protection. jobs matter!\n",
      "18 : today earth day, celebrate beautiful forests, lakes land. stand committed preserving natural beauty our nation.\n",
      "19 : big tax reform tax reduction be announced next wednesday.\n",
      "20 : next saturday night will holding big rally pennsylvania. look forward it!\n",
      "21 : getting ready visit walter reed medical center melania. looking forward seeing bravest greatest americans!\n",
      "22 : rt israeli pm netanyahu praises u.s. policy changes meeting defense. sec mattis\n",
      "23 : rt chicago approves new plan hide illegal immigrants the feds, plus give access city services\n",
      "24 : 5 sb victories since 2002, was honor give bob kraft, coach belichick, the players first to‚Ä¶\n",
      "25 : 'presidential executive order identifying reducing tax regulatory burdens' executive order:‚Ä¶\n",
      "26 : rt .@potus @ivankatrump, jared kushner, &amp; dina powell the oval office today w/ aya &amp; brother basel. #w‚Ä¶\n",
      "27 : welcome home, aya! #godblesstheusaüá∫üá∏\n",
      "28 : china very much economic lifeline north korea so, nothing easy, want solve north korean problem, they will\n",
      "29 : matter much accomplish the ridiculous standard the first 100 days, &amp; has a lot (including s.c.), media kill!\n",
      "30 : another terrorist attack paris. people france not take much of this. have big effect presidential election!\n",
      "31 : rt nyt editor apologizes misleading tweet new england patriots' visit the white house (via h‚Ä¶\n",
      "32 : great honor host pm paolo gentiloni italy the white house afternoon! #icymi- joint press conference‚Ä¶\n",
      "33 : we're going use american steel, we're going use american labor, are going come first all deals. ‚û°Ô∏è‚Ä¶\n",
      "34 : failing has calling wrong two years, got caught a big lie concerning new england patriots visit w.h.\n",
      "35 : great honor host champion new england the white house today. congratulations!‚Ä¶\n",
      "36 : today signed veterans (our heroes) choice program extension &amp; improvement act #s544 watch‚Ä¶\n",
      "37 : #buyamericanhireamericanüá∫üá∏\n",
      "38 : dems failed kansas are failing georgia. great job karen handel! is hollywood vs. georgia june 20th.\n",
      "39 : despite major outside money, fake media support eleven republican candidates, big \"r\" win runoff georgia. glad be help!\n",
      "40 : #buyamericanhireamericanüá∫üá∏ https://t.co/rf9aivvb7g\n",
      "41 : learned jon is running congress georgia, doesn't even live the district. republicans, get and vote!\n",
      "42 : republicans must get today vote georgia 6. force runoff easy win! dem ossoff raise taxes-very bad crime &amp; 2nd a.\n",
      "43 : democrat jon ossoff would a disaster congress. weak crime illegal immigration, bad jobs wants higher taxes. say\n",
      "44 : will interviewed by starting 6:00 a.m. enjoy!\n",
      "45 : weak illegal immigration policies the obama admin. allowed bad ms 13 gangs form cities across u.s. are removing fast!\n",
      "46 : eleven republican candidates running georgia (on tuesday) congress, runoff be win. vote \"r\" lower taxes &amp; safety!\n",
      "47 : see tomorrow wisconsin! 'trump spurs small-business optimism milwaukee area'\n",
      "48 : trump approval hits 50%\n",
      "49 : rt trump approval hits 50%\n",
      "50 : super liberal democrat the georgia congressioal race tomorrow wants protect criminals, allow illegal immigration raise taxes!\n",
      "51 : fake media (not real media) gotten even worse since election. every story badly slanted. have hold to truth!\n",
      "52 : great book your reading enjoyment: \"reasons vote democrats\" michael j. knowles.\n",
      "53 : \"the first 90 days my presidency exposed total failure the last eight years foreign policy!\" true.\n",
      "54 : recent kansas election (congress) a really big media event, republicans won. they play the same game georgia-bad!\n",
      "55 : military building is rapidly becoming stronger ever before. frankly, have choice!\n",
      "56 : someone look who paid small organized rallies yesterday. the election over!\n",
      "57 : did was almost impossible thing do a republican-easily the electoral college! tax returns brought again?\n",
      "58 : happy easter everyone!\n",
      "59 : would call china currency manipulator they working us the north korean problem? will see happens!\n",
      "60 : rt looking forward hosting annual easter egg roll the monday!\n",
      "61 : weekly address- https://t.co/b2nqzj53ft\n",
      "62 : rt great again: feds arrest murder suspect 'fast furious' scandal...\n",
      "63 : was great honor welcome atlanta's heroic first responders the white house afternoon!\n",
      "64 : things work fine the u.s.a. russia. the right time everyone come their senses &amp; will lasting peace!\n",
      "65 : have great confidence china properly deal north korea. they unable do so, u.s., its allies, will! u.s.a.\n",
      "66 : jobs returning, illegal immigration plummeting, law, order justice being restored. are truly making america great again!\n",
      "67 : one one keeping promises - the border, energy, jobs, regulations. big changes are happening!\n",
      "68 : economic confidence soaring we unleash power private sector job creation stand for american workers. #americafirst\n",
      "69 : great meeting w/ nato sec. gen. agreed the importance getting countries pay fair share &amp; focus on‚Ä¶\n",
      "70 : great win kansas last night ron estes, easily winning congressional race the dems, spent heavily &amp; predicted victory!\n",
      "71 : a good call last night president china concerning the menace north korea.\n",
      "72 : will interviewed at 6:00 a.m. enjoy!\n",
      "73 : great strategic &amp; policy ceo forum today my cabinet secretaries top ceo's around united states.‚Ä¶\n",
      "74 : ron estes running today congress the great state kansas. wonderful guy, need help healthcare &amp; tax cuts (reform).\n",
      "75 : north korea looking trouble. china decides help, would great. not, will solve problem without them! u.s.a.\n",
      "76 : explained president china a trade deal the u.s. be far better them they solve the north korean problem!\n",
      "77 : rt grateful syrians react strike: 'i'll name son donald' #syrianstrikes\n",
      "78 : happy passover everyone celebrating united states america, israel, around the world. #chagsameach\n",
      "79 : congratulations justice neil gorsuch his elevation the united states supreme court. great day americ‚Ä¶\n",
      "80 : thank #usa\n",
      "81 : ...confidence president al sisi handle situation properly.\n",
      "82 : sad hear the terrorist attack egypt. u.s. strongly condemns. have great...\n",
      "83 : judge gorsuch sworn rose garden the white house monday at 11:00 a.m. will be a great justice. proud him!\n",
      "84 : reason don't generally hit runways that are easy inexpensive quickly fix (fill and top)!\n",
      "85 : congratulations our great military men women representing united states, the world, well the syria attack.\n",
      "86 : ...goodwill friendship formed, only time tell trade.\n",
      "87 : was great honor have president xi jinping madame peng liyuan china our guests the united states. tremendous...\n",
      "88 : rt proud arabella joseph their performance honor president xi jinping madame peng liyuan's official‚Ä¶\n",
      "89 : was honor host american heroes the #soldierridedc the today @vp‚Ä¶\n",
      "90 : jobs, jobs, jobs! https://t.co/b5qbn6llze\n",
      "91 : am deeply committed preserving strong relationship &amp; strengthening america's long-standing support for‚Ä¶\n",
      "92 : great talk jobs #nabtu2017. tremendous spirit &amp; optimism - will deliver!\n",
      "93 : thank sean mcgarvey &amp; entire governing board presidents honoring w/an invite speak. #nabtu2017‚Ä¶\n",
      "94 : #ceotownhall https://t.co/xhfq6zmf2h\n",
      "95 : rt rice ordered spy docs trump?\n",
      "96 : was honor welcome president al sisi egypt we renew the historic partnership betwe‚Ä¶\n",
      "97 : looking forward hosting heroes the wounded warrior project soldier ride the th‚Ä¶\n",
      "98 : getting ready meet president al-sisi egypt. behalf the united states, look forward a long wonderful relationship.\n",
      "99 : multiple sources: \"there electronic surveillance trump, people close trump. is unprecedented.\"\n"
     ]
    }
   ],
   "source": [
    "for index in range(0, 100):\n",
    "    print(index, \":\", tweets[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
