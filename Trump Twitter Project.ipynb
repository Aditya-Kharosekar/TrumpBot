{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keeping Trump on Topic: LIN353C Final Project\n",
    "\n",
    "By Hannah Brinsko and Aditya Kharosekar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aditya Kharosekar\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import datetime\n",
    "import gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing - Scraping tweets, and cleaning them up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing tweets from the CSV file - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3218, 28)\n",
      "5     2016-09-27T22:13:24\n",
      "8     2016-09-27T21:08:22\n",
      "11    2016-09-27T20:31:14\n",
      "12    2016-09-27T20:14:33\n",
      "13    2016-09-27T20:06:25\n",
      "Name: time, dtype: object\n"
     ]
    }
   ],
   "source": [
    "tweets_csv = pd.read_csv(\"tweets.csv\")\n",
    "trump_tweets = tweets_csv[tweets_csv['handle']==\"realDonaldTrump\"]\n",
    "print(trump_tweets.shape)\n",
    "print(trump_tweets['time'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the CSV file, we see that it contains tweets only up to 09/27/2016. We need his more recent tweets as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting his most recent ~3200 tweets.\n",
    "\n",
    "3200 is approximately the limit to how many tweets Tweepy allows us to scrape. As it turns out, this is more than enough for our use when combined with our CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import json\n",
    "from tweepy import OAuthHandler\n",
    "import codecs\n",
    "\n",
    "consumer_key = \"i387QW7Eqgh12UHmK3VoQO9K5\"\n",
    "consumer_secret = \"BQI8c5eKale4etdA21mawnFqOmAziDQpnThm679V7UtLjbWlMG\"\n",
    "access_token = \"816857419338764288-S8Ay111O2Mo32QAs88tSnv5uKvmGCkF\"\n",
    "access_secret = \"HVU19yLuV0klltJl1fsDibAi7Hiq1U4GwsEV9kozTAc1m\"\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "all_tweets = []\n",
    "\n",
    "new_tweets = api.user_timeline(screen_name=\"realDonaldTrump\", count=200)\n",
    "\n",
    "\n",
    "all_tweets.extend(new_tweets)\n",
    "oldest = all_tweets[-1].id-1\n",
    "\n",
    "t = new_tweets[0];\n",
    "\n",
    "while len(new_tweets) > 0:\n",
    "   new_tweets = api.user_timeline(screen_name = \"realDonaldTrump\", count=200, max_id = oldest)\n",
    "   all_tweets.extend(new_tweets)\n",
    "   oldest = all_tweets[-1].id-1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3246\n"
     ]
    }
   ],
   "source": [
    "print(len(all_tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have his most recent tweets.\n",
    "\n",
    "However, there is significant overlap between the tweets that we have scraped from his account and the tweets that are in the CSV file.\n",
    "\n",
    "The latest tweet in the CSV file was posted on September 27, 2016 at 22:13:24. So, we need to keep any scraped tweets which were posted after this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets = []\n",
    "i = 0\n",
    "while (all_tweets[i].created_at!=datetime.datetime(2016, 9, 27, 22, 13, 24)):\n",
    "    tweets.append(all_tweets[i].text)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "We have  4645 tweets to work with\n"
     ]
    }
   ],
   "source": [
    "tweets1 = trump_tweets['text']\n",
    "tweets1 = tweets1.tolist()\n",
    "for t in tweets1:\n",
    "    tweets.append(t)\n",
    "print(type(tweets))\n",
    "print(\"We have \", len(tweets), \"tweets to work with\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making distributional vectors from each tweet\n",
    "\n",
    "But to do that, we need to - \n",
    "1. Remove any twitter links and image links\n",
    "2. Remove any stopwords\n",
    "3. Make sure that we have a list of tweets where each tweet is a string\n",
    "4. Then use CountVectorizer http://scikit-learn.org/stable/modules/feature_extraction.html#common-vectorizer-usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['congratulations', 'to', 'justice', 'neil', 'gorsuch', 'on', 'his', 'elevation', 'to', 'the', 'united', 'states', 'supreme', 'court.', 'a', 'great', 'day', 'for', 'americ…', 'https://t.co/rm9lftaeps']\n",
      "['congratulations', 'to', 'justice', 'neil', 'gorsuch', 'on', 'his', 'elevation', 'to', 'the', 'united', 'states', 'supreme', 'court.', 'a', 'great', 'day', 'for', 'americ…']\n"
     ]
    }
   ],
   "source": [
    "temp_tweets = []\n",
    "for t in tweets:\n",
    "    temp_tweets.append(t.lower().split())\n",
    "\n",
    "print(temp_tweets[1])\n",
    "for t in temp_tweets:\n",
    "    for w in t:\n",
    "        if \"http\" in w or \"@\" in w: #I've removed any instances where he tags anyone in his tweets. \n",
    "                                    #I thought the word vectors might be too sparse if I left those in.\n",
    "            t.remove(w)\n",
    "print(temp_tweets[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE: This link removal is not working properly. Have to fix later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "for t in temp_tweets:\n",
    "    for w in t:\n",
    "        if w in stop:\n",
    "            t.remove(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Succesfully removed stopwords. At this point, each tweet is a list of words and temp_tweets is a list. What we need to use CountVectorizer is a list where each element is a string.\n",
    "\n",
    "Therefore, we need to convert each tweet from a lists of words to a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = []\n",
    "for t in temp_tweets:\n",
    "    tweets.append(' '.join(t))\n",
    "type(tweets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our methodology for classifying tweets\n",
    "\n",
    "Step 0 - Download a pre-trained Word2Vec model. We tried training our own model, but we did not have enough data.\n",
    "\n",
    "Step 1 - Hand tag some number of tweets (we ended up tagging about 280 tweets) and classify them into the following categories - \n",
    "1. Foreign Policy / International News\n",
    "2. Domestic Policy / domestic news\n",
    "3. Tweets about the media\n",
    "4. Attack tweets\n",
    "5. Other tweets\n",
    "6. Tweets about the election\n",
    "\n",
    "Step 2 - From our hand-tagged corpus, and for each category, create a list of words used.\n",
    "\n",
    "Step 3 - Create a word vector for each category by summing up the individual word vectors\n",
    "\n",
    "Step 4 - For each subsequent tweet, find cosine similarity between it and each category vector. Assign that tweet to the category it is most similar to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0 - Downloading a pre-trained Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 400000  words loaded\n"
     ]
    }
   ],
   "source": [
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r', encoding = 'utf8')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = [float(val) for val in splitLine[1:]]\n",
    "        model[word] = embedding\n",
    "    print(\"Done.\",len(model),\" words loaded\")\n",
    "    return model\n",
    "\n",
    "glove_model = loadGloveModel('glove.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "glove_model['hillary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "google_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31701805595455718"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_model.similarity('King', 'Kong')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "google_model is our pre-trained model which we will be using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Hand tagging tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tagged_tweets = pd.read_csv(\"tagged_tweets.csv\")\n",
    "tagged_tweets.columns = ['Tweet', 'Category']\n",
    "tweet_text = tagged_tweets['Tweet']\n",
    "tweet_categories = tagged_tweets['Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "foreign = []\n",
    "domestic =[]\n",
    "media =[]\n",
    "attack = []\n",
    "election = []\n",
    "other = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(a[40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
