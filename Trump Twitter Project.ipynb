{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Keeping Trump on Topic: LIN353C Final Project\n",
    "\n",
    "By Hannah Brinsko and Aditya Kharosekar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import csv\n",
    "import datetime\n",
    "import gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocessing - Scraping tweets, and cleaning them up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Importing tweets from the CSV file - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3218, 28)\n",
      "5     2016-09-27T22:13:24\n",
      "8     2016-09-27T21:08:22\n",
      "11    2016-09-27T20:31:14\n",
      "12    2016-09-27T20:14:33\n",
      "13    2016-09-27T20:06:25\n",
      "Name: time, dtype: object\n"
     ]
    }
   ],
   "source": [
    "tweets_csv = pd.read_csv(\"tweets.csv\")\n",
    "trump_tweets = tweets_csv[tweets_csv['handle']==\"realDonaldTrump\"]\n",
    "print(trump_tweets.shape)\n",
    "print(trump_tweets['time'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Looking at the CSV file, we see that it contains tweets only up to 09/27/2016. We need his more recent tweets as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Getting his most recent ~3200 tweets.\n",
    "\n",
    "3200 is approximately the limit to how many tweets Tweepy allows us to scrape. As it turns out, this is more than enough for our use when combined with our CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import json\n",
    "from tweepy import OAuthHandler\n",
    "import codecs\n",
    "\n",
    "consumer_key = \"i387QW7Eqgh12UHmK3VoQO9K5\"\n",
    "consumer_secret = \"BQI8c5eKale4etdA21mawnFqOmAziDQpnThm679V7UtLjbWlMG\"\n",
    "access_token = \"816857419338764288-S8Ay111O2Mo32QAs88tSnv5uKvmGCkF\"\n",
    "access_secret = \"HVU19yLuV0klltJl1fsDibAi7Hiq1U4GwsEV9kozTAc1m\"\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "all_tweets = []\n",
    "\n",
    "new_tweets = api.user_timeline(screen_name=\"realDonaldTrump\", count=200)\n",
    "\n",
    "\n",
    "all_tweets.extend(new_tweets)\n",
    "oldest = all_tweets[-1].id-1\n",
    "\n",
    "t = new_tweets[0];\n",
    "\n",
    "while len(new_tweets) > 0:\n",
    "   new_tweets = api.user_timeline(screen_name = \"realDonaldTrump\", count=200, max_id = oldest)\n",
    "   all_tweets.extend(new_tweets)\n",
    "   oldest = all_tweets[-1].id-1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3246\n"
     ]
    }
   ],
   "source": [
    "print(len(all_tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We now have his most recent tweets.\n",
    "\n",
    "However, there is significant overlap between the tweets that we have scraped from his account and the tweets that are in the CSV file.\n",
    "\n",
    "The latest tweet in the CSV file was posted on September 27, 2016 at 22:13:24. So, we need to keep any scraped tweets which were posted after this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tweets = []\n",
    "i = 0\n",
    "while (all_tweets[i].created_at!=datetime.datetime(2016, 9, 27, 22, 13, 24)):\n",
    "    tweets.append(all_tweets[i].text)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "We have  4645 tweets to work with\n"
     ]
    }
   ],
   "source": [
    "tweets1 = trump_tweets['text']\n",
    "tweets1 = tweets1.tolist()\n",
    "for t in tweets1:\n",
    "    tweets.append(t)\n",
    "print(type(tweets))\n",
    "print(\"We have \", len(tweets), \"tweets to work with\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Making distributional vectors from each tweet\n",
    "\n",
    "But to do that, we need to - \n",
    "1. Remove any twitter links and image links\n",
    "2. Remove any stopwords\n",
    "3. Make sure that we have a list of tweets where each tweet is a string\n",
    "4. Then use CountVectorizer http://scikit-learn.org/stable/modules/feature_extraction.html#common-vectorizer-usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Removing links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['congratulations', 'to', 'justice', 'neil', 'gorsuch', 'on', 'his', 'elevation', 'to', 'the', 'united', 'states', 'supreme', 'court.', 'a', 'great', 'day', 'for', 'americ…', 'https://t.co/rm9lftaeps']\n",
      "['congratulations', 'to', 'justice', 'neil', 'gorsuch', 'on', 'his', 'elevation', 'to', 'the', 'united', 'states', 'supreme', 'court.', 'a', 'great', 'day', 'for', 'americ…']\n"
     ]
    }
   ],
   "source": [
    "temp_tweets = []\n",
    "for t in tweets:\n",
    "    temp_tweets.append(t.lower().split())\n",
    "\n",
    "print(temp_tweets[1])\n",
    "for t in temp_tweets:\n",
    "    for w in t:\n",
    "        if \"http\" in w or \"@\" in w: #I've removed any instances where he tags anyone in his tweets. \n",
    "                                    #I thought the word vectors might be too sparse if I left those in.\n",
    "            t.remove(w)\n",
    "print(temp_tweets[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## NOTE: This link removal is not working properly. Have to fix later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "for t in temp_tweets:\n",
    "    for w in t:\n",
    "        if w in stop:\n",
    "            t.remove(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Succesfully removed stopwords. At this point, each tweet is a list of words and temp_tweets is a list. What we need to use CountVectorizer is a list where each element is a string.\n",
    "\n",
    "Therefore, we need to convert each tweet from a lists of words to a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = []\n",
    "for t in temp_tweets:\n",
    "    tweets.append(' '.join(t))\n",
    "type(tweets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Our methodology for classifying tweets\n",
    "\n",
    "Step 0 - Download a pre-trained Word2Vec model. We tried training our own model, but we did not have enough data.\n",
    "\n",
    "Step 1 - Hand tag some number of tweets (we ended up tagging about 280 tweets) and classify them into the following categories - \n",
    "1. Foreign Policy / International News\n",
    "2. Domestic Policy / domestic news\n",
    "3. Tweets about the media\n",
    "4. Attack tweets\n",
    "5. Other tweets\n",
    "6. Tweets about the election\n",
    "\n",
    "Step 2 - From our hand-tagged corpus, and for each category, create a list of words used.\n",
    "\n",
    "Step 3 - Create a word vector for each category by summing up the individual word vectors\n",
    "\n",
    "Step 4 - For each subsequent tweet, find cosine similarity between it and each category vector. Assign that tweet to the category it is most similar to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Step 0 - Downloading a pre-trained Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'glove.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-db932d7e1ff8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mglove_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadGloveModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'glove.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-db932d7e1ff8>\u001b[0m in \u001b[0;36mloadGloveModel\u001b[0;34m(gloveFile)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloadGloveModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgloveFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading Glove Model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgloveFile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'glove.txt'"
     ]
    }
   ],
   "source": [
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r', encoding = 'utf8')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = [float(val) for val in splitLine[1:]]\n",
    "        model[word] = embedding\n",
    "    print(\"Done.\",len(model),\" words loaded\")\n",
    "    return model\n",
    "\n",
    "glove_model = loadGloveModel('glove.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "glove_model['hillary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "google_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "google_model.similarity('King', 'Kong')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "google_model is our pre-trained model which we will be using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Step 1 - Hand tagging tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Using a .csv file of ~280 hand tagged tweets, we place the tweets into the preselected categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "foreign = []\n",
    "domestic =[]\n",
    "media =[]\n",
    "attack = []\n",
    "election = []\n",
    "other = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('tagged_tweets.csv')\n",
    "csv_f = csv.reader(f)\n",
    "\n",
    "for row in csv_f:\n",
    "    tweet = row[0]\n",
    "    cats = row[1]\n",
    "    if \"1\" in cats:\n",
    "        foreign.append(tweet)\n",
    "    elif \"2\" in cats:\n",
    "        domestic.append(tweet)\n",
    "    elif \"3\" in cats:\n",
    "        media.append(tweet)\n",
    "    elif \"4\" in cats:\n",
    "        attack.append(tweet)\n",
    "    elif \"5\" in cats:\n",
    "        other.append(tweet)\n",
    "    elif \"6\" in cats:\n",
    "        election.append(tweet)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "['today honored true american heroes the first-ever national vietnam war veterans day. #thankaveteran_', 'you believe @unionleader nh demanding ads? look enclosed letter them, received:', 'failing @unionleader newspaper n.h. sent trump organization letter asking we take ads. stupid, desperate!', '.@macys one the worst performing stocks the s&amp;p last year, plunging 46%. disloyal company. another win trump! boycott.', 'was very wise move ted cruz renounced canadian citizenship 18 months ago. senator john mccain certainly friend ted!', '.@sentedcruz ted--free legal advice how pre-empt dems citizen issue. go court &amp; seek declaratory judgment--you win!', 'hank greenberg, formerly aig, gave $10 million the @jebbush campaign 3 months ago. is happy, total waste money!', 'good news jeb bush', 'hope workers demand their @teamsters reps endorse donald j. trump. nobody knows jobs like do! don_t let sell out!', 'love seeing union &amp; non-union members alike defecting trump. will create jobs like one else. #dem leaders can_t compete!']\n",
      "10\n",
      "['germany going massive attacks its people migrants allowed enter the country. new years eve a disaster. think!', 'man shot inside paris police station. announced terror threat at highest level. germany a total mess-big crime. get smart!', \"iran deal terrible. didn't get uranium stockpile - was sent russia. #sotu\", \"iran toys u.s. days pay them, ridiculously, billions dollars. don't release money. we want hostages back now!\", 'iran humiliated united states the capture our 10 sailors. horrible pictures &amp; images. are weak. will forget!', 'you think iran would acted tough they russian sailors? country humiliated.', 'radical islam attacks today - never ends! strengthen borders, must vigilant smart. more politically correct.', 'iran deal get 4 prisoners. get $150 billion, 7 wanted many watch list. will create great incentive others!', \"far killed anticipated radical islamic terror attack yesterday. get tough smart u.s., we won't a country anymore!\", \"watched jeb's ad he desperately needed mommy help him. jeb --- mom can't help with isis, chinese with putin.\"]\n",
      "38\n",
      "['wow @unionleader circulation nh dropped 75,000 around 10_bad management. wonder begged for ads.', \"don_t know @samuelljackson, best my knowledge haven't played golf w/him &amp; think does many tv commercials_boring. a fan.\", 'don_t cheat golf @samuelljackson cheats_with game has choice_and stop commercials!', 'don_t like @samuelljackson_s golf swing. athletic. i_ve many club championships. play for charity!', \"have idea @jebbush whose campaign a disaster. try using last name don't ashamed it!\", 'weak &amp; ineffective @jebbush doing ads he shows statement the debate not response. false advertising!', 'hillary her friends!', 'explosive trump attack hrc, bill, monica, cosby, weiner. trump camp upped ante \"women\\'s rights\"', 'does @cnn &amp; @andersoncooper waste airtime putting failed campaign strategist, stuart stevens - lost big romney - the show?', 'ted cruz born canada was canadian citizen 15 months ago. lawsuits just filed more follow. told so']\n",
      "57\n",
      "['will on @wolfblitzer a @cnnsitroom interview today. please join us 5pm et.', '@theview @abc, great headed @barbarajwalters, now total freefall. whoopi goldberg terrible. sad!', 'joy behar, was fired her last show lack ratings, even worse @theview. love barbara!', 'pat buchanan gave fantastic interview morning @cnn - way go pat, way ahead your time!', 'thank you, @thefix- chris cillizza. true person character can change opinion &amp; what is right.', '.@jonahnro, watched @seanhannity appreciate statements --- have waiting them a long time. thank you.', '(1/2) time magazine me the cover week. david von drehle written one the best stories have ever had.', '(2/2) david brilliantly tells like is -- real deal! read it!', 'will interviewed chris wallace fox tomorrow morning. tune in!', 'be meet press @chucktodd tomorrow morning. enjoy!']\n",
      "71\n",
      "['\"@salriccobono: @realdonaldtrump @troyconway donald get big business back and# make america great for 2016\"', '\"@iloveidevices: @edwinro47796972 @happyjack225 @foxnews @krauthammer minimizing dependency china crucial.only trump talks that', '\"@ghosthunter_lol: iowa key endorsement @realdonaldtrump can\\'t wait the iowa caucus 4 weeks! #trump2016', '\"@marybnall01: @realdonaldtrump watched lowell mass speech. awesome. great crowd. make america great again!!!!!!\"', '\"@lilredfrmkokomo: @realdonaldtrump facebook groups all voting trump /4000 people! !!\" great!', '\"@lucky5713 @newday what\\'s wrong showing morocco? nothing! illustrates point! duh! love video!\"', '\"@longtalltexan20 @realdonaldtrump love when trump calls msm what really are! cams starting show crowds rallies!\"', '\"@pearl_brendan: @dphilbs @realdonaldtrump doesn\\'t win, i\\'m leaving country #trump4president\"', '\"@trumpy17: @realdonaldtrump gave awesome speech claremont tonight!!', '\"@rubinsteinnel: @realdonaldtrump @pearl_brendan @dphilbs trump doesn\\'t win, may be able recuperate america.\"']\n",
      "75\n",
      "['am new hampshire. received great news reuters poll. thank for support!', 'huge crowd expected tomorrow night! vt police say first come, first serve. arrive early!', 'in beautiful burlington, vermont, tonight a rally. will be great fun. make america great again!', 'massive crowd vt tonight. venue big enough. officials say to outside event sound system. arrive early!', \"i'm leaving for burlington, vermont. will wild!\", 'could get small fraction this 25k crowd in. movement make america great is unbelievable!', 'great time burlington, vermont. crowd amazing.', 'my way south carolina. big crowd--- look forward it!', '#makeamericagreatagain #trump2016', '#foxnews poll - thank you! #makeamericagreatagain #trump2016']\n"
     ]
    }
   ],
   "source": [
    "print(len(domestic))\n",
    "print(domestic[:10])\n",
    "print(len(foreign))\n",
    "print(foreign[:10])\n",
    "print(len(attack))\n",
    "print(attack[:10])\n",
    "print(len(media))\n",
    "print(media[:10])\n",
    "print(len(other))\n",
    "print(other[:10])\n",
    "print(len(election))\n",
    "print(election[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-998d3cc7befb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "type(a[40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
