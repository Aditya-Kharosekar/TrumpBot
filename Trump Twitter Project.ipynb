{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Keeping Trump on Topic: LIN353C Final Project\n",
    "\n",
    "By Hannah Brinsko and Aditya Kharosekar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aditya Kharosekar\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import csv\n",
    "import datetime\n",
    "import gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocessing - Scraping tweets, and cleaning them up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Importing tweets from the CSV file - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3218, 28)\n",
      "5     2016-09-27T22:13:24\n",
      "8     2016-09-27T21:08:22\n",
      "11    2016-09-27T20:31:14\n",
      "12    2016-09-27T20:14:33\n",
      "13    2016-09-27T20:06:25\n",
      "Name: time, dtype: object\n"
     ]
    }
   ],
   "source": [
    "tweets_csv = pd.read_csv(\"tweets.csv\")\n",
    "trump_tweets = tweets_csv[tweets_csv['handle']==\"realDonaldTrump\"]\n",
    "print(trump_tweets.shape)\n",
    "print(trump_tweets['time'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Looking at the CSV file, we see that it contains tweets only up to 09/27/2016. We need his more recent tweets as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Getting his most recent ~3200 tweets.\n",
    "\n",
    "3200 is approximately the limit to how many tweets Tweepy allows us to scrape. As it turns out, this is more than enough for our use when combined with our CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import json\n",
    "from tweepy import OAuthHandler\n",
    "import codecs\n",
    "\n",
    "consumer_key = \"i387QW7Eqgh12UHmK3VoQO9K5\"\n",
    "consumer_secret = \"BQI8c5eKale4etdA21mawnFqOmAziDQpnThm679V7UtLjbWlMG\"\n",
    "access_token = \"816857419338764288-S8Ay111O2Mo32QAs88tSnv5uKvmGCkF\"\n",
    "access_secret = \"HVU19yLuV0klltJl1fsDibAi7Hiq1U4GwsEV9kozTAc1m\"\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "all_tweets = []\n",
    "\n",
    "new_tweets = api.user_timeline(screen_name=\"realDonaldTrump\", count=200)\n",
    "\n",
    "\n",
    "all_tweets.extend(new_tweets)\n",
    "oldest = all_tweets[-1].id-1\n",
    "\n",
    "t = new_tweets[0];\n",
    "\n",
    "while len(new_tweets) > 0:\n",
    "   new_tweets = api.user_timeline(screen_name = \"realDonaldTrump\", count=200, max_id = oldest)\n",
    "   all_tweets.extend(new_tweets)\n",
    "   oldest = all_tweets[-1].id-1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3210\n"
     ]
    }
   ],
   "source": [
    "print(len(all_tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We now have his most recent tweets.\n",
    "\n",
    "However, there is significant overlap between the tweets that we have scraped from his account and the tweets that are in the CSV file.\n",
    "\n",
    "The latest tweet in the CSV file was posted on September 27, 2016 at 22:13:24. So, we need to keep any scraped tweets which were posted after this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tweets = []\n",
    "i = 0\n",
    "while (all_tweets[i].created_at!=datetime.datetime(2016, 9, 27, 22, 13, 24)):\n",
    "    tweets.append(all_tweets[i].text)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "We have  4660 tweets to work with\n"
     ]
    }
   ],
   "source": [
    "tweets1 = trump_tweets['text']\n",
    "tweets1 = tweets1.tolist()\n",
    "for t in tweets1:\n",
    "    tweets.append(t)\n",
    "print(type(tweets))\n",
    "print(\"We have \", len(tweets), \"tweets to work with\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Making distributional vectors from each tweet\n",
    "\n",
    "But to do that, we need to - \n",
    "1. Remove any twitter links and image links\n",
    "2. Remove any stopwords\n",
    "3. Make sure that we have a list of tweets where each tweet is a string\n",
    "4. Then use CountVectorizer http://scikit-learn.org/stable/modules/feature_extraction.html#common-vectorizer-usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Removing links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', 'was', 'a', 'great', 'honor', 'to', 'welcome', \"atlanta's\", 'heroic', 'first', 'responders', 'to', 'the', 'white', 'house', 'this', 'afternoon!', 'https://t.co/ztc14aj0xs']\n",
      "['it', 'was', 'a', 'great', 'honor', 'to', 'welcome', \"atlanta's\", 'heroic', 'first', 'responders', 'to', 'the', 'white', 'house', 'this', 'afternoon!']\n"
     ]
    }
   ],
   "source": [
    "temp_tweets = []\n",
    "for t in tweets:\n",
    "    temp_tweets.append(t.lower().split())\n",
    "\n",
    "print(temp_tweets[1])\n",
    "for t in temp_tweets:\n",
    "    for w in t:\n",
    "        if \"http\" in w or \"@\" in w: #I've removed any instances where he tags anyone in his tweets. \n",
    "            t.remove(w)\n",
    "print(temp_tweets[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## NOTE: This link removal is not working properly. Have to fix later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "for t in temp_tweets:\n",
    "    for w in t:\n",
    "        if w in stop:\n",
    "            t.remove(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Succesfully removed stopwords. At this point, each tweet is a list of words and temp_tweets is a list. What we need to use CountVectorizer is a list where each element is a string.\n",
    "\n",
    "Therefore, we need to convert each tweet from a lists of words to a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = []\n",
    "for t in temp_tweets:\n",
    "    tweets.append(' '.join(t))\n",
    "type(tweets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Our methodology for classifying tweets\n",
    "\n",
    "Step 0 - Download a pre-trained Word2Vec model. We tried training our own model, but we did not have enough data.\n",
    "\n",
    "Step 1 - Hand tag some number of tweets (we ended up tagging about 280 tweets) and classify them into the following categories - \n",
    "1. Foreign Policy / International News\n",
    "2. Domestic Policy / domestic news\n",
    "3. Tweets about the media\n",
    "4. Tweets about the election\n",
    "5. Other tweets\n",
    "\n",
    "Step 2 - From our hand-tagged corpus, and for each category, create a list of words used.\n",
    "\n",
    "Step 3 - Create a word vector for each category by summing up the individual word vectors\n",
    "\n",
    "Step 4 - For each subsequent tweet, find cosine similarity between it and each category vector. Assign that tweet to the category it is most similar to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Step 0 - Downloading a pre-trained Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 400000  words loaded\n"
     ]
    }
   ],
   "source": [
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r', encoding = 'utf8')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = [float(val) for val in splitLine[1:]]\n",
    "        model[word] = embedding\n",
    "    print(\"Done.\",len(model),\" words loaded\")\n",
    "    return model\n",
    "\n",
    "glove_model = loadGloveModel('glove.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.14675,\n",
       " 1.1692,\n",
       " 0.69416,\n",
       " -0.061429,\n",
       " -0.13677,\n",
       " 0.42015,\n",
       " -0.716,\n",
       " 0.019014,\n",
       " -0.52896,\n",
       " -0.83643,\n",
       " -1.8561,\n",
       " -0.18324,\n",
       " 0.057648,\n",
       " -0.31188,\n",
       " 0.024997,\n",
       " 0.045878,\n",
       " -0.098728,\n",
       " -0.21451,\n",
       " 0.14298,\n",
       " -0.0080809,\n",
       " -0.14569,\n",
       " 0.38326,\n",
       " 0.63811,\n",
       " -0.46426,\n",
       " 1.0953,\n",
       " -2.15,\n",
       " -0.18462,\n",
       " 0.1738,\n",
       " -0.50607,\n",
       " 0.00057719,\n",
       " 0.52828,\n",
       " 0.6685,\n",
       " -0.89692,\n",
       " -0.34346,\n",
       " -0.15456,\n",
       " -0.97313,\n",
       " -0.69441,\n",
       " 0.59201,\n",
       " -1.2194,\n",
       " -1.3469,\n",
       " -0.25691,\n",
       " 0.34537,\n",
       " -0.43824,\n",
       " -0.096233,\n",
       " 0.29882,\n",
       " -0.29174,\n",
       " -0.47201,\n",
       " -0.32221,\n",
       " 0.079279,\n",
       " 0.59419]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_model['hillary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "google_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.63017073801944812"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_model.similarity('Germany', 'Europe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "google_model is our pre-trained model which we will be using. We are not using the Glove model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Step 1 - Hand tagging tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Using a .csv file of hand tagged tweets, we place the tweets into the preselected categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(483, 2)\n"
     ]
    }
   ],
   "source": [
    "tagged_tweets = pd.read_csv(\"tagged_tweets.csv\")\n",
    "print(tagged_tweets.shape)\n",
    "foreign = []\n",
    "domestic =[]\n",
    "media =[]\n",
    "attack = []\n",
    "election = []\n",
    "other = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "f = open('tagged_tweets.csv')\n",
    "csv_f = csv.reader(f)\n",
    "\n",
    "for row in f:\n",
    "    tweet = row[0]\n",
    "    cats = row[1]\n",
    "    if \"1\" in cats:\n",
    "        foreign.append(tweet)\n",
    "    elif \"2\" in cats:\n",
    "        domestic.append(tweet)\n",
    "    elif \"3\" in cats:\n",
    "        media.append(tweet)\n",
    "    elif \"4\" in cats:\n",
    "        attack.append(tweet)\n",
    "    elif \"5\" in cats:\n",
    "        other.append(tweet)\n",
    "    elif \"6\" in cats:\n",
    "        election.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domestic:  1\n",
      "Foreign:  3\n",
      "Attack:  1\n",
      "Media:  0\n",
      "Other:  0\n",
      "Election:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Domestic: \",len(domestic))\n",
    "print(\"Foreign: \", len(foreign))\n",
    "print(\"Attack: \", len(attack))\n",
    "print(\"Media: \", len(media))\n",
    "print(\"Other: \",len(other))\n",
    "print(\"Election: \",len(election))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Making a list of words from each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Each tweet is a string right now.\n",
    "#This function will split up the string into individual words, remove any words which start with @\n",
    "#(i.e our generated tweets won't have any tags)\n",
    "\n",
    "def clean_up(tweets):\n",
    "    tweets1 = []\n",
    "    for t in tweets:\n",
    "        tweets1.append(t.split())\n",
    "        \n",
    "    tweets_words = []\n",
    "    for t in tweets1:\n",
    "        for w in t:\n",
    "            tweets_words.append(w)\n",
    "    tweets_words = tweets_words\n",
    "    \n",
    "    #removing '@' from any word which has it. The google_model does not have any words which start with @\n",
    "    temp_words = []\n",
    "    for word in tweets_words:\n",
    "        if word[0]=='@':\n",
    "            temp_words.append(word[1:])\n",
    "        else:\n",
    "            temp_words.append(word)\n",
    "    return temp_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "domestic_words = clean_up(domestic)\n",
    "foreign_words = clean_up(foreign)\n",
    "media_words = clean_up(media)\n",
    "attack_words = clean_up(attack)\n",
    "election_words = clean_up(election)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1843\n",
      "1253\n",
      "926\n",
      "1163\n",
      "923\n"
     ]
    }
   ],
   "source": [
    "print(len(domestic_words))\n",
    "print(len(foreign_words))\n",
    "print(len(attack_words))\n",
    "print(len(media_words))\n",
    "print(len(election_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115\n",
      "162\n",
      "156\n",
      "91\n",
      "['wow', 'unionleader', 'circulation', 'nh', 'dropped', '75,000', 'around', '10_bad', 'management.', 'wonder', 'begged', 'for', 'ads.', 'don_t', 'know', 'samuelljackson,', 'best', 'my', 'knowledge', \"haven't\", 'played', 'golf', 'w/him', '&amp;', 'think', 'does', 'many', 'tv', 'commercials_boring.', 'a', 'fan.', 'don_t', 'cheat', 'golf', 'samuelljackson', 'cheats_with', 'game', 'has', 'choice_and', 'stop', 'commercials!', 'don_t', 'like', 'samuelljackson_s', 'golf', 'swing.', 'athletic.', 'i_ve', 'many', 'club', 'championships.', 'play', 'for', 'charity!', 'have', 'idea', 'jebbush', 'whose', 'campaign', 'a', 'disaster.', 'try', 'using', 'last', 'name', \"don't\", 'ashamed', 'it!', 'weak', '&amp;', 'ineffective', 'jebbush', 'doing', 'ads', 'he', 'shows', 'statement', 'the', 'debate', 'not', 'response.', 'false', 'advertising!', 'hillary', 'her', 'friends!', 'explosive', 'trump', 'attack', 'hrc,', 'bill,', 'monica,', 'cosby,', 'weiner.', 'trump', 'camp', 'upped', 'ante', '\"women\\'s', 'rights\"', 'does', 'cnn', '&amp;', 'andersoncooper', 'waste', 'airtime', 'putting', 'failed', 'campaign', 'strategist,', 'stuart', 'stevens', '-', 'lost', 'big', 'romney', '-', 'the', 'show?', 'ted', 'cruz', 'born', 'canada', 'was', 'canadian', 'citizen', '15', 'months', 'ago.', 'lawsuits', 'just', 'filed', 'more', 'follow.', 'told', 'so', 'ted', 'cruz', 'said', '\"didn\\'t', 'know\"', 'a', 'canadian', 'citizen.', 'he', 'also', 'forgot', 'file', 'goldman', 'sachs', 'million', '$', 'loan', 'papers.not', 'believable', 'there', 'another', 'loan', 'ted', 'cruz', 'forgot', 'file.', 'goldman', 'sachs', 'owns', 'him,', 'will', 'anything', 'demand.', 'much', 'a', 'reformer!', 'ted', 'cruz', 'wiseguy', 'apology', 'the', 'people', 'new', 'york', 'a', 'disgrace.', 'remember,', \"wife's\", 'employer,', 'his', 'lender,', 'located', 'there!', 'oh', 'no,', 'reported', 'ted', 'cruz', \"didn't\", 'report', 'another', 'loan,', 'one', 'citi.', 'wow,', 'wonder', 'banks', 'so', 'well', 'the', 'u.s.', 'senate.', 'based', 'the', 'fact', 'ted', 'cruz', 'born', 'canada', 'is', 'therefore', '\"natural', 'born', 'canadian,\"', 'he', 'borrow', 'unreported', 'loans', 'c', 'banks?', 'ted', 'cruz', 'purposely,', 'illegally,', 'not', 'list', 'his', 'personal', 'disclosure', 'form', 'personally', 'guaranteed', 'loans', 'banks.', 'own', 'him!', 'ted', 'the', 'ultimate', 'hypocrite.', 'says', 'one', 'thing', 'money,', 'another', 'votes.', 'ted', 'cruz', 'so', 'opposed', 'gay', 'marriage,', 'did', 'accept', 'money', 'people', 'espouse', 'gay', 'marriage?', 'everybody', 'loves', 'people', 'new', 'york,', 'all', 'have', 'thru,', 'get', 'hypocrites', 'like', 'ted', 'cruz', 'of', 'politics!', 'will', 'tedcruz', 'give', 'the', 'new', 'york', 'based', 'campaign', 'contributions', 'back', 'the', 'special', 'interests', 'control', 'him.', 'greatly', 'dishonest', 'tedcruz', 'file', 'financial', 'disclosure', 'form', '&amp;', 'list', 'lending', 'banks-', 'pretend', 'is', 'going', 'clean', 'wall', 'st', 'wow!', 'ted', 'cruz', 'received', '$487k', 'campaign', 'contributions,', '$11m', 'a', 'ny', 'hedge', 'fund', 'mogul,', '&amp;', '$1m', 'low', 'int.', 'loan', 'goldman', 'sachs.', 'hypocrite', 'this', 'new', 'york', 'ted', 'cruz', 'talking', '&amp;', 'demeaning?', 'ted', 'cruz', 'falling', 'the', 'polls.', 'is', 'nervous.', 'people', 'worried', 'place', 'birth', 'his', 'failure', 'report', 'his', 'loans', 'banks!', \"don't\", 'think', 'ted', 'cruz', 'even', 'run', 'president', 'he', 'assure', 'republican', 'voters', 'being', 'born', 'canada', 'not', 'problem.', 'doubt!', 'wow,', 'new', 'polls', 'out', 'trump', 'and', 'cruz', '-', 'is', 'nervous', 'wreck!', '.@tedcruz', 'conflicting', 'stances', 'birthright', 'citizenship', '[14th', 'amendment]', 'gives', '#teamtrump', 'credit.', 'wow,', 'highly', 'respected', 'governor', 'iowa', 'stated', '\"ted', 'cruz', 'must', 'defeated.\"', 'big', 'shoker!', 'people', 'not', 'like', 'ted.', 'sad', 'sack', 'jebbush', 'just', 'done', 'another', 'ad', 'me,', 'special', 'interest', 'money,', 'saying', \"won't\", 'beat', 'hillary', '-', 'will.', 'he', \"can't\", 'beat', 'me.', 'low', 'energy', 'candidate', 'jebbush', 'wasted', '$80', 'million', 'his', 'failed', 'presidential', 'campaign.', 'millions', 'spent', 'me.', 'should', 'go', 'home', 'relax!', 'spending', '$89', 'million,', 'jebbush', 'at', 'bottom', 'barrel', 'polls.', 'is', 'ashamed', 'use', 'the', 'name', '\"bush\"', 'ads.', 'low', 'energy', 'guy!', 'ted', 'cruz', 'went', 'big', 'just', 'released', 'reuters', 'poll', '-', \"what's\", 'going', 'on?', 'it', 'goldman', 'sachs/citi', 'loans', 'canada?', 'cruz', 'says', 'supported', 'tarp,', 'gave', '$25', 'million', 'goldman', 'sachs,', 'bank', 'loaned', 'the', 'money', \"didn't\", 'disclose.', 'puppet!', 'cruz', 'not', 'renounce', 'canadian', 'citizenship', 'a', 'us', 'senator-', 'when', 'started', 'run', '#potus.', 'could', 'canadian', 'prime', 'minister.', 'cruz', 'honest?', 'bed', 'w/', 'wall', 'st.', '&amp;', 'is', 'funded', 'goldman', 'sachs/citi,', 'low', 'interest', 'loans.', 'legal', 'disclosure', '&amp;', 'never', 'sold', 'assets.', 'am', 'iowa', 'watching', 'of', 'phony', 't.v.', 'ads', 'the', 'candidates.', 'bull,', 'politicians', 'all', 'talk', 'no', 'action-it', \"won't\", 'happen!', 'ted', 'cruz', 'poll', 'numbers', 'down', 'big.', 'born', 'canada', 'was,', 'recently,', 'canadian', 'citizen,', 'many', 'believe', 'he', 'cannot', 'run!', 'word', 'that', 'crying', 'glennbeck', 'left', 'gop', \"doesn't\", 'the', 'right', 'vote', 'the', 'republican', 'primary.', 'dumb', 'a', 'rock.', 'Can', 'you', 'imagine', 'what', 'the', 'outcry', 'would', 'be', 'if', 'SnoopDogg,', 'failing', 'career', 'and', 'all,', 'had', 'aimed', 'and', 'fired', 'the', 'gun', 'at', 'President', 'Obama?', 'Jail', 'time!', 'Arnold', 'Schwarzenegger', \"isn't\", 'voluntarily', 'leaving', 'the', 'Apprentice,', 'he', 'was', 'fired', 'by', 'his', 'bad', '(pathetic)', 'ratings,', 'not', 'by', 'me.', 'Sad', 'end', 'to', 'great', 'show', 'I', 'know', 'the', '\"Governors\"', 'and', 'Jeb', 'Bush,', 'who', 'has', 'gone', 'nasty', 'with', 'lies,', 'is', 'by', 'far', 'the', 'weakest', 'of', 'the', 'lot.', 'His', 'family', 'used', 'private', 'eminent', 'domain!', 'Remember', 'JebBush', 'wants', 'COMMON', 'CORE', '(education', 'from', 'D.C.)', 'and', 'is', 'very', 'weak', 'on', 'ILLEGAL', 'IMMIGRATION', '(\"come', 'as', 'act', 'of', 'love\").', 'Not', 'a', 'leader!', 'Jeb', 'Bush', 'has', 'zero', 'communication', 'skills', 'so', 'he', 'spent', 'a', 'fortune', 'of', 'special', 'interest', 'money', 'on', 'a', 'Super', 'Bowl', 'ad.', 'He', 'is', 'a', 'weak', 'candidate!', 'Wow,', 'just', 'saw', 'an', 'ad', '-', 'Cruz', 'is', 'lying', 'on', 'so', 'many', 'levels.', 'There', 'is', 'nobody', 'more', 'against', 'ObamaCare', 'than', 'me,', 'will', 'repeal', '&amp;', 'replace.', 'He', 'lies!', 'Ted', 'Cruz', 'is', 'in', 'trouble', 'for', 'not', 'reporting', 'his', 'bank', 'borrowing', 'in', 'his', 'very', 'important', 'Financial', 'Disclosure', 'Form.', 'Very', 'low', 'interest', 'loans,', 'scam!', 'Ted', 'Cruz', 'is', 'totally', 'unelectable,', 'if', 'he', 'even', 'gets', 'to', 'run', '(born', 'in', 'Canada).', 'Will', 'loose', 'big', 'to', 'Hillary.', 'Polls', 'show', 'I', 'beat', 'Hillary', 'easily!', 'WIN!', 'Hillary', 'Clinton', 'has', 'been', 'involved', 'in', 'corruption', 'for', 'most', 'of', 'her', 'professional', 'life!', 'Who', 'should', 'star', 'in', 'a', 'reboot', 'of', 'Liar', 'Liar-', 'Hillary', 'Clinton', 'or', 'Ted', 'Cruz?', 'Let', 'me', 'know.', 'https://t.co/ESdiEftWGs', 'Goofy', 'Elizabeth', 'Warren', 'didn_t', 'have', 'the', 'guts', 'to', 'run', 'for', 'POTUS.', 'Her', 'phony', 'Native', 'American', 'heritage', 'stops', 'that', 'and', 'VP', 'cold.', 'I', \"don't\", 'want', 'to', 'hit', 'Crazy', 'Bernie', 'Sanders', 'too', 'hard', 'yet', 'because', 'I', 'love', 'watching', 'what', 'he', 'is', 'doing', 'to', 'Crooked', 'Hillary.', 'His', 'time', 'will', 'come!', 'Big', 'wins', 'in', 'West', 'Virginia', 'and', 'Nebraska.', 'Get', 'ready', 'for', 'November', '-', 'Crooked', 'Hillary,', 'who', 'is', 'looking', 'very', 'bad', 'against', 'Crazy', 'Bernie,', 'will', 'lose!', 'Bad', 'performance', 'by', 'Crooked', 'Hillary', 'Clinton!', 'Reading', 'poorly', 'from', 'the', 'telepromter!', 'She', \"doesn't\", 'even', 'look', 'presidential!', 'Crooked', 'Hillary', 'no', 'longer', 'has', 'credibility', '-', 'too', 'much', 'failure', 'in', 'office.', 'People', 'will', 'not', 'allow', 'another', 'four', 'years', 'of', 'incompetence!']\n"
     ]
    }
   ],
   "source": [
    "print(len(set(attack_words).intersection(foreign_words)))\n",
    "print(len(set(attack_words).intersection(domestic_words)))\n",
    "print(len(set(attack_words).intersection(media_words)))\n",
    "print(len(set(attack_words).intersection(election_words)))\n",
    "print(attack_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Create a category vector by adding up individual word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_category_vector(words):\n",
    "    vector = np.ones(300)\n",
    "    for word in words:\n",
    "        try:\n",
    "            vector = vector + google_model[word]\n",
    "        except KeyError: #some words are not in model. I don't want to pre-process everything so I'm just handling each exception\n",
    "            pass\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "domestic_vector = create_category_vector(domestic_words)\n",
    "foreign_vector = create_category_vector(foreign_words)\n",
    "media_vector = create_category_vector(media_words)\n",
    "attack_vector = create_category_vector(attack_words)\n",
    "election_vector = create_category_vector(election_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sad hear the terrorist attack egypt. u.s. strongly condemns. have great...'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_tweet_vector(tweet):\n",
    "    vector = np.ones(300)\n",
    "    for word in tweet:\n",
    "        try:\n",
    "            vector = vector + google_model[word]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_cosine_similarity(tweet_vector, category_vector):\n",
    "    return cosine_similarity(tweet_vector, category_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_scores(tweet):\n",
    "    score = calc_cosine_similarity(create_tweet_vector(tweet), domestic_vector)\n",
    "    print(\"Domestic: \",score)\n",
    "    score = calc_cosine_similarity(create_tweet_vector(tweet), foreign_vector)\n",
    "    print(\"Foreign:\", score)\n",
    "    score = calc_cosine_similarity(create_tweet_vector(tweet), attack_vector)\n",
    "    print(\"Attack:\", score)\n",
    "    score = calc_cosine_similarity(create_tweet_vector(tweet), media_vector)\n",
    "    print(\"Media:\", score)\n",
    "    score = calc_cosine_similarity(create_tweet_vector(tweet), election_vector)\n",
    "    print(\"Election: \",score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -9.35113525e+00,   7.78948975e+00,  -1.57104492e-01,\n",
       "         8.05053711e+00,  -2.08206177e+00,   1.96066284e+00,\n",
       "        -2.93334961e+00,  -2.16210938e+00,  -4.97985840e-01,\n",
       "         1.25921631e+00,  -2.61226082e+00,  -2.60345459e+00,\n",
       "        -9.91833496e+00,   3.18066406e+00,  -5.80615234e+00,\n",
       "         6.56176758e+00,   5.89794922e+00,   1.18876953e+01,\n",
       "        -1.69174194e-01,   1.76597595e+00,  -1.26691895e+01,\n",
       "        -1.72669983e+00,   5.63740540e+00,   1.57710266e+00,\n",
       "        -2.25479126e+00,   1.20983410e+00,  -1.14291992e+01,\n",
       "         4.43084717e+00,  -1.89587402e+00,  -1.04830933e+00,\n",
       "         1.24900818e+00,   2.55762482e+00,  -2.18862915e+00,\n",
       "        -3.87451172e+00,  -5.41833496e+00,   6.10845947e+00,\n",
       "        -1.24753113e+01,   6.36840820e+00,  -9.47307587e-01,\n",
       "         5.69799805e+00,  -1.46270752e+00,  -1.67822266e+00,\n",
       "         4.04980469e+00,   5.26690674e+00,   2.95907974e+00,\n",
       "        -9.87697601e-01,  -1.41525269e+00,  -7.01513672e+00,\n",
       "        -4.31079102e+00,   7.53967285e+00,  -7.37023926e+00,\n",
       "         1.45258789e+01,  -1.90161133e+00,   1.37374268e+01,\n",
       "         5.86682129e+00,   6.47863770e+00,  -7.21728516e+00,\n",
       "        -6.35250092e+00,   4.15649414e-01,  -7.67749023e+00,\n",
       "        -6.26966858e+00,  -2.35784912e+00,  -8.19415283e+00,\n",
       "        -2.05527592e+00,  -1.03770447e+00,  -9.56933594e+00,\n",
       "        -4.45956421e+00,   6.00781250e+00,  -1.77026367e+00,\n",
       "         3.85046387e+00,   1.90368652e+00,  -3.12280273e+00,\n",
       "         2.56262207e+00,  -2.29873657e-01,   8.12454224e-02,\n",
       "        -1.96533203e-02,   9.22076416e+00,   2.48294067e+00,\n",
       "         1.84506226e+00,  -4.57421875e+00,  -6.16186523e+00,\n",
       "         1.01736450e+00,  -1.73442078e+00,   1.56616211e+00,\n",
       "         8.56036377e+00,   4.03784180e+00,   5.97167969e-01,\n",
       "         1.23936768e+01,   1.59611511e+00,   2.99438477e+00,\n",
       "         2.08587646e+00,   1.89501953e+00,  -8.59436035e-01,\n",
       "        -6.67266846e+00,   4.88317871e+00,   6.71975708e+00,\n",
       "        -4.39984131e+00,   6.06109619e+00,   1.40446777e+01,\n",
       "         2.18017578e-01,  -2.95874023e+00,   3.93981934e-02,\n",
       "        -3.36572266e+00,   8.01757812e-01,  -2.36358643e+00,\n",
       "         8.24902344e+00,  -2.44006348e+00,   5.30728149e+00,\n",
       "         9.20654297e-01,   3.88540649e+00,  -5.86480713e+00,\n",
       "        -8.26766968e+00,  -4.25468445e-01,  -7.14920044e-01,\n",
       "         3.64830017e+00,   1.04970703e+01,   4.97963715e+00,\n",
       "        -1.08816528e+00,   3.57452393e-01,   8.57406616e-01,\n",
       "         6.80914307e+00,   5.47534180e+00,   1.11206055e-01,\n",
       "         3.87341309e+00,   8.63989258e+00,  -7.30017090e+00,\n",
       "        -3.61831665e+00,  -1.87677002e+00,   2.62756348e+00,\n",
       "         5.27014923e+00,  -2.99017334e+00,   3.36181641e-01,\n",
       "        -2.53201294e+00,  -1.21343994e+00,  -7.68392181e+00,\n",
       "         5.88470459e+00,   4.92553711e-02,   3.23352051e+00,\n",
       "         1.46103516e+01,   8.50613403e+00,   1.16485596e+01,\n",
       "        -2.80741501e+00,   3.29681396e+00,  -1.43725586e+00,\n",
       "        -1.73937988e+00,  -3.81545258e+00,  -4.71496582e-01,\n",
       "         1.00085449e+01,  -1.43908691e+00,   2.05760193e+00,\n",
       "         3.78839111e+00,  -1.56030273e+01,  -2.18203735e+00,\n",
       "        -1.33892822e+00,  -4.23327637e+00,  -4.72290039e+00,\n",
       "         4.00619507e+00,   1.06308594e+01,  -1.23533630e+00,\n",
       "        -3.19915771e-01,   1.58740234e+00,   5.36474609e+00,\n",
       "         1.22348022e+00,   1.06179810e+00,   8.00109863e-01,\n",
       "        -6.48941040e-01,   9.32336426e+00,   3.08833313e+00,\n",
       "        -2.99401855e+00,   6.18780518e+00,  -4.92150879e+00,\n",
       "        -3.39587402e+00,  -1.12808228e+00,  -1.49035645e+00,\n",
       "         5.81558228e-01,   1.13706055e+01,   1.26682129e+01,\n",
       "        -1.03452148e+01,   3.40576172e-02,  -3.87231445e+00,\n",
       "         1.13674927e+00,   2.43624306e+00,   3.03693771e+00,\n",
       "        -1.51379395e+00,   4.64233398e-01,   4.65840149e+00,\n",
       "        -2.25598145e+00,   5.74815369e+00,  -1.82348633e+00,\n",
       "         4.42967224e+00,   7.86495972e+00,  -7.75878906e-01,\n",
       "        -1.20888672e+01,   3.01757812e-01,   7.65917969e+00,\n",
       "         1.87054443e+00,  -2.95317078e+00,  -1.55639648e-02,\n",
       "         4.80647278e+00,  -3.00793457e+00,  -3.94036865e+00,\n",
       "         1.58581543e+00,  -9.70031738e+00,  -5.04101562e+00,\n",
       "         6.06420898e+00,  -2.95584106e+00,  -4.20043945e-01,\n",
       "         7.27105713e+00,  -1.25122070e+00,   9.41723633e+00,\n",
       "         8.55267334e+00,   6.22351074e+00,  -6.18165588e+00,\n",
       "         1.80664062e+00,  -3.27563477e+00,   3.72961426e+00,\n",
       "         8.83740234e+00,  -3.07936859e+00,  -7.69683838e+00,\n",
       "         2.28259277e+00,  -4.69165039e+00,   8.21600342e+00,\n",
       "         4.64526367e+00,   1.36260986e-01,   1.50891113e+00,\n",
       "        -3.35235596e+00,   5.23669434e+00,   4.18041992e+00,\n",
       "         2.36755943e+00,  -6.15093994e+00,   2.29980469e+00,\n",
       "        -2.91616821e+00,  -3.63248730e+00,   1.26919556e+00,\n",
       "         9.88647461e-01,   2.30671692e+00,   2.74865723e+00,\n",
       "         5.79861641e+00,   4.57641602e+00,   6.58691406e+00,\n",
       "         4.50100708e+00,   3.25708008e+00,   1.55539551e+01,\n",
       "        -1.84271240e+00,   2.00329590e+00,   6.46401978e+00,\n",
       "        -6.66802979e+00,   8.45178223e+00,   3.41432965e+00,\n",
       "        -3.36634827e+00,   4.94770241e+00,   2.68615723e-01,\n",
       "         5.05950928e+00,   4.72079468e+00,  -8.85314941e-01,\n",
       "        -6.38374329e+00,  -1.04907227e+00,   4.49816895e+00,\n",
       "         3.10757446e+00,  -2.14959717e+00,   5.03466797e+00,\n",
       "         1.04068756e+00,   6.35375977e+00,   8.31298828e-01,\n",
       "        -2.81304932e+00,   1.14807129e+00,   1.57495117e+00,\n",
       "         7.56713867e-01,   2.45056152e-02,  -1.96395874e+00,\n",
       "        -3.33399963e+00,   5.66528320e-01,   3.26623535e+00,\n",
       "         1.55023193e+00,   5.00170898e+00,  -3.45556641e+00,\n",
       "        -7.37554932e+00,  -5.72900391e+00,  -3.36090088e+00,\n",
       "         3.68402100e+00,  -5.62945557e+00,   9.41577148e+00,\n",
       "         4.38720703e-01,   1.19618225e+00,   5.43823242e-01,\n",
       "         4.50054932e+00,  -4.31152344e-01,  -7.27050781e-01,\n",
       "        -1.67968750e+00,  -2.12692261e+00,   3.41589355e+00,\n",
       "        -5.68115234e-01,  -3.13574219e+00,   6.69982910e+00,\n",
       "        -4.30419922e-01,  -7.14794922e+00,  -3.26428223e+00,\n",
       "        -1.01501465e+00,  -3.48999023e+00,   1.02577515e+01])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = create_tweet_vector(tweets[20])\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domestic:  [[ 0.28798119]]\n",
      "Foreign: [[ 0.27586971]]\n",
      "Attack: [[ 0.41688624]]\n",
      "Media: [[ 0.33680319]]\n",
      "Election:  [[ 0.33807142]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aditya Kharosekar\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\Aditya Kharosekar\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\Aditya Kharosekar\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\Aditya Kharosekar\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\Aditya Kharosekar\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\Aditya Kharosekar\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\Aditya Kharosekar\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\Aditya Kharosekar\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\Aditya Kharosekar\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\Aditya Kharosekar\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "calc_scores(tweets[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
