{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Keeping Trump on Topic: LIN353C Final Project\n",
    "\n",
    "By Hannah Brinsko and Aditya Kharosekar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aditya Kharosekar\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import csv\n",
    "import datetime\n",
    "import gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocessing - Scraping tweets, and cleaning them up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Importing tweets from the CSV file - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3218, 28)\n",
      "5     2016-09-27T22:13:24\n",
      "8     2016-09-27T21:08:22\n",
      "11    2016-09-27T20:31:14\n",
      "12    2016-09-27T20:14:33\n",
      "13    2016-09-27T20:06:25\n",
      "Name: time, dtype: object\n"
     ]
    }
   ],
   "source": [
    "tweets_csv = pd.read_csv(\"tweets.csv\")\n",
    "trump_tweets = tweets_csv[tweets_csv['handle']==\"realDonaldTrump\"]\n",
    "print(trump_tweets.shape)\n",
    "print(trump_tweets['time'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Looking at the CSV file, we see that it contains tweets only up to 09/27/2016. We need his more recent tweets as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Getting his most recent ~3200 tweets.\n",
    "\n",
    "3200 is approximately the limit to how many tweets Tweepy allows us to scrape. As it turns out, this is more than enough for our use when combined with our CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import json\n",
    "from tweepy import OAuthHandler\n",
    "import codecs\n",
    "\n",
    "consumer_key = \"i387QW7Eqgh12UHmK3VoQO9K5\"\n",
    "consumer_secret = \"BQI8c5eKale4etdA21mawnFqOmAziDQpnThm679V7UtLjbWlMG\"\n",
    "access_token = \"816857419338764288-S8Ay111O2Mo32QAs88tSnv5uKvmGCkF\"\n",
    "access_secret = \"HVU19yLuV0klltJl1fsDibAi7Hiq1U4GwsEV9kozTAc1m\"\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "all_tweets = []\n",
    "\n",
    "new_tweets = api.user_timeline(screen_name=\"realDonaldTrump\", count=200)\n",
    "\n",
    "\n",
    "all_tweets.extend(new_tweets)\n",
    "oldest = all_tweets[-1].id-1\n",
    "\n",
    "t = new_tweets[0];\n",
    "\n",
    "while len(new_tweets) > 0:\n",
    "   new_tweets = api.user_timeline(screen_name = \"realDonaldTrump\", count=200, max_id = oldest)\n",
    "   all_tweets.extend(new_tweets)\n",
    "   oldest = all_tweets[-1].id-1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3248\n"
     ]
    }
   ],
   "source": [
    "print(len(all_tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We now have his most recent tweets.\n",
    "\n",
    "However, there is significant overlap between the tweets that we have scraped from his account and the tweets that are in the CSV file.\n",
    "\n",
    "The latest tweet in the CSV file was posted on September 27, 2016 at 22:13:24. So, we need to keep any scraped tweets which were posted after this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tweets = []\n",
    "i = 0\n",
    "while (all_tweets[i].created_at!=datetime.datetime(2016, 9, 27, 22, 13, 24)):\n",
    "    tweets.append(all_tweets[i].text)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "We have  4698 tweets to work with\n"
     ]
    }
   ],
   "source": [
    "tweets1 = trump_tweets['text']\n",
    "tweets1 = tweets1.tolist()\n",
    "for t in tweets1:\n",
    "    tweets.append(t)\n",
    "print(type(tweets))\n",
    "print(\"We have \", len(tweets), \"tweets to work with\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Making distributional vectors from each tweet\n",
    "\n",
    "But to do that, we need to - \n",
    "1. Remove any twitter links and image links\n",
    "2. Remove any stopwords\n",
    "3. Make sure that we have a list of tweets where each tweet is a string\n",
    "4. Then use CountVectorizer http://scikit-learn.org/stable/modules/feature_extraction.html#common-vectorizer-usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Removing links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'presidential\", 'executive', 'order', 'on', 'identifying', 'and', 'reducing', 'tax', 'regulatory', \"burdens'\", 'executive', 'order:…', 'https://t.co/dpe6hdzlat']\n",
      "[\"'presidential\", 'executive', 'order', 'on', 'identifying', 'and', 'reducing', 'tax', 'regulatory', \"burdens'\", 'executive', 'order:…']\n"
     ]
    }
   ],
   "source": [
    "temp_tweets = []\n",
    "for t in tweets:\n",
    "    temp_tweets.append(t.lower().split())\n",
    "\n",
    "print(temp_tweets[1])\n",
    "for t in temp_tweets:\n",
    "    for w in t:\n",
    "        if \"http\" in w or \"@\" in w: #I've removed any instances where he tags anyone in his tweets. \n",
    "                                    #I thought the word vectors might be too sparse if I left those in.\n",
    "            t.remove(w)\n",
    "print(temp_tweets[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## NOTE: This link removal is not working properly. Have to fix later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "for t in temp_tweets:\n",
    "    for w in t:\n",
    "        if w in stop:\n",
    "            t.remove(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Succesfully removed stopwords. At this point, each tweet is a list of words and temp_tweets is a list. What we need to use CountVectorizer is a list where each element is a string.\n",
    "\n",
    "Therefore, we need to convert each tweet from a lists of words to a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = []\n",
    "for t in temp_tweets:\n",
    "    tweets.append(' '.join(t))\n",
    "type(tweets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Our methodology for classifying tweets\n",
    "\n",
    "Step 0 - Download a pre-trained Word2Vec model. We tried training our own model, but we did not have enough data.\n",
    "\n",
    "Step 1 - Hand tag some number of tweets (we ended up tagging about 280 tweets) and classify them into the following categories - \n",
    "1. Foreign Policy / International News\n",
    "2. Domestic Policy / domestic news\n",
    "3. Tweets about the media\n",
    "4. Attack tweets\n",
    "5. Other tweets\n",
    "6. Tweets about the election\n",
    "\n",
    "Step 2 - From our hand-tagged corpus, and for each category, create a list of words used.\n",
    "\n",
    "Step 3 - Create a word vector for each category by summing up the individual word vectors\n",
    "\n",
    "Step 4 - For each subsequent tweet, find cosine similarity between it and each category vector. Assign that tweet to the category it is most similar to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Step 0 - Downloading a pre-trained Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "google_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.24023438, -0.046875  , -0.05786133, -0.17285156,  0.13476562,\n",
       "       -0.03466797,  0.05957031, -0.02209473,  0.00334167, -0.03564453,\n",
       "       -0.04589844,  0.04248047, -0.09570312,  0.21582031, -0.12597656,\n",
       "       -0.06835938,  0.15332031,  0.17773438, -0.03662109,  0.03515625,\n",
       "        0.04418945,  0.28320312,  0.05297852, -0.01953125, -0.27929688,\n",
       "       -0.23828125,  0.00238037, -0.04345703,  0.26367188,  0.06591797,\n",
       "       -0.02624512,  0.03369141,  0.02880859, -0.15332031,  0.11083984,\n",
       "       -0.046875  , -0.02355957,  0.01000977,  0.23632812, -0.07421875,\n",
       "        0.27734375, -0.14746094,  0.02478027,  0.10351562, -0.33007812,\n",
       "        0.0050354 , -0.04736328,  0.16699219,  0.015625  ,  0.30859375,\n",
       "        0.15039062, -0.09472656,  0.08349609,  0.05883789, -0.17578125,\n",
       "       -0.00273132, -0.04101562, -0.30859375, -0.15332031, -0.05200195,\n",
       "       -0.19140625,  0.13476562, -0.28515625, -0.06445312, -0.00058365,\n",
       "        0.01348877, -0.00527954,  0.10498047,  0.20605469,  0.01538086,\n",
       "        0.06445312,  0.13574219,  0.05737305, -0.05541992,  0.02648926,\n",
       "        0.02868652,  0.33007812,  0.0246582 , -0.08642578, -0.0625    ,\n",
       "       -0.05834961, -0.25390625,  0.01055908, -0.20019531,  0.02770996,\n",
       "        0.15820312, -0.38867188, -0.06005859,  0.24414062,  0.09472656,\n",
       "        0.12695312, -0.14746094, -0.08300781, -0.10253906, -0.03540039,\n",
       "       -0.2421875 ,  0.03637695,  0.0057373 ,  0.265625  ,  0.31835938,\n",
       "       -0.25976562, -0.15429688, -0.06079102,  0.14941406,  0.00765991,\n",
       "       -0.09716797, -0.07226562, -0.01306152, -0.03955078, -0.01245117,\n",
       "       -0.140625  , -0.13964844,  0.01080322,  0.10253906,  0.11816406,\n",
       "       -0.31640625, -0.05371094,  0.06225586, -0.01977539,  0.15039062,\n",
       "       -0.05664062,  0.03564453,  0.11669922,  0.04956055, -0.171875  ,\n",
       "        0.11621094,  0.16601562, -0.0378418 , -0.03149414, -0.10986328,\n",
       "        0.03515625, -0.18359375,  0.03759766, -0.36914062, -0.18847656,\n",
       "        0.03833008,  0.03588867,  0.07763672, -0.01123047, -0.01904297,\n",
       "        0.09912109,  0.11328125,  0.02050781, -0.12792969,  0.16015625,\n",
       "       -0.23242188,  0.15722656, -0.13867188, -0.12792969,  0.0234375 ,\n",
       "        0.2734375 , -0.078125  ,  0.30078125, -0.11914062,  0.35742188,\n",
       "       -0.0703125 , -0.04394531, -0.20507812,  0.16308594, -0.10888672,\n",
       "       -0.05419922, -0.00585938,  0.27734375,  0.22363281, -0.02185059,\n",
       "       -0.33984375,  0.21386719, -0.28125   ,  0.19824219, -0.06982422,\n",
       "        0.03930664, -0.12695312,  0.04272461, -0.23925781,  0.08398438,\n",
       "        0.09912109,  0.04467773,  0.02246094, -0.06835938,  0.02282715,\n",
       "       -0.07373047,  0.04589844, -0.15234375,  0.08154297,  0.06884766,\n",
       "       -0.13183594, -0.29296875,  0.13085938, -0.03051758,  0.07324219,\n",
       "       -0.09375   ,  0.00778198,  0.22558594,  0.296875  , -0.21679688,\n",
       "        0.06884766, -0.01019287,  0.22167969, -0.10302734,  0.14941406,\n",
       "       -0.03417969,  0.22460938,  0.09472656, -0.06494141,  0.24414062,\n",
       "        0.04321289,  0.08154297, -0.06347656, -0.05981445, -0.02392578,\n",
       "       -0.08642578,  0.03686523, -0.24414062,  0.12402344,  0.11816406,\n",
       "        0.00180817, -0.01470947,  0.12109375, -0.15039062, -0.02612305,\n",
       "       -0.10058594,  0.07128906,  0.29492188,  0.02038574,  0.19238281,\n",
       "        0.25195312,  0.05224609,  0.16015625,  0.140625  , -0.13964844,\n",
       "        0.04321289,  0.12060547,  0.01098633, -0.09423828, -0.06542969,\n",
       "        0.01977539, -0.03222656,  0.1484375 ,  0.05737305, -0.30859375,\n",
       "       -0.1171875 , -0.03491211,  0.06835938, -0.03857422, -0.06347656,\n",
       "        0.03491211,  0.00891113,  0.17382812,  0.05151367,  0.03466797,\n",
       "       -0.04736328, -0.13964844, -0.12207031, -0.13085938,  0.02844238,\n",
       "       -0.04736328,  0.16503906,  0.30664062,  0.07958984, -0.2890625 ,\n",
       "        0.02941895,  0.04614258,  0.07519531,  0.13378906, -0.30664062,\n",
       "       -0.04174805, -0.23535156, -0.13964844, -0.23535156,  0.15722656,\n",
       "        0.01623535, -0.03051758,  0.0703125 , -0.13085938, -0.05664062,\n",
       "       -0.12988281,  0.23046875,  0.03881836, -0.0559082 , -0.0201416 ,\n",
       "       -0.08398438, -0.1953125 ,  0.09277344,  0.05444336, -0.03442383,\n",
       "       -0.11474609, -0.23046875,  0.10058594,  0.26953125, -0.18066406,\n",
       "       -0.18554688, -0.22363281, -0.06054688, -0.25195312,  0.02832031,\n",
       "       -0.28710938,  0.25      , -0.14355469, -0.140625  ,  0.03222656], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_model['campaign']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "google_model is our pre-trained model which we will be using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Step 1 - Hand tagging tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Using a .csv file of a number of hand tagged tweets, we place the tweets into the preselected categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "foreign = []\n",
    "domestic =[]\n",
    "media =[]\n",
    "attack = []\n",
    "election = []\n",
    "other = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "f = open('tagged_tweets.csv')\n",
    "csv_f = csv.reader(f)\n",
    "\n",
    "for row in csv_f:\n",
    "    tweet = row[0]\n",
    "    cats = row[1]\n",
    "    if \"1\" in cats:\n",
    "        foreign.append(tweet)\n",
    "    elif \"2\" in cats:\n",
    "        domestic.append(tweet)\n",
    "    elif \"3\" in cats:\n",
    "        media.append(tweet)\n",
    "    elif \"5\" in cats:\n",
    "        other.append(tweet)\n",
    "    elif \"6\" in cats:\n",
    "        election.append(tweet)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domestic:  104\n",
      "Foreign:  56\n",
      "Media:  83\n",
      "Other:  95\n",
      "Election:  79\n"
     ]
    }
   ],
   "source": [
    "print(\"Domestic: \",len(domestic))\n",
    "print(\"Foreign: \", len(foreign))\n",
    "print(\"Media: \", len(media))\n",
    "print(\"Other: \",len(other))\n",
    "print(\"Election: \",len(election))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Step 2 - Making a list of words used in each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Each tweet is a string right now.\n",
    "#This function will split up the string into individual words, remove any words which start with @\n",
    "#(i.e our generated tweets won't have any tags) and remove any punctuation\n",
    "\n",
    "def clean_up(tweets):\n",
    "    tweets1 = []\n",
    "    for t in tweets:\n",
    "        tweets1.append(t.split())\n",
    "        \n",
    "    tweets_words = []\n",
    "    for t in tweets1:\n",
    "        for w in t:\n",
    "            tweets_words.append(w)\n",
    "    tweets_words = tweets_words\n",
    "    \n",
    "    #removing '@' from any word which has it. The google_model does not have any words which start with @\n",
    "    temp_words = []\n",
    "    for word in tweets_words:\n",
    "        if word[0]=='@':\n",
    "            temp_words.append(word[1:])\n",
    "        else:\n",
    "            temp_words.append(word)\n",
    "    return temp_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "domestic_words = clean_up(domestic)\n",
    "foreign_words = clean_up(foreign)\n",
    "media_words = clean_up(media)\n",
    "election_words = clean_up(election)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(words):\n",
    "    x = [''.join(c for c in s if c not in string.punctuation) for s in words]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "domestic_words = remove_punctuation(domestic_words)\n",
    "foreign_words = remove_punctuation(foreign_words)\n",
    "media_words = remove_punctuation(media_words)\n",
    "election_words = remove_punctuation(election_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def remove_short_words(words, length):\n",
    "    temp_words = []\n",
    "    for word in words:\n",
    "        if len(word)>=length:\n",
    "            temp_words.append(word)\n",
    "    return temp_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "d_short_words = remove_short_words(domestic_words, 4)\n",
    "f_short_words = remove_short_words(foreign_words, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Step 3 - Create a category vector by adding up individual word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_category_vector(words):\n",
    "    vector = np.ones(300)\n",
    "    for word in words:\n",
    "        try:\n",
    "            vector = vector + google_model[word]\n",
    "        except KeyError: #some words are not in model. I don't want to pre-process everything so I'm just handling each exception\n",
    "            pass\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "domestic_vector = create_category_vector(domestic_words)\n",
    "foreign_vector = create_category_vector(foreign_words)\n",
    "media_vector = create_category_vector(media_words)\n",
    "election_vector = create_category_vector(election_words)\n",
    "obamacare = create_category_vector(\"Health\")\n",
    "isis = create_category_vector(\"Russia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fake media (not real media) gotten even worse since election. every story badly slanted. have hold to truth!'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_tweet_vector(tweet):\n",
    "    vector = np.ones(300)\n",
    "    for word in tweet:\n",
    "        try:\n",
    "            vector = vector + google_model[word]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def calc_cosine_similarity(tweet_vector, category_vector):\n",
    "    return cosine_similarity(tweet_vector, category_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "specific_foreign = []\n",
    "specific_domestic = []\n",
    "for word in foreign_words:\n",
    "    if word not in domestic_words:\n",
    "        specific_foreign.append(word)\n",
    "for word in domestic_words:\n",
    "    if word not in foreign_words:\n",
    "        specific_domestic.append(word)\n",
    "        \n",
    "specific_dshort = []\n",
    "specific_fshort = []\n",
    "for word in d_short_words:\n",
    "    if word not in f_short_words:\n",
    "        specific_fshort.append(word)\n",
    "for word in f_short_words:\n",
    "    if word not in d_short_words:\n",
    "        specific_dshort.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "specific_foreign = clean_up(specific_foreign)\n",
    "specific_domestic = clean_up(specific_domestic)\n",
    "\n",
    "specific_foreign = remove_punctuation(specific_foreign)\n",
    "specific_domestic = remove_punctuation(specific_domestic)\n",
    "\n",
    "specific_foreign_vector = create_category_vector(specific_foreign)\n",
    "specific_domestic_vector = create_category_vector(specific_domestic)\n",
    "dshort_vector = create_category_vector(specific_dshort)\n",
    "fshort_vector = create_category_vector(specific_fshort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def calc_scores(tweet):\n",
    "#     score = calc_cosine_similarity(create_tweet_vector(tweet), specific_domestic_vector)\n",
    "#     print(\"Domestic:\", score)\n",
    "    dscore = calc_cosine_similarity(create_tweet_vector(tweet), dshort_vector)\n",
    "#     print(\"Domestic Long words:\", dscore)\n",
    "#     score = calc_cosine_similarity(create_tweet_vector(tweet), specific_foreign_vector)\n",
    "#     print(\"Foreign: \",score)\n",
    "    fscore = calc_cosine_similarity(create_tweet_vector(tweet), fshort_vector)\n",
    "#     print(\"Foreign Long words:\", fscore)\n",
    "    return [dscore, fscore]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of domestic tweets =  104\n",
      "Number of domestic tweets tagged as domestic =  15\n",
      "Accuracy of domestic tweets =  0.14423076923076922\n",
      "\n",
      "Number of foreign tweets =  56\n",
      "Number of foreign tweets tagged as foreign =  48\n",
      "Accuracy of foreign tweets =  0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "dcount = 0\n",
    "fcount = 0\n",
    "total = 0\n",
    "for tweet in domestic:\n",
    "    domestic_score, foreign_score = calc_scores(tweet)\n",
    "    total+=1\n",
    "    if domestic_score > foreign_score:\n",
    "        dcount+=1\n",
    "    else:\n",
    "        fcount+=1\n",
    "\n",
    "print(\"Number of domestic tweets = \", total)\n",
    "print(\"Number of domestic tweets tagged as domestic = \", dcount)\n",
    "print(\"Accuracy of domestic tweets = \", dcount / total)\n",
    "\n",
    "dcount = 0\n",
    "fcount = 0\n",
    "total = 0\n",
    "for tweet in foreign:\n",
    "    domestic_score, foreign_score = calc_scores(tweet)\n",
    "    total+=1\n",
    "    if domestic_score > foreign_score:\n",
    "        dcount+=1\n",
    "    else:\n",
    "        fcount+=1\n",
    "        \n",
    "print(\"\\nNumber of foreign tweets = \", total)\n",
    "print(\"Number of foreign tweets tagged as foreign = \", fcount)\n",
    "print(\"Accuracy of foreign tweets = \", fcount / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
