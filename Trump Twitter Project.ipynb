{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Keeping Trump on Topic: LIN353C Final Project\n",
    "\n",
    "By Hannah Brinsko and Aditya Kharosekar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import csv\n",
    "import datetime\n",
    "import gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocessing - Scraping tweets, and cleaning them up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Importing tweets from the CSV file - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3218, 28)\n",
      "5     2016-09-27T22:13:24\n",
      "8     2016-09-27T21:08:22\n",
      "11    2016-09-27T20:31:14\n",
      "12    2016-09-27T20:14:33\n",
      "13    2016-09-27T20:06:25\n",
      "Name: time, dtype: object\n"
     ]
    }
   ],
   "source": [
    "tweets_csv = pd.read_csv(\"tweets.csv\")\n",
    "trump_tweets = tweets_csv[tweets_csv['handle']==\"realDonaldTrump\"]\n",
    "print(trump_tweets.shape)\n",
    "print(trump_tweets['time'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Looking at the CSV file, we see that it contains tweets only up to 09/27/2016. We need his more recent tweets as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Getting his most recent ~3200 tweets.\n",
    "\n",
    "3200 is approximately the limit to how many tweets Tweepy allows us to scrape. As it turns out, this is more than enough for our use when combined with our CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import json\n",
    "from tweepy import OAuthHandler\n",
    "import codecs\n",
    "\n",
    "consumer_key = \"i387QW7Eqgh12UHmK3VoQO9K5\"\n",
    "consumer_secret = \"BQI8c5eKale4etdA21mawnFqOmAziDQpnThm679V7UtLjbWlMG\"\n",
    "access_token = \"816857419338764288-S8Ay111O2Mo32QAs88tSnv5uKvmGCkF\"\n",
    "access_secret = \"HVU19yLuV0klltJl1fsDibAi7Hiq1U4GwsEV9kozTAc1m\"\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "all_tweets = []\n",
    "\n",
    "new_tweets = api.user_timeline(screen_name=\"realDonaldTrump\", count=200)\n",
    "\n",
    "\n",
    "all_tweets.extend(new_tweets)\n",
    "oldest = all_tweets[-1].id-1\n",
    "\n",
    "t = new_tweets[0];\n",
    "\n",
    "while len(new_tweets) > 0:\n",
    "   new_tweets = api.user_timeline(screen_name = \"realDonaldTrump\", count=200, max_id = oldest)\n",
    "   all_tweets.extend(new_tweets)\n",
    "   oldest = all_tweets[-1].id-1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3225\n"
     ]
    }
   ],
   "source": [
    "print(len(all_tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We now have his most recent tweets.\n",
    "\n",
    "However, there is significant overlap between the tweets that we have scraped from his account and the tweets that are in the CSV file.\n",
    "\n",
    "The latest tweet in the CSV file was posted on September 27, 2016 at 22:13:24. So, we need to keep any scraped tweets which were posted after this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tweets = []\n",
    "i = 0\n",
    "while (all_tweets[i].created_at!=datetime.datetime(2016, 9, 27, 22, 13, 24)):\n",
    "    tweets.append(all_tweets[i].text)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "We have  4675 tweets to work with\n"
     ]
    }
   ],
   "source": [
    "tweets1 = trump_tweets['text']\n",
    "tweets1 = tweets1.tolist()\n",
    "for t in tweets1:\n",
    "    tweets.append(t)\n",
    "print(type(tweets))\n",
    "print(\"We have \", len(tweets), \"tweets to work with\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Making distributional vectors from each tweet\n",
    "\n",
    "But to do that, we need to - \n",
    "1. Remove any twitter links and image links\n",
    "2. Remove any stopwords\n",
    "3. Make sure that we have a list of tweets where each tweet is a string\n",
    "4. Then use CountVectorizer http://scikit-learn.org/stable/modules/feature_extraction.html#common-vectorizer-usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Removing links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['trump', 'approval', 'hits', '50%', 'https://t.co/vjzkgtyqb9']\n",
      "['trump', 'approval', 'hits', '50%']\n"
     ]
    }
   ],
   "source": [
    "temp_tweets = []\n",
    "for t in tweets:\n",
    "    temp_tweets.append(t.lower().split())\n",
    "\n",
    "print(temp_tweets[1])\n",
    "for t in temp_tweets:\n",
    "    for w in t:\n",
    "        if \"http\" in w or \"@\" in w: #I've removed any instances where he tags anyone in his tweets. \n",
    "                                    #I thought the word vectors might be too sparse if I left those in.\n",
    "            t.remove(w)\n",
    "print(temp_tweets[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## NOTE: This link removal is not working properly. Have to fix later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "for t in temp_tweets:\n",
    "    for w in t:\n",
    "        if w in stop:\n",
    "            t.remove(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Succesfully removed stopwords. At this point, each tweet is a list of words and temp_tweets is a list. What we need to use CountVectorizer is a list where each element is a string.\n",
    "\n",
    "Therefore, we need to convert each tweet from a lists of words to a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = []\n",
    "for t in temp_tweets:\n",
    "    tweets.append(' '.join(t))\n",
    "type(tweets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Our methodology for classifying tweets\n",
    "\n",
    "Step 0 - Download a pre-trained Word2Vec model. We tried training our own model, but we did not have enough data.\n",
    "\n",
    "Step 1 - Hand tag some number of tweets (we ended up tagging about 280 tweets) and classify them into the following categories - \n",
    "1. Foreign Policy / International News\n",
    "2. Domestic Policy / domestic news\n",
    "3. Tweets about the media\n",
    "4. Attack tweets\n",
    "5. Other tweets\n",
    "6. Tweets about the election\n",
    "\n",
    "Step 2 - From our hand-tagged corpus, and for each category, create a list of words used.\n",
    "\n",
    "Step 3 - Create a word vector for each category by summing up the individual word vectors\n",
    "\n",
    "Step 4 - For each subsequent tweet, find cosine similarity between it and each category vector. Assign that tweet to the category it is most similar to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Step 0 - Downloading a pre-trained Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 400000  words loaded\n"
     ]
    }
   ],
   "source": [
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r', encoding = 'utf8')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = [float(val) for val in splitLine[1:]]\n",
    "        model[word] = embedding\n",
    "    print(\"Done.\",len(model),\" words loaded\")\n",
    "    return model\n",
    "\n",
    "glove_model = loadGloveModel('glove.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.14675,\n",
       " 1.1692,\n",
       " 0.69416,\n",
       " -0.061429,\n",
       " -0.13677,\n",
       " 0.42015,\n",
       " -0.716,\n",
       " 0.019014,\n",
       " -0.52896,\n",
       " -0.83643,\n",
       " -1.8561,\n",
       " -0.18324,\n",
       " 0.057648,\n",
       " -0.31188,\n",
       " 0.024997,\n",
       " 0.045878,\n",
       " -0.098728,\n",
       " -0.21451,\n",
       " 0.14298,\n",
       " -0.0080809,\n",
       " -0.14569,\n",
       " 0.38326,\n",
       " 0.63811,\n",
       " -0.46426,\n",
       " 1.0953,\n",
       " -2.15,\n",
       " -0.18462,\n",
       " 0.1738,\n",
       " -0.50607,\n",
       " 0.00057719,\n",
       " 0.52828,\n",
       " 0.6685,\n",
       " -0.89692,\n",
       " -0.34346,\n",
       " -0.15456,\n",
       " -0.97313,\n",
       " -0.69441,\n",
       " 0.59201,\n",
       " -1.2194,\n",
       " -1.3469,\n",
       " -0.25691,\n",
       " 0.34537,\n",
       " -0.43824,\n",
       " -0.096233,\n",
       " 0.29882,\n",
       " -0.29174,\n",
       " -0.47201,\n",
       " -0.32221,\n",
       " 0.079279,\n",
       " 0.59419]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_model['hillary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "google_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.05322266, -0.14746094, -0.01037598,  0.11865234, -0.00066757,\n",
       "        0.09375   , -0.0456543 ,  0.02746582,  0.07910156,  0.07177734,\n",
       "       -0.09912109, -0.14941406, -0.07226562, -0.00349426, -0.0255127 ,\n",
       "        0.12597656,  0.04638672,  0.02172852,  0.01916504, -0.10009766,\n",
       "        0.05078125,  0.05737305,  0.19433594, -0.02807617, -0.14257812,\n",
       "        0.11816406, -0.04345703,  0.24316406,  0.11816406, -0.07177734,\n",
       "       -0.06982422,  0.09375   , -0.1953125 , -0.06176758, -0.13183594,\n",
       "        0.08056641,  0.00698853,  0.23828125,  0.00891113,  0.06225586,\n",
       "       -0.00152588, -0.12988281,  0.13769531,  0.04077148, -0.01806641,\n",
       "        0.00512695,  0.05297852, -0.07421875, -0.02636719,  0.0559082 ,\n",
       "       -0.18457031,  0.05004883,  0.03491211,  0.10107422,  0.0246582 ,\n",
       "        0.00077438, -0.04272461, -0.01055908,  0.08984375, -0.10449219,\n",
       "        0.12988281,  0.03515625,  0.07519531, -0.09277344,  0.01757812,\n",
       "       -0.10546875, -0.07128906, -0.11669922,  0.00082779, -0.03369141,\n",
       "       -0.02026367,  0.12060547,  0.08496094, -0.02368164, -0.09082031,\n",
       "       -0.04272461,  0.04467773, -0.06445312, -0.0300293 ,  0.05981445,\n",
       "       -0.10498047,  0.02294922, -0.02624512, -0.08789062,  0.09326172,\n",
       "       -0.03369141,  0.03198242,  0.14550781,  0.04614258, -0.04174805,\n",
       "       -0.12158203,  0.10839844, -0.0456543 , -0.08984375, -0.12451172,\n",
       "        0.15917969,  0.0057373 ,  0.14257812,  0.11767578,  0.02331543,\n",
       "       -0.08886719,  0.03442383,  0.05786133,  0.02355957,  0.06933594,\n",
       "        0.18652344, -0.00518799,  0.22265625, -0.03808594, -0.03344727,\n",
       "       -0.08300781, -0.11132812,  0.05908203,  0.03881836,  0.09570312,\n",
       "        0.02941895, -0.08105469, -0.10546875,  0.08789062, -0.06445312,\n",
       "       -0.07128906,  0.06176758, -0.10839844,  0.01989746,  0.01745605,\n",
       "        0.01397705, -0.109375  ,  0.20996094,  0.01257324, -0.13183594,\n",
       "       -0.24511719, -0.18066406, -0.04418945,  0.11181641,  0.04248047,\n",
       "        0.16113281, -0.18945312,  0.03271484,  0.13183594,  0.09765625,\n",
       "        0.17675781,  0.00650024,  0.04980469, -0.05151367, -0.00534058,\n",
       "       -0.07324219,  0.08740234,  0.0324707 ,  0.10742188, -0.1484375 ,\n",
       "        0.22558594, -0.12060547, -0.03710938, -0.18359375, -0.02868652,\n",
       "       -0.13867188, -0.0078125 , -0.06201172, -0.04516602, -0.05151367,\n",
       "       -0.11181641,  0.234375  , -0.01855469, -0.0456543 , -0.0300293 ,\n",
       "       -0.15625   ,  0.10205078, -0.04882812, -0.15234375,  0.1328125 ,\n",
       "        0.01672363,  0.06640625, -0.09765625, -0.05566406, -0.11328125,\n",
       "        0.09033203,  0.04321289, -0.05932617,  0.01855469, -0.10546875,\n",
       "       -0.05004883,  0.02697754, -0.06835938,  0.04467773,  0.07128906,\n",
       "        0.0100708 , -0.171875  , -0.18066406,  0.07910156,  0.06494141,\n",
       "        0.10205078,  0.08300781,  0.00964355,  0.05688477,  0.09619141,\n",
       "        0.06298828, -0.11376953,  0.01483154,  0.06982422, -0.04760742,\n",
       "       -0.17382812,  0.04443359, -0.09960938, -0.04980469, -0.18261719,\n",
       "       -0.02404785, -0.0177002 ,  0.05102539, -0.10253906, -0.09472656,\n",
       "       -0.06884766,  0.09667969,  0.06396484, -0.11132812, -0.04418945,\n",
       "        0.15136719, -0.05957031, -0.07958984,  0.0324707 , -0.15332031,\n",
       "       -0.08544922, -0.14550781,  0.00294495,  0.05249023,  0.0402832 ,\n",
       "        0.03930664,  0.05957031,  0.08642578,  0.05126953, -0.00970459,\n",
       "        0.05859375, -0.0324707 ,  0.01397705, -0.09619141,  0.04882812,\n",
       "        0.11181641,  0.12695312,  0.14453125,  0.12890625, -0.08056641,\n",
       "        0.21679688,  0.00479126, -0.04003906,  0.12207031, -0.12255859,\n",
       "        0.06787109,  0.00482178,  0.09423828, -0.22167969, -0.0402832 ,\n",
       "        0.0703125 ,  0.04711914,  0.03881836,  0.06054688,  0.14941406,\n",
       "       -0.10302734,  0.05175781, -0.05834961, -0.04223633, -0.1875    ,\n",
       "        0.05810547,  0.06079102,  0.078125  , -0.01867676, -0.09082031,\n",
       "        0.13183594, -0.11328125, -0.12597656,  0.07324219, -0.04736328,\n",
       "       -0.11669922,  0.2109375 , -0.01757812, -0.09814453, -0.02490234,\n",
       "        0.08300781, -0.03088379,  0.01989746, -0.00285339,  0.19042969,\n",
       "       -0.17382812,  0.02661133, -0.05908203,  0.01031494, -0.04199219,\n",
       "       -0.12988281, -0.04077148, -0.05981445,  0.03833008,  0.01647949,\n",
       "        0.12890625, -0.13085938,  0.01928711,  0.11083984, -0.10107422,\n",
       "       -0.18164062, -0.01251221, -0.02575684,  0.01867676,  0.11523438], dtype=float32)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_model['foxnews']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "google_model is our pre-trained model which we will be using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Step 1 - Hand tagging tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Using a .csv file of ~280 hand tagged tweets, we place the tweets into the preselected categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "foreign = []\n",
    "domestic =[]\n",
    "media =[]\n",
    "attack = []\n",
    "election = []\n",
    "other = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "f = open('tagged_tweets.csv')\n",
    "csv_f = csv.reader(f)\n",
    "\n",
    "for row in csv_f:\n",
    "    tweet = row[0]\n",
    "    cats = row[1]\n",
    "    if \"1\" in cats:\n",
    "        foreign.append(tweet)\n",
    "    elif \"2\" in cats:\n",
    "        domestic.append(tweet)\n",
    "    elif \"3\" in cats:\n",
    "        media.append(tweet)\n",
    "    elif \"5\" in cats:\n",
    "        other.append(tweet)\n",
    "    elif \"6\" in cats:\n",
    "        election.append(tweet)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domestic:  104\n",
      "Foreign:  56\n",
      "Media:  83\n",
      "Other:  95\n",
      "Election:  79\n"
     ]
    }
   ],
   "source": [
    "print(\"Domestic: \",len(domestic))\n",
    "print(\"Foreign: \", len(foreign))\n",
    "print(\"Media: \", len(media))\n",
    "print(\"Other: \",len(other))\n",
    "print(\"Election: \",len(election))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Making a list of words used in each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Each tweet is a string right now.\n",
    "#This function will split up the string into individual words, remove any words which start with @\n",
    "#(i.e our generated tweets won't have any tags)\n",
    "\n",
    "def clean_up(tweets):\n",
    "    tweets1 = []\n",
    "    for t in tweets:\n",
    "        tweets1.append(t.split())\n",
    "        \n",
    "    tweets_words = []\n",
    "    for t in tweets1:\n",
    "        for w in t:\n",
    "            tweets_words.append(w)\n",
    "    tweets_words = tweets_words\n",
    "    \n",
    "    #removing '@' from any word which has it. The google_model does not have any words which start with @\n",
    "    temp_words = []\n",
    "    for word in tweets_words:\n",
    "        if word[0]=='@':\n",
    "            temp_words.append(word[1:])\n",
    "        else:\n",
    "            temp_words.append(word)\n",
    "    return temp_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "domestic_words = clean_up(domestic)\n",
    "foreign_words = clean_up(foreign)\n",
    "media_words = clean_up(media)\n",
    "election_words = clean_up(election)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Create a category vector by adding up individual word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_category_vector(words):\n",
    "    vector = np.ones(300)\n",
    "    for word in words:\n",
    "        try:\n",
    "            vector = vector + google_model[word]\n",
    "        except KeyError: #some words are not in model. I don't want to pre-process everything so I'm just handling each exception\n",
    "            pass\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "domestic_vector = create_category_vector(domestic_words)\n",
    "foreign_vector = create_category_vector(foreign_words)\n",
    "media_vector = create_category_vector(media_words)\n",
    "election_vector = create_category_vector(election_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one one keeping promises - the border, energy, jobs, regulations. big changes are happening!'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_tweet_vector(tweet):\n",
    "    vector = np.ones(300)\n",
    "    for word in tweet:\n",
    "        try:\n",
    "            vector = vector + google_model[word]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_cosine_similarity(tweet_vector, category_vector):\n",
    "    return cosine_similarity(tweet_vector, category_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_scores(tweet):\n",
    "    score = calc_cosine_similarity(create_tweet_vector(tweet), domestic_vector)\n",
    "    print(\"Domestic: \",score)\n",
    "    score = calc_cosine_similarity(create_tweet_vector(tweet), foreign_vector)\n",
    "    print(\"Foreign:\", score)\n",
    "#     score = calc_cosine_similarity(create_tweet_vector(tweet), media_vector)\n",
    "#     print(\"Media:\", score)\n",
    "#     score = calc_cosine_similarity(create_tweet_vector(tweet), election_vector)\n",
    "#     print(\"Election: \",score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for t in tweets[100:150]:\n",
    "    print(t)\n",
    "    print(calc_scores(t))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for word in media_words:\n",
    "    if word==\"foxnews\":\n",
    "        count+=1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
